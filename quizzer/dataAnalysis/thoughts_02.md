# On training a large text embedding model

## Transfer Learning
The first step that seems apparent is to gather a large corpus of academic text, as described in Manning's paper, slice academic papers up into short n-k sentence chunks (initial intuition says just 2-3 sentences should be ideal, but who knows), then with a data set of millions of "documents" we could proceed to the next step.

We then compile a list of question documents that will actually be embedded. But for first the embedding space itself must be crafted.

Using the large corpus of academic documents we train a large model and embed that space, we should get a low resolution of the entire academic literature, I would recommend partnering with annas-archive or other such "provider" to acquire this dataset. Ensuring we only pull from academic material other than fiction since the focus is on academic knowledge. The embedding space then represents the entire space of knowledgee across all documents.

The dataset of question documents is then finally fed into the trained model to be embedded, we then extract the distance information in the space, recording only the distance and connections between questions that are close enough to influence knowledge in that local zone.

## Recursive structure
What we get amounts to an index node in what becomes a hierarchical embedding model. A tree of embedding models.

However given the low resolution nature of the initial space, we need to recursively train smaller and smaller models. The process would involve running a topic model to cluster the broad topics in the space, if there are outliers in the space we need to collect more data. We then clone the large model (taking advantage of transfer learning effects) for each broad topic identified. For each topic identified we fine-tune the large model on only this topic documents (re-feeding the same information again into it). We then have multiple models trained on each specific broad domain. We then recurse, for each specialized model identify the broad clusters or topics found inside that specific domain, repeating the same process (clone the specialized model for each topic, and fine-tune on the specific topic information). We define a stopping point for the recursive training, when the clustering model is no longer able to find multiple meaningful clusters we stop and declare this as the deepest part of the model tree.

To compile a final embedding space, we first copy the initial embedding space created by the index node of our model tree. This becomes our final compiled embedding space. At each node of the tree we are able to record the section of the embedding space that the next model uses. We then expand the compiled embedding space. On a 2d line if the section of the line expands, all points to the left of the expanded space get moved by that amount, as well as those of the right, this follows for all dimensions from 2 to k. We run this expansion from the index node down to the bottom. We are left with an extremely large, extremely sparse high resolution embedding space on which we can model and learn how individuals form new knowledge and how the information in one zone effects information in another part of the space.

## Use of the space
A separate model is trained on user responses to presented questions in order to predict the probability that user will get any given question (seen or not) correct, the model is provided detailed information about the overall performance and statistics on the Quizzer platform, performance data (which includes temporal data) on the question being predicted, their personal profile, the user's history with adjacent questions that fall within the desired radius, and performance with other topics. Combined this large high dimensional training sample should contain everything we need for this model to learn what the knowledge dependencies are and accurately predict user performance in any region of the knowledge space.

A second model is trained on user responses to how well they understand any given question, and predicts how well the user understands the underlying material in the question. This model will use the same training samples as the first, but the target variable will be different. We will periodically ask after a question is answered how well the user felt they understand the underlying concepts beings asked. (this does present an issue where the understanding assessment is unduly influenced by the asking of the question itself likely causing some skewed bias that should be accounted for). This model provides us an overlay of how well the user feels they understand the space which should be noted is a different problem then if a user actually can answer in that space.

A third model is trained on user responses to how much interest they have in the questions underlying concepts, and will be presented as a 5 star ranking embedded at the bottom of the interface. This model will serve as a recommender system that can inform the system as to what to ask and when, since user interest is tied to how well a user learns we must account for interest. The dataset for this will be simply the embedding coordinate(n features one for each dimension) and the user ranking at that position.

A consideration should be made for learning goals specifically, and more so for learning goals that the user has set out for themselves but for whichever reason decide to rank the questions along that path as uninteresting and unengaging. This programming would serve as an override to the recommendation system should the user desire a more targeted cirriculum based learning path, rather than an exploratory program. This would not be a model but a tweak to the algorithm that ties all these model systems together.

With an embedding space in hand, and multiple model to give us a detailed overlay of the user's familiarity and interest with the space, we train a reinforcement learning model to learn what questions to ask and when to the individual user. It must learn how best to maximize retention over time, increase overall performance across the entire space, and allow for user interests to take precedent, in as few amount of questions as possible. So if it's given 100 questions to ask it must pick them wisely and in such an order than maximizes the understanding of the individual.



## Another consideration
Linguistics - The use of utterances of the vocal tract to produce noise in which meaning is interpreted and allows communication between living beings.

Such utterances shift and over time, and the meaning that is interpreted changes over time as well. To illustrate for older generations the term "game" is clearly understood to be the ability of a human being (usually male) to sexually attract another (usually female) given our social context here in the USA. For newer generations this term is now replaced with the term "rizz", "rizzler" instead of "player". Academic literature has the benefit of generally being immune to such fluctuations in meaning over time. This leaves the question of how to account for non-academic literature, I propose two options. The first option is to omit any non-academic literature from the embedding model. The second is much more computationally intense, but should give much better results for individuals. We time periods, the way in which jargon has changed, on which periods they were popular and when they began to fall off. We segment out generational eras and locations (would need to account for regional dialects as well). We take all the non-Academic literature and parse it out into these categories based on regional dialects, and pop culture time periods. Then we get a separate embedding model trained for each and every category here. Based on the user's profile we choose which model to become active for them. 