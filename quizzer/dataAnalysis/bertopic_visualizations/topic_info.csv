Topic,Count,Name,Representation,Representative_Docs
-1,549,-1_learning_memory_error_input,"['learning', 'memory', 'error', 'input', 'time', 'phi phi', 'neural', 'linear', 'function', 'neural networks']","['What does the Yonelinas familiarity-recollection model assume about the relationship between recollection and familiarity during recognition? The Yonelinas Familiarity-Recollection Model (often called the Dual-Process Model of Recognition Memory) proposes that recognition is based on two distinct processes: a fast, automatic process called familiarity (a feeling of knowing), and a slower, effortful process called recollection (retrieval of specific details). The model assumes a threshold, all-or-none role for recollection; if recollection succeeds, it is sufficient for the ""yes"" response and overrides any familiarity signal. If recollection fails, the decision falls back to a familiarity-based signal-detection process. Thus, the three selected options correctly capture the core tenets of the model: recollection is all-or-none and overrides familiarity, when recollection fails, the decision uses familiarity, and consequently, if recollection occurs, the participant answers ""yes"". The other options, like familiarity always overriding recollection or the processes combining into a single scalar value, are contrary to the distinct, hierarchical nature of this model. If recollection occurs, participant answers ""yes"" When recollection fails, decision uses familiarity Recollection is all-or-none and overrides familiarity Familiarity always overrides recollection in responses Participants respond ""no"" when recollection and familiarity conflict Recollection and familiarity are combined into a single scalar value', ""How do the margins relate to support vectors in the SVM model? In a Support Vector Machine (SVM) model, the margin is specifically defined as the shortest perpendicular distance between the separating hyperplane (decision boundary) and the closest data points from either class, which are known as the support vectors . While the other are incorrect because margins do not measure general data variance, which is a property of principal component analysis, nor do they represent the difference between predicted and actual labels, which defines a residual or error. Furthermore, margins are not used to define a number of clusters—a task for unsupervised learning—nor do they indicate the average distance between all points in the dataset. Therefore, the margin serves solely as a geometric buffer zone that the SVM algorithm seeks to maximize to improve the model's generalization and classification accuracy. They are distances from the boundary to nearest support vectors. Margins measure the overall data variance across all feature dimensions. Margins represent the difference between predicted and actual labels. Margins define the number of clusters formed from support vectors. Margins indicate the average distance between all data points."", 'What is the main reason usability and performance improved in neural networks post-2010 under the name \'deep learning\'? The main reason for the dramatic improvement in neural networks post-2010, which led to the term ""deep learning,"" was the combination of two crucial factors. Firstly, the availability of vastly larger datasets (like ImageNet) provided the necessary volume of data to train more complex models without overfitting. Secondly, the emergence and popularization of powerful parallel computing resources, specifically Graphics Processing Units (GPUs), made it computationally feasible to train neural networks with significantly more layers (deeper architectures). This ability to train deeper, more complex models on massive data led to breakthroughs in performance, distinguishing deep learning from earlier shallow neural network approaches. Large datasets and better computing power allowed deeper architectures and improved results. Shallow networks became more popular due to simpler training algorithms and smaller data needs. The concept of neural networks was abandoned until new statistical theories revived it. Neural networks were replaced with decision trees leading to the rise of deep learning. Support vector machines were integrated inside neural networks, combining advantages of both.']"
0,1331,0_dna_energy_rna_pyruvate,"['dna', 'energy', 'rna', 'pyruvate', 'proteins', 'strand', 'chromosomes', 'replication', 'genetic', 'carbon']","['Which chemical agent causes aneuploidy by disrupting microtubule polymerization? this chemical specifically binds to tubulin dimers, preventing the assembly of spindle fibers required for equal chromosome segregation during cell division. Without functional microtubules, sister chromatids or homologous chromosomes fail to separate correctly, leading to daughter cells with an abnormal number of chromosomes, a condition known as aneuploidy. In contrast, the statement regarding X-rays is scientifically incorrect because X-rays are ionizing radiation, not UV radiation, and they primarily cause DNA strand breaks rather than direct microtubule disruption. Benzene is primarily associated with DNA adduct formation and bone marrow toxicity rather than specifically blocking replication without microtubule effects, while fenvalerate and tobacco smoke involve complex neurotoxic or carcinogenic pathways that do not center on the specific disruption of microtubule polymerization as their primary mechanism for aneuploidy. Colchicine causes aneuploidy by disrupting microtubules. X-rays cause aneuploidy through UV radiation damage. Benzene blocks DNA replication without microtubule effects. Fenvalerate pesticide activates chromosome repair mechanisms. Tobacco smoke induces aneuploidy by enzymatic inhibition.', ""Which is a reason that explains why DNA replication occurs in a continuous fashion on one parental DNA strand and in a discontinuous fashion on the other parental DNA strand? This enzyme can only synthesize DNA in the 5' to 3' direction, requiring a free hydroxyl group to attach new nucleotides. Since the two parental strands are antiparallel, one strand (the leading strand) can be synthesized continuously toward the replication fork, while the other (the lagging strand) must be built in short, discontinuous Okazaki fragments to maintain that specific chemical orientation. While it is true that initiation requires a primer, base pairing is essential, and enzymes ensure speed, these facts apply equally to both strands and do not explain the directional discrepancy. Therefore, the physical constraint of the polymerase's active site is the sole reason for the asymmetric nature of the replication process. DNA polymerase only works to add nucleotides in one direction of the growing chain. Initiation of a new DNA strand requires the use of a primer. DNA replication depends on base pairing between a template and nucleotides being added to the new strand. A polymerase enzyme is needed to ensure that the replication process is both accurate and rapid"", ""Why is the double-helical structure of DNA essential to its function? The double-helical structure is crucial because the two strands are complementary, meaning the sequence of one dictates the sequence of the other (Adenine pairs with Thymine, Guanine with Cytosine). This complementarity allows each strand to act as a template for the synthesis of a new, identical strand during replication, ensuring the accurate passage of genetic information to daughter cells during cell division. Furthermore, the helical twist and double-strand arrangement provide stability and enable the orderly stacking and binding of proteins necessary for gene regulation and packaging. The other options are incorrect: DNA doesn't directly transcribe protein (RNA does), its function is dependent on the double strand, and the twists allow, not prevent, replication. Complementary strands allow accurate replication of genetic info. Complementarity enables info passage during cell division. The helical form facilitates orderly stacking and binding. Double helix ensures RNA directly transcribes protein structure. Function depends solely on DNA strands being single and linear. Helical twists prevent any replication or transcription processes.""]"
1,1022,1_cell_dna_cycle_division,"['cell', 'dna', 'cycle', 'division', 'replication', 'mitosis', 'meiosis', 'carbon', 'cell division', 'chromatids']","['The mitotic spindle The mitotic spindle is a dynamic structure composed of microtubules that forms during mitosis (and meiosis) and is crucial for accurately separating chromosomes between two daughter cells. Microtubules attach to the kinetochores of chromosomes, and through their regulated growth and shortening (depolymerization), along with the action of motor proteins, they generate the pushing and pulling forces required to align chromosomes at the metaphase plate and pull sister chromatids apart during anaphase. The other options are incorrect: the spindle begins to form during prophase, not metaphase; it is composed of microtubules, not intermediate filaments or microfilaments; and it is necessary for mitosis in all eukaryotes, including both animal and plant cells. is involved in creating the force needed for chromosome movement begins to form during metaphase is composed of intermediate filaments is composed of two different types of microfilaments is necessary for mitosis in animal cells but not plant cells', 'How does cytokinesis differ in animal versus plant cells? The first correct point is that Plant cells have rigid cell walls affecting division, which prevents them from pinching in like an animal cell. This leads to the second correct point: Animal cells form a contractile ring to pinch cytoplasm, using a ring of actin and myosin filaments to form a cleavage furrow and divide the cell. Because plant cells cannot form a furrow, they rely on the third correct point: Plant cells form a cell plate from vesicles, where Golgi-derived vesicles merge in the center of the cell to build a new cell wall and membrane to separate the two daughter cells. The other options are incorrect because animal cells do not build a cell plate, cytokinesis occurs in both types of cells, and plant cells do not use a contractile ring. Plant cells have rigid cell walls affecting division Animal cells form a contractile ring to pinch cytoplasm Plant cells form a cell plate from vesicles Animal cells build a cell plate Cytokinesis only occurs in animal cells Plant cells use a contractile ring', ""What are mutagens and their role in causing cancer? Mutagens are defined as physical or chemical agents that interact with the cell's genetic material, typically \\text{DNA}, and cause permanent changes known as mutations. These agents, which include chemicals, radiation (like  text  and \\text{X-rays}), and certain viruses, damage or chemically alter the structure of \\text{DNA} bases, leading to errors during \\text{DNA} replication or repair. The accumulation of these mutations in genes that control cell growth and division (proto-oncogenes and tumor suppressor genes) can disrupt the normal cell cycle and regulatory mechanisms, ultimately driving the uncontrolled cell proliferation characteristic of cancer. The other options describe entirely different biological molecules or processes, such as lipids, normal enzymes, repair proteins, or \\text{RNA} molecules, which do not fit the definition or primary role of a mutagen. Agents that chemically alter DNA bases and cause mutations linked to cancer Lipids that protect DNA from damage Enzymes that promote normal cell division Proteins that repair mutated DNA bases RNA molecules that regulate gene expression in cells""]"
2,937,2_data_regression_trees_variable,"['data', 'regression', 'trees', 'variable', 'classification', 'decision', 'overfitting', 'bias', 'models', 'statements true']","['Which of the following statements is false about Ensemble learning? Ensemble learning is not an unsupervised learning algorithm. It is a supervised learning algorithm that combines several machine learning techniques into one predictive model to decrease variance and bias. It can be trained and then used to make predictions. And this ensemble can be shown to have more flexibility in the functions they can represent. It is a supervised learning algorithm More random algorithms can be used to produce a stronger ensemble It is an unsupervised learning algorithm Ensembles can be shown to have more flexibility in the functions they can represent', 'What is the role of cross-validation in determining the optimal tree size during pruning? Cross-validation is utilized in decision tree pruning to provide a statistically sound estimate of the model\'s performance on unseen data, allowing the algorithm to identify a subtree that minimizes error without overfitting. This highlights the necessary trade-off between a tree that is too complex (high variance) and one that is too simple (high bias). While cross-validation does not eliminate qualitative variables, nor does it increase tree size without limit, which would exacerbate overfitting. Additionally, it does not remove all internal nodes, as that would result in a useless model, nor can it compute the ""exact"" test error on future data, only an estimate based on available samples. Consequently, cross-validation serves as a critical tuning mechanism to ensure the resulting pruned tree generalizes effectively to new observations. It estimates test error to select a subtree balancing accuracy and complexity It eliminates qualitative variables from the predictor set It increases the tree size without limit for better training fit It removes all internal nodes set by the splitting function It directly computes the exact test error on unseen data', 'Which of the following statements is false about Ensemble learning? Ensemble learning is not an unsupervised learning algorithm. It is a supervised learning algorithm that combines several machine learning techniques into one predictive model to decrease variance and bias. It can be trained and then used to make predictions. And this ensemble can be shown to have more flexibility in the functions they can represent. It is a supervised learning algorithm More random algorithms can be used to produce a stronger ensemble It is an unsupervised learning algorithm Ensembles can be shown to have more flexibility in the functions they can represent']"
3,584,3_open parenthesis_parenthesis close parenthesis_open parenthesis close_open parenthesis close parenthesis,"['open parenthesis', 'parenthesis close parenthesis', 'open parenthesis close', 'open parenthesis close parenthesis', 'square root', 'power minus', 'close parenthesis equals', 'parenthesis power', 'sud sud sud sud', 'parenthesis minus']","['Factor the expression below.\r\n\r\nx to the power of 3−3x to the power of 2−18x x to the power of 3  minus  3x to the power of 2  minus  18x\r\nFirst, factor out an x, since it is present in all terms.\r\nx open parenthesis x to the power of 2  minus  3x  minus  18 close parenthesis \r\nWe need two factors that multiply to  minus 18 and add to  minus 3.\r\n minus 6  times  3  equals   minus 18 and  minus 6  plus  3  equals   minus 3\r\nOur factors are  minus 6 and  plus 3.\r\nx open parenthesis x  minus  6 close parenthesis  open parenthesis x  plus  3 close parenthesis \r\nWe can check our answer using FOIL to get back to the original expression.\r\nFirst:  open parenthesis x close parenthesis  open parenthesis x close parenthesis   equals  x to the power of 2\r\nOutside:  open parenthesis x close parenthesis  open parenthesis 3 close parenthesis   equals  3x\r\nInside:  open parenthesis x close parenthesis  open parenthesis  minus 6 close parenthesis   equals   minus 6x\r\nLast:  open parenthesis  minus 6 close parenthesis  open parenthesis 3 close parenthesis   equals   minus 18\r\nAdd together and combine like terms.\r\nx to the power of 2  plus  3x  minus  6x  minus  18  equals  x to the power of 2  minus  3x  minus  18\r\nDistribute the x that was factored out first.\r\nx open parenthesis x to the power of 2  minus  3x  minus  18 close parenthesis   equals  x to the power of 3  minus  3x to the power of 2  minus  18x x open parenthesis x  minus  6 close parenthesis  open parenthesis x  plus  3 close parenthesis   open parenthesis x to the power of 2  minus  6x close parenthesis  open parenthesis x  plus  3 close parenthesis   open parenthesis x to the power of 3  minus  6 close parenthesis  open parenthesis 2x  plus  3 close parenthesis  x open parenthesis x  minus  4 close parenthesis  open parenthesis x  plus  2 close parenthesis ', ""Derive the following function:\nf(x) = f open parenthesis x close parenthesis   equals    square root of 3  divided by x to the power of 3   -\\frac{7}{3\\sqrt[3]{{x}^{10}}} Step 1: Identify which is the numerator/dividend and which is the denominator/divisor. The numerator/dividend will be denoted as u while the denominator/divisor will be v.\n\nTherefore, we have\n\nu  equals   square root of 3 \nv  equals  x to the power of 3 Step 2: Derive u and v individually.\n\nThen, we have\n\nu  equals   square root of 3 \nu  equals  x to the power of  2 divided by 3 \nu'  equals   2 divided by 3 x to the power of  minus  1 divided by 3 \n\nv  equals  x to the power of 3\nv'  equals  3x to the power of 2 Step 3: Substitute u, u', v and v' into the quotient rule formula.\n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals   vu'  minus  uv' divided by v to the power of 2 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    left  open parenthesis x to the power of 3 right  close parenthesis   times  left  open parenthesis  2 divided by 3 x to the power of  minus  1 divided by 3  right  close parenthesis   minus   left  open parenthesis x to the power of  2 divided by 3  right  close parenthesis   times  left  open parenthesis 3x to the power of 2 right  close parenthesis  divided by  left  open parenthesis x to the power of 3 right  close parenthesis  to the power of 2  Step 4: Simplify algebraically.\n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    left  open parenthesis x to the power of 3 right  close parenthesis   times  left  open parenthesis  2 divided by 3 x to the power of  minus  1 divided by 3  right  close parenthesis   minus   left  open parenthesis x to the power of  2 divided by 3  right  close parenthesis   times  left  open parenthesis 3x to the power of 2 right  close parenthesis  divided by  left  open parenthesis x to the power of 3 right  close parenthesis  to the power of 2 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    2 divided by 3 x to the power of  8 divided by 3   minus  3x to the power of  8 divided by 3  divided by x to the power of 6 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    minus  7 divided by 3 x to the power of  8 divided by 3  divided by x to the power of 6 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals   minus   7 divided by 3 x to the power of  8 divided by 3  divided by x to the power of 6 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals   minus  7 divided by 3x to the power of  10 divided by 3  \n\nThe final answer is:\n\nf' open parenthesis x close parenthesis   equals   minus  7 divided by 3 square root of 3  "", 'Suppose we have an equality optimization problem as follows: Minimize f open parenthesis x, y close parenthesis   equals  x  plus  2y subject to x to the power of 2  plus  y to the power of 2  minus  4  equals  0. While solving the above equation we get x  equals   plus or minus  2 divided by  square root of   , y  equals   plus or minus  4 divided by  square root of   ,  lambda  equals   plus or minus   square root of   divided by 4 . At what value of x and y does the function f open parenthesis x, y close parenthesis  has its minimum value? When x  equals   minus  2 divided by  square root of   , y  equals   minus  4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 ,\n\nf open parenthesis x, y,  lambda  close parenthesis   equals  x  plus  2y  plus   lambda  open parenthesis x to the power of 2  plus  y to the power of 2  minus  4 close parenthesis \n\n equals   minus  2 divided by  square root of     plus  2 open parenthesis  minus  4 divided by  square root of    close parenthesis   plus or minus   square root of   divided by 4  open parenthesis  4 divided by 5   plus   16 divided by 5   minus  4 close parenthesis \n\n equals   minus  2 divided by  square root of     minus   8 divided by  square root of     plus or minus   square root of   divided by 4  open parenthesis  20 divided by 5   minus  4 close parenthesis \n\n equals   minus  10 divided by  square root of     plus or minus   square root of   divided by 4  open parenthesis 4  minus  4 close parenthesis \n\n equals   minus  10 divided by  square root of     plus or minus   square root of   divided by 4   times 0\n\n equals   minus  10 divided by  square root of   \n\nSimilarly when x  equals   2 divided by  square root of   , y  equals   4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 ,\n\nf open parenthesis x, y,  lambda  close parenthesis   equals   10 divided by  square root of   \n\nWhen x  equals   minus  2 divided by  square root of   , y  equals   4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 \n\nf open parenthesis x, y,  lambda  close parenthesis   equals   6 divided by  square root of   \n\nWhen x  equals   2 divided by  square root of   , y  equals   minus  4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 \n\nf open parenthesis x, y,  lambda  close parenthesis   equals   minus  6 divided by  square root of   \n\nSo the function f open parenthesis x, y close parenthesis  has its minimum value  open parenthesis  minus  10 divided by  square root of    close parenthesis  at x  equals   minus  2 divided by  square root of    and y  equals   minus  4 divided by  square root of   .  minus  2 divided by  square root of   ,  minus  4 divided by  square root of     2 divided by  square root of   ,  minus  4 divided by  square root of     minus  2 divided by  square root of   ,  4 divided by  square root of     2 divided by  square root of   ,  4 divided by  square root of   ']"
4,411,4_times equals_times equals times equals_times equals times_equals times equals,"['times equals', 'times equals times equals', 'times equals times', 'equals times equals', 'times 13 equals', 'times 13', 'times 12 equals', 'equals times equals times', '13 equals', '19 times']","['1  times 3  equals  3 three 1  times 3  equals  3', '1 times 3  equals  3 1 times 3  equals 3', '2 times 2  equals  4 2 times 2  equals  4']"
5,201,5_println_method_python_my_list,"['println', 'method', 'python', 'my_list', 'public static void main', 'public static', 'void main', 'main string', 'java code', 'static void main string']","['What is the output of the following Java program? class variable_scope \n        {\n            public static void main(String args[]) \n            {\n                int x;\n                x = 5;\n                {\n    \t               int y = 6;\n    \t               System.out.print(x + "" "" + y);\n                }\n                System.out.println(x + "" "" + y);\n            } \n        }  Compilation Error exception in thread ""main"" java.lang.error: unresolved compilation problem: y cannot be resolved to a variable The second print statement doesn\'t have access to y, scope y was limited to the block defined after initialization of x. Output:\nException in thread ""main"" java.lang.Error: Unresolved compilation problem: y cannot be resolved to a variable', 'What will be the output of the following Java code snippet? import java.util.*;\n       class Arraylists\n       {\n           public static void main(String args[])\n           {\n               ArrayList obj = new ArrayList();\n               obj.add(""A"");\n               obj.add(""B"");\n               obj.add(""C"");\n               obj.add(1, ""D"");\n               System.out.println(obj);\n           }\n       } obj is an object of class ArrayLists hence it is a dynamic array which can increase and decrease its size. obj.add(""X"") adds to the array element X and obj.add(1, ""X"") add element x at index position 1 in the list, Hence obj.add(1, ""D"") stores D at position 1 of obj and shifts the previous value stored at that position by 1 [A, D, C] [A, B, C] [A, B, C, D] [A, D, B, C]', 'What will be the output of the following Java code? class output\n        {\n            public static void main(String args[])\n            { \n               String c = ""Hello i love java"";\n               boolean var;\n               var = c.startsWith(""hello"");\n               System.out.println(var);\n            }\n        } The startsWith() method is case-sensitive; for example, ""hello"" and ""Hello"" are treated as different strings. Therefore, the result of the method is false, which is stored in the variable since the return type of startsWith() is a boolean.\n\nNote: Although var is a keyword used for type inference in Java, It can still be used as an Identifier in earlier versions where var is not a reserved keyword 0 true false 1']"
6,201,6_memory_strength_familiarity_memories,"['memory', 'strength', 'familiarity', 'memories', 'memory strength', 'yonelinas model', 'recognition memory', 'recollection model', 'response', 'remember']","['In the Yonelinas Familiarity-Recollection model, what is the nature of recollection? The Dual-Process Theory of recognition memory, particularly the model proposed by Yonelinas, posits that memory judgments result from two distinct processes: familiarity and recollection . Familiarity is conceptualized as a continuous, signal-detection-like process, reflecting a weak sense of ""knowing"" without specific details. In stark contrast, recollection is defined as the retrieval of specific contextual or episodic details, which the model treats as an all-or-none, threshold-based process . This means that either the rich episodic details are retrieved (success) or they are not (failure); it is not a continuously varying strength, which makes the other options describing gradual increase or continuous strength incorrect for the definition of recollection within this specific model. Recollection is an all-or-none process, either successful or failed. Recollection gradually increases with repeated exposures. Recollection occurs only after familiarity is fully assessed. Recollection only happens for lures and not targets. Recollection is a continuous variable with varying strengths.', 'What is one key difference between the Yonelinas familiarity-recollection model and traditional strength theory? The key difference is that the Yonelinas familiarity-recollection model (also known as the Dual-Process Model) posits that recognition memory is based on two discrete processes: a continuous familiarity signal and a distinct, all-or-none (threshold) recollection process. In contrast, traditional strength theory (e.g., Signal Detection Theory applied to memory) treats recognition as being based on a single, continuous strength-of-memory signal. Therefore, options claiming the Yonelinas model ignores familiarity or treats strength as only binary are incorrect, and traditional strength theory models only one continuous process, not two discrete ones. Yonelinas model includes an all-or-none recollection process alongside familiarity. Traditional strength theory assumes recollection is more important than familiarity. Yonelinas model ignores familiarity in all recognition judgments. Yonelinas model treats strength as a binary state only. Traditional strength theory models two discrete retrieval processes.', 'How do confidence judgments differ from R-K judgments regarding sources of evidence? The Dual-Process Theory of Recognition Memory, posits that recognition is based on two processes: Familiarity (a fast, context-free feeling of knowing) and Recollection (a slower retrieval of specific contextual details). R-K (Remember-Know) judgments are specifically designed to separate these two, with ""Remember"" mapping onto Recollection and ""Know"" mapping onto Familiarity. Confidence judgments, in contrast, are typically holistic ratings of certainty that are influenced by both familiarity and recollection, often weighting familiarity heavily, especially for rapid or low-signal responses. The other options are incorrect: R-K judgments are not random guesses, they are highly related to memory; both judgments do not rely exclusively on familiarity; they are not identical; and confidence judgments reflect more than just recollection signal strength. Confidence judgments may weight familiarity more Confidence judgments only reflect recollection signal strength. R-K judgments are random guesses unrelated to memory. Both judgments rely exclusively on familiarity evidence. Confidence and R-K judgments are identical in evidence weighting.']"
7,190,7_image_question image_question_text equals,"['image', 'question image', 'question', 'text equals', 'text', 'figure', 'equals text', 'question image diagram', 'equals square root', 'equals square']","['Find the circumference of a circle inscribed in a square that has a diagonal of 32 square root of  . When you draw out the circle that is inscribed in a square, you should notice two things. The first thing you should notice is that the diagonal of the square is also the hypotenuse of a right isosceles triangle that has the side lengths of the square as its legs. The second thing you should notice is that the diameter of the circle has the same length as the length of one side of the square.\r\n\r\nFirst, use the Pythagorean theorem to find the length of a side of the square.\r\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\r\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\r\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \r\n\n text   equals   square root of    equals    text  square root of   divided by 2 \r\n\nSubstitute in the length of the diagonal to find the length of the square.\r\n\n text   equals   32 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \r\n\nSimplify.\r\n\n text   equals  32\r\n\nNow, recall the relationship between the diameter of the circle and the side of the square.\r\n text   equals   text   equals  32\r\n\nNow, recall how to find the circumference of a circle.\r\n\n text   equals   pi  times  text \r\n\nSubstitute in the diameter you just found to find the circumference.\r\n\n text   equals  32 pi  32 pi  8 pi  12 pi  4 pi  a circle with an orange line in the middle', 'Find the radius of a circle inscribed in a square with a diagonal of 6 square root of  . Notice that the diagonal of the square is also the hypotenuse of a right isosceles triangle whose legs are also the sides of the square. You should also notice that the diameter of the circle has the same length as that of a side of the square.\n\nIn order to find the radius of the circle, we need to first use the Pythagorean theorem to find the length of the side of the square.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \n\n text   equals   square root of    equals    text  square root of   divided by 2 \n\nNow, substitute in the value of the diagonal to find the length of a side of the square.\n\n text   equals   6 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \n\nSimplify.\n\n text   equals  6\n\nNow keep in mind the following relationship between the diameter and the side of the square:\n\n text   equals   text   equals  6\n\nRecall the relationship between the diameter and the radius.\n\n text   equals   1 divided by 2  open parenthesis  text  close parenthesis \n\nSubstitute in the value of the radius by plugging in the value of the diameter.\n\n text   equals   1 divided by 2  open parenthesis 6 close parenthesis \n\nSolve.\n text   equals  3 6 square root of  . 3 3 square root of  . 6 an image of a circle with a line going through it', 'Find the length of the radius of a circle inscribed in a square that has a diagonal of 10 square root of  . Notice that the diagonal of the square is also the hypotenuse of a right isosceles triangle whose legs are also the sides of the square. You should also notice that the diameter of the circle has the same length as that of a side of the square.\n\nIn order to find the radius of the circle, we need to first use the Pythagorean theorem to find the length of the side of the square.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \n\n text   equals   square root of    equals    text  square root of   divided by 2 \n\nNow, substitute in the value of the diagonal to find the length of a side of the square.\n\n text   equals   10 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \n\nSimplify.\n\n text   equals  10\n\nNow keep in mind the following relationship between the diameter and the side of the square:\n\n text   equals   text   equals  10\n\nRecall the relationship between the diameter and the radius.\n\n text   equals   1 divided by 2  open parenthesis  text  close parenthesis \n\nSubstitute in the value of the radius by plugging in the value of the diameter.\n\n text   equals   1 divided by 2  open parenthesis 10 close parenthesis \n\nSolve.\n\n text   equals  5 10 square root of   5 square root of   5 2 an image of a circle with a yellow line in the middle']"
8,189,8_nerve_lesion_aphasia_year old,"['nerve', 'lesion', 'aphasia', 'year old', 'medulla', 'cranial', 'spinal', 'brain', 'artery', 'trigeminal']","['A 30-year-old man presents with loss of pain and temperature in the right face and left body. The lesion is most likely in the: The most likely location of the lesion is the right lateral medulla. This is a classic presentation of Wallenberg syndrome, or lateral medullary syndrome. The loss of pain and temperature on the right side of the face and left side of the body is caused by damage to the spinothalamic tract and the spinal trigeminal tract in the lateral medulla. These two tracts are responsible for pain and temperature sensation. This specific pattern of crossed sensory loss is a hallmark of a lesion in the right lateral medulla. Right lateral medulla  Left lateral medulla Right medial medulla  Left medial medulla', ""A 45-year-old woman presents with right-sided hemianesthesia and left-sided tongue deviation. The lesion is most likely in the: The left medial medulla is the correct answer. This is because the patient's symptoms point to a lesion in this specific area. The right-sided hemianesthesia (loss of sensation) indicates damage to the medial lemniscus, a sensory pathway that crosses over in the brainstem. The left-sided tongue deviation is a classic sign of damage to the hypoglossal nerve (CN XII), which originates in the left medial medulla. The combination of these two findings is characteristic of medial medullary syndrome, also known as Dejerine syndrome. This syndrome is caused by an infarction (stroke) in the territory of the anterior spinal artery or a penetrating branch of the vertebral artery, which supplies the medial medulla. Left medial medulla  Right medial medulla Left lateral medulla Right lateral medulla"", ""Which structure is essential for consolidation of procedural (implicit) memory? The basal ganglia handles procedural learning and habit formation - skills acquired through practice without conscious awareness. The hippocampus manages declarative memory - facts and events we can consciously recall. These systems work independently but can interact during learning.\n Further Reading:\nThe basal ganglia primarily supports non-declarative memory processes, particularly procedural learning and habit formation. This system enables the gradual acquisition of skills and behaviors through repeated practice, often without conscious awareness of the learning process itself. The basal ganglia is heavily dependent on dopaminergic signaling and specializes in reward-based learning, helping organisms learn to predict outcomes and develop automatic behavioral patterns. When you learn to ride a bicycle, play a musical instrument, or develop diagnostic skills as a radiologist, the basal ganglia is primarily responsible for encoding these procedural memories. In contrast, the hippocampus is essential for declarative memory, enabling the formation of explicit memories for facts and events that can be consciously recalled and verbally expressed. The hippocampus creates rich, contextual representations that allow for flexible memory retrieval and the ability to generalize across different situations. When you remember what you had for breakfast, recall historical facts, or remember the details of a conversation, the hippocampus is primarily responsible for encoding and retrieving these declarative memories. These systems operate largely independently but can interact during certain learning tasks. The functional separation is evident in patients with selective damage to either system - those with hippocampal damage can still learn new procedures despite severe amnesia for facts and events, while those with basal ganglia dysfunction (such as in Parkinson's disease) show deficits in habit learning while maintaining declarative memory abilities. This dissociation demonstrates that the brain has evolved specialized neural circuits optimized for different types of learning and memory demands. Hippocampus Amygdala Basal ganglia  Prefrontal cortex""]"
9,179,9_ridge_ridge regression_lasso_high dimensional,"['ridge', 'ridge regression', 'lasso', 'high dimensional', 'regression', 'training data', 'overfitting', 'selection', 'cross validation', 'regularization']","['In statistical learning, smoothing splines are function estimates,  hat  open parenthesis x close parenthesis , obtained from a set of noisy observations y subscript i of the target f(x_i), in order to balance a measure of goodness of fit of  hat  open parenthesis x subscript i close parenthesis , with a derivative based measure of the smoothness of  hat  open parenthesis x close parenthesis  Smoothing splines are a method in statistical learning used to estimate an unknown function based on noisy data. The goal is to find a function, often represented as  hat  open parenthesis x subscript i close parenthesis , that provides a good balance between two competing objectives: fitting the data closely and maintaining a certain level of smoothness. This balance is achieved by minimizing a cost function that includes a term for goodness of fit, typically the sum of squared errors, and a penalty term that measures the ""roughness"" or smoothness of the function, which is usually based on the integral of its squared second derivative. The smoothing parameter, λ, controls the trade-off between these two terms.', ""R-squared alone is not sufficient to detect overfitting in a model, and other methods and validation techniques should be used. R-squared (or the coefficient of determination) measures the proportion of the variance in the dependent variable that's predictable from the independent variables. In the context of a model, R-squared will always increase or stay the same as you add more predictors, even if those predictors are not meaningful, meaning a high R-squared can be achieved by a model that is heavily overfit to the training data. Overfitting occurs when a model learns the noise and specific details of the training data too well, leading to high performance on training data but poor generalization to new, unseen data. Because R-squared only reflects the fit on the training data, it fails to capture this drop in generalization performance, making it an insufficient metric to detect overfitting. Other options are incorrect as they do not relate to the fundamental limitation of R-squared in model evaluation; for instance, R-squared is designed to measure the goodness-of-fit, not model complexity itself, and therefore cannot alone identify the problem of overfitting."", ""In the context of Partial Least Squares (PLS), what determines the number M of PLS components used in prediction? The number of Partial Least Squares (PLS) components, M, is a hyperparameter that controls the complexity of the model. Choosing the optimal M is critical to achieving a good balance between model bias and variance, avoiding both underfitting and overfitting. This optimal value is not arbitrary and is typically determined using an external validation technique like cross-validation (e.g., K-fold cross-validation or Leave-One-Out Cross-Validation). Cross-validation estimates the prediction error for different values of M, and the value that minimizes this error is selected. The number M is not always fixed to the number of observations or predictors, and while PLS is related to Principal Component Regression (PCR), M is not determined by Principal Component Analysis (PCA) alone, but by optimizing the model's predictive performance. M is chosen by cross-validation as a tuning parameter. The number M is chosen arbitrarily by the researcher. The number M is always equal to the number of observations. The number M is fixed based on the number of predictors. The number M is determined by the principal components analysis.""]"
10,149,10_voting_hyperplane_votes_hard svm,"['voting', 'hyperplane', 'votes', 'hard svm', 'decision', 'multiclass classification', 'classifiers', 'multiclass', 'class', 'ensemble']","['Different algorithms make different assumptions about the data and lead to different classifiers in generating diverse learners. Different algorithms make different assumptions about the data and lead to different classifiers. For example one base – learner may be parametric and another may be nonparametric. When we decide on a single algorithm, we give importance to a single method and ignore all others.', 'Different algorithms make different assumptions about the data and lead to different classifiers in generating diverse learners. Different algorithms make different assumptions about the data and lead to different classifiers. For example one base – learner may be parametric and another may be nonparametric. When we decide on a single algorithm, we give importance to a single method and ignore all others.', 'Given a two-class classification problem with data points x subscript 1  equals   minus 5, x subscript 2  equals  3, x subscript 3  equals  5, having class label +1 and x subscript 4  equals  2 with class label -1. The problem can never be solved using Hard SVM. The given problem is a one dimensional two-class classification problem and the data points are non-linearly separable. So the problem cannot be solved by the Hard SVM directly. But it can be solved using Hard SVM if the one dimensional data set is transformed into a 2-dimensional dataset using some function like (x, x2). Then the problem is linearly separable and can be solved by Hard SVM.']"
11,147,11_war_russia_government_britain,"['war', 'russia', 'government', 'britain', 'british', 'world war', 'united states', 'political', 'american revolution', 'nations']","['During the American Revolution, unanimous approval of the Articles of Confederation was achieved when: The Articles of Confederation, America’s first governing document post-independence, required unanimous consent for amendments and significant decisions. Initially, states held substantial power, especially regarding western lands.\nSignificance of Land Surrender:\n\n1. Centralization of Authority: By surrendering western lands, states acknowledged the need for a centralized government to manage these territories.\n2. Facilitating Expansion: This act helped the U.S. govern newly acquired lands effectively and allowed for future state admissions.\n3. Promoting Unity: Surrendering land interests fostered cooperation among states, enhancing national cohesion and addressing regional rivalries. All states claiming western lands surrendered them to the national government A compromise on slavery was reached States gave up their right to establish tariffs The states gave up their power to print money Three co-equal branches of government were established', '""We [are] determined to save succeeding generations from the scourge of war, which twice in our lifetime has brought untold sorrow to man kind . . ."" this statement comes from which of the following texts? The United Nations was formed at the end of the Second World War and was designed to accomplish what the earlier League of Nations had failed to accomplish: to prevent another world war. the ""Declaration of the Rights of Man and Citizen"" was the founding document of the French Revolution, and Burke\'s work was a critique of that revolution. Bossuet\'s book promoted the theory of ""divine right"". the U.S. Constitution is not explicitly interested in preventing future wars. Charter of the United Nations National Assembly\'s ""Declaration of the Rights of Man and Citizen"" U.S. Constitution Edmund Burke\'s Reflections on the Revolution in France Bishop bossuet\'s Politics Drawn from the Very Words of Holy Scripture', 'The phrase ""Caesar non est supra grammaticos"", translating roughly to ""Caesar/The Emperor is not above grammarians"", originated in an incident involving the Holy Roman Emperor Sigismund at the Council of Constance in 1414 While not believed to be a direct quote from the word of the monks, we do have this source:\n\n""…A similar anecdote is told of the German Emperor Sigismund. When presiding at the Council of Constance, he addressed the assembly in a Latin speech, exhorting them to eradicate the schism of the Hussites. \'Videte Patres,\' he said, \'ut eradicetis schismam Hussitarum.\' He was very unceremoniously called to order by a monk, who called out \'Serenissime Rex, schisma est generis neutri.\' The emperor, however, without losing his presence of mind, asked the impertinent monk, \'How do you know it?\' The old Bohemian schoolmaster replied, \'Alexander Gallus says so.\' \'And who is Alexander Gallus?\' the emperor rejoined. The monk replied, \'He was a monk.\' \'Well,\' said the emperor, \'and I am emperor of Rome; and my word, I trust, will be as good as the word of any monk.\' No doubt the laughers were with the emperor; but for all that, schisma remained a neuter, and not even an emperor could change its gender or termination.""\n\n    Source: The Science of Language, Founded on Lectures Delivered at the Royal Institution in 1861 and 1863 by F. Max Müller, K.M., pp. 39–40. (1899)']"
12,131,12_atomic number_correct order atomic number_sort following elements atomic_correct order atomic,"['atomic number', 'correct order atomic number', 'sort following elements atomic', 'correct order atomic', 'following elements atomic number', 'order atomic number', 'elements atomic number', 'elements atomic', 'order atomic number lowest', 'highest correct order atomic']","['Sort the following elements by atomic number (lowest to highest): The correct order by atomic number (lowest to highest) is:\nRb - Rubidium (Atomic #: 37)\nCe - Cerium (Atomic #: 58)\nTa - Tantalum (Atomic #: 73)\nFl - Flerovium (Atomic #: 114)\nTs - Tennessine (Atomic #: 117) Rb - Rubidium Ce - Cerium Ta - Tantalum Fl - Flerovium Ts - Tennessine', 'Sort the following elements by atomic number (lowest to highest): The correct order by atomic number (lowest to highest) is:\nSc - Scandium (Atomic #: 21)\nMo - Molybdenum (Atomic #: 42)\nCe - Cerium (Atomic #: 58)\nPr - Praseodymium (Atomic #: 59)\nEs - Einsteinium (Atomic #: 99) Sc - Scandium Mo - Molybdenum Ce - Cerium Pr - Praseodymium Es - Einsteinium', 'Sort the following elements by atomic number (lowest to highest): The correct order by atomic number (lowest to highest) is:\nH - Hydrogen (Atomic #: 1)\nCr - Chromium (Atomic #: 24)\nZr - Zirconium (Atomic #: 40)\nAc - Actinium (Atomic #: 89)\nDs - Darmstadtium (Atomic #: 110) H - Hydrogen Cr - Chromium Zr - Zirconium Ac - Actinium Ds - Darmstadtium']"
13,110,13_deep_deep learning_local learning_neural networks,"['deep', 'deep learning', 'local learning', 'neural networks', 'local deep learning', 'local deep', 'deep local learning', 'deep local', 'hidden units', 'synapses']","['Deep targets learning is a subclass of local deep learning where the transmitted target information for weight updates depends on the presynaptic unit. The statement is False because Deep Targets Learning (DTL) is a specific biologically plausible learning rule for deep neural networks, but the core mechanism it describes involves the postsynaptic unit, not the presynaptic unit, determining the transmitted target information. In DTL, the error signal or ""target"" used to update the weights of a layer is derived from the difference between the postsynaptic neuron\'s actual activity and its intended target activity (often derived from a backpropagated error signal). Therefore, the computation of the update depends on the postsynaptic neuron\'s state and its target, violating the statement\'s claim that the target information depends on the presynaptic unit. DTL aims for local computation, but the target information flows backward and influences the postsynaptic unit\'s weight change.', 'What main advantage does a multilayer network have compared to a single hidden layer network? The main advantage of a multilayer (deep) neural network lies in its hierarchical structure, allowing it to compose complex functions from simpler, intermediate representations . While the Universal Approximation Theorem states that a single hidden layer can theoretically model any function, it often requires an exponentially large number of neurons for complex problems, making training computationally intractable and leading to poor generalization. Multilayer networks, by contrast, can achieve the same complexity with fewer overall parameters spread across several modest-sized layers, which simplifies the learning process, improves feature extraction, and makes the model easier to train efficiently. Multiple layers allow building complex functions from modest-sized layers, simplifying learning compared to one very large layer. Multilayers improve interpretability by reducing the number of parameters required. A single hidden layer can model any function exactly but requires exponential computation time. Single-layer networks can only perform linear regression, unable to fit nonlinear data. Multilayers always guarantee perfect fitting whereas single layers do not approximate functions well.', ""What does the fundamental limitation of deep local learning imply about its practical usage? Deep local learning (like purely Hebbian learning) relies only on information available locally at a synapse, typically the activity of the pre- and post-synaptic neurons, to update weights. The fundamental limitation is that it cannot effectively utilize the global error signal (the network's final output error) to adjust weights throughout the network, particularly in deeper layers. In contrast, backpropagation propagates this global error backward. Therefore, deep local learning methods may not find the globally or even locally optimal set of weights to minimize the overall error, making them less effective than backpropagation for training complex, deep neural networks in real-world applications where optimal error minimization is essential. It may not find weights to minimize error optimally in deep networks. It does not depend on local information at synapses. It is the perfect method for all deep networks. It can always replace backpropagation without issues.""]"
14,103,14_darwin_dna_evolution_natural selection,"['darwin', 'dna', 'evolution', 'natural selection', 'watson', 'genetic', 'mendel', 'ray diffraction', 'genetics', 'inheritance']","[""Why were dyes developed in the early 20th century important to DNA studies? Early \\text{DNA}-specific dyes, like the Feulgen stain, were crucial because they directly bind to \\text{DNA}, enabling scientists to visualize it under a microscope within the cell nucleus. . This staining clearly showed that chromosomes, the carriers of hereditary information, were composed largely of this material. Furthermore, the intensity of the dye uptake was observed to correlate directly with the quantity of \\text{DNA} present in a cell's nucleus, which was essential evidence supporting \\text{DNA} as the genetic material. The other options are incorrect as \\text{DNA} staining was relevant, it focused on the nucleus, and the dyes bound to \\text{DNA}, not exclusively to proteins Staining revealed chromosomes that contain most DNA. Dyes bind DNA, allowing visualization under microscope. Dye intensity correlated with DNA quantity in cells. Staining technology was irrelevant to genetic material identification. Early dyes stained cytoplasm more than nuclear material. Dyes bind exclusively to proteins, not DNA, in the nucleus."", ""What role did geology play in Darwin's evolutionary thinking? This is based on the geological theory of Uniformitarianism, championed by Charles Lyell (whose book Darwin read), which proposed that the Earth was shaped by the same slow, continuous processes operating today over vast spans of time. This idea of deep time and gradual change provided the necessary framework for Darwin's theory of gradual evolution through natural selection, suggesting that the small changes observed in individuals could accumulate over immense timescales to produce new species. The other options are incorrect because geology provided the foundation for an old Earth, supported a relationship between changes in the environment and organisms, and Darwin integrated geological principles into his theory. Geology showed Earth's slow changes, supporting Darwin's view of gradual evolution. Earth's geological changes implied no relation to living organisms. Geology disproved slow Earth changes, favoring sudden events. Geology showed Earth was too young for evolution to occur. Darwin rejected geology as irrelevant to biological processes."", ""What experiment provided clear evidence that DNA replication follows a semiconservative mechanism? The Meselson-Stahl experiment provided the definitive evidence for the semiconservative model of DNA replication. They grew E. coli in a heavy nitrogen isotope (\\text{}^{15}\\text{N}) and then transferred them to a light nitrogen isotope (\\text{}^{14}\\text{N}). Using density gradient centrifugation , they showed that after one generation, DNA had an intermediate density, and after two generations, it had two bands (intermediate and light), exactly as predicted by the semiconservative model, where each new DNA molecule consists of one old strand and one new strand. The other experiments, while significant, established different facts: Hershey-Chase identified DNA as the genetic material; Watson-Crick determined the structure; and Avery and Griffith studied bacterial transformation. The Meselson-Stahl experiment used nitrogen isotopes to prove semiconservative replication. Hershey-Chase used bacteriophage labeling to confirm DNA as genetic material. The Watson-Crick model demonstrated DNA forms a double helix structure. Avery's work identified DNA as the molecule responsible for transformation. Griffith's experiment showed that genetic material could transform bacteria types.""]"
15,101,15_odd numbers_select odd numbers odd_numbers odd numbers_numbers odd numbers divisible,"['odd numbers', 'select odd numbers odd', 'numbers odd numbers', 'numbers odd numbers divisible', 'odd numbers divisible', 'select odd numbers', 'select odd', 'odd numbers odd', 'odd numbers odd numbers', 'select numbers numbers divisible']","['Select all the ODD numbers. Odd numbers are not divisible by 2. 2 6 42 72 55 45', 'Select all the ODD numbers. Odd numbers are not divisible by 2. 82 39 61 7 43 5', 'Select all the ODD numbers. Odd numbers are not divisible by 2. 52 5 62 6 7 91']"
16,99,16_game_japan_dynasty_players,"['game', 'japan', 'dynasty', 'players', 'hon inbō', 'played', 'games', 'edo period', 'shogunate', 'greek mythology']","[""How did Go's cultural role differ in China compared to Tibet and Mongolia? In China, Go (Weiqi) was linked with the scholar arts and self-cultivation, forming one of the Four Arts essential for the educated elite, where its strategic depth was valued for cultivating intellect and discipline. In contrast, its role in neighboring regions was often more spiritual or practical: in Tibet, Go was part of shamanic and ritual traditions, and Mongols used Go stones for divination and vision. The other options are incorrect because Go's role in China was profoundly intellectual and esteemed, not only for gambling; it was not solely a competitive sport in Tibet and Mongolia, as the spiritual uses show; and Go has a much longer history in China, predating the 19th century by millennia, so the last option is historically false. Tibet: Go part of shamanic and ritual traditions Mongols used Go stones for divination and vision China: Go linked with scholar arts, self-cultivation China used Go only for gambling and fortune-telling In Tibet and Mongolia, Go was solely played as competitive sport Mongolia introduced Go to China in the 19th century"", 'The Tokugawa shogunate in Japan established four official Go schools and patronized formal competitive play starting from 1700 onwards. The Tokugawa shogunate provided essential official patronage to Go, institutionalizing it through the four official schools—Hon’inbo, Inoue, Yasui, and Hayashi—each led by a hereditary master or iemoto. This system emerged after Tokugawa Ieyasu, an avid Go player, granted salaries to top experts, eventually formalizing their status as government-supported professionals. While the schools were active earlier, the system matured into its most prestigious form by the early 18th century (1700 onwards), characterized by the famous Oshirogo (Castle Go) matches played annually in the presence of the shogun at Edo Castle. Other options claiming Go was only a casual pastime or lacked official structure are incorrect because the shogunate specifically integrated these schools into the national administrative framework, governed under the Magistrate of Temples and Shrines. This high-level patronage transformed Go from a simple game into a formalized, competitive art with deep political and social significance in Edo Japan.', ""How did the opening strategies in Japanese Go evolve during the Edo period? The Edo period (1603-1868) was the Golden Age of Go in Japan, characterized by intense study under the patronage of the Shogunate. This environment fostered great theoretical advancements, including the development of numerous fuseki (whole-board opening patterns) beyond simple corner-first play. Go strategy evolved from older practices, like handicap games with preset stone placements, toward a modern game played on an empty board, requiring more strategic depth. Furthermore, new josekis (corner sequences) and advanced concepts like sacrifice tactics emerged as masters innovated and refined the game's opening theory. Various opening patterns (fuseki) then developed Shifted from preset to empty board starts New josekis and sacrifice tactics emerged Opening strategies remained unchanged from Chinese traditions Players started with all stones placed for gameplay The Edo period discouraged opening strategy experimentation""]"
17,81,17_11_11 12_11 13_10 11 11,"['11', '11 12', '11 13', '10 11 11', '15 10', '17 10', '15 13', '15 16', '16 19', '12 10']","['8 + 5 = ? 8+5 = 13 11 13 12 22 85 14', '9 + 3 = ? 9+3 =12 12 13 93 18 14 11', '3 + 8 = ? 3+8 = 11 11 38 21 13 12 10']"
18,76,18_joint probability_square method_variable_events,"['joint probability', 'square method', 'variable', 'events', 'probability density', 'probability theory', 'marginal probability', 'best fit', 'relationship', 'probability x_i']","['(\\bar{A}) in statistics represents the complement of event A, which is the set of all outcomes in the sample space that not in event A. The bar over the letter A, denoted as \\bar{A}, is the standard notation in probability and statistics for the complement of an event. The complement of an event A includes all possible outcomes in the sample space that are not in A. For example, if the sample space is rolling a standard six-sided die, and event A is rolling an even number (2, 4, 6), then the complement \\bar{A} is rolling an odd number (1, 3, 5). The sum of the probability of an event and its complement is always 1, i.e., P(A) + P(\\bar{A}) = 1. This is because an event and its complement are mutually exclusive and collectively exhaustive, meaning one of them must occur.', ""The equation p(X = x_i, Y = y_j) = p(Y = y_j | X = x_i)p(X = x_i) represents the product rule The product rule defines the joint probability of two events, P(A, B), as the probability of the second event given the first, multiplied by the probability of the first event, i.e., P(A, B) = P(B|A)P(A). The given equation exactly follows this structure for discrete random variables X and Y: the joint probability of X=x_i and Y=y_j, P(X=x_i, Y=y_j), is calculated by multiplying the conditional probability P(Y=y_j | X=x_i) by the marginal probability P(X=x_i). This rule is fundamental for calculating the intersection of dependent events, and it is a rearrangement of the conditional probability formula. Other probability rules like the addition rule or Bayes' theorem have different forms."", 'A probability space in which every outcome has the same probability is called a uniform probability space A uniform probability space is defined as any sample space where every possible outcome has the exact same probability of occurring . For a finite space with N outcomes, the probability of any single outcome E is P(E) = 1/N. This is the simplest and most fundamental type of probability space, often used as a starting point for teaching probability theory, where examples like rolling a fair six-sided die or flipping a fair coin are classic demonstrations of uniformity. Terms like ""marginal"" or ""conditional"" describe specific types of probabilities, not the inherent nature of the entire space.']"
19,72,19_predictors_predictor_regression_covariance,"['predictors', 'predictor', 'regression', 'covariance', 'parametric methods', 'model', 'variables', 'logistic regression', 'dummy', 'interaction term']","['What consequence does collinearity have on t-statistics of predictor coefficients? Collinearity, or multicollinearity, refers to a high correlation between two or more predictor variables in a regression model. This correlation causes the standard errors of the corresponding predictor coefficient estimates ( hat ) to increase. Since the t-statistic is calculated as the coefficient divided by its standard error (t  equals   hat   divided by  SE open parenthesis  hat  close parenthesis ), an increased standard error directly reduces the magnitude of the t-statistic. A smaller t-statistic leads to a higher p-value, making it harder to reject the null hypothesis (H subscript 0:  beta  equals  0) and thus harder to detect a statistically significant effect for the individual predictor, even if the overall model is significant. It reduces t-statistics, making it harder to detect significant effects. It makes t-statistics invalid. It has no effect on t-statistics. It increases t-statistics.', 'How does soft weight sharing differ from traditional weight sharing? Soft weight sharing is a regularization technique in neural networks where weights are encouraged to take values near certain learned means (the cluster centers), typically using a Gaussian prior or penalty term. Unlike traditional (or hard) weight sharing, which forces weights in different parts of the network to be exactly identical (e.g., in convolutional layers), soft weight sharing allows for variations within the groups, promoting clustering instead of strict equality. The other options are incorrect as they misrepresent this core mechanism: soft sharing does not force identical weights, does not ignore weights, and does not involve random weight freezing. Soft weight sharing encourages weights to cluster around learned groups, not exact equality. Soft weight sharing forces all weights to be identical without any clustering. Soft weight sharing ignores weight values and only shares biases among groups. Soft weight sharing freezes weights randomly during training without grouping.', 'What happens to the training mean squared error (MSE) and test set MSE as the number of unrelated features increases in a model? This is the classic symptom of overfitting, where the model becomes overly complex and starts to fit the noise in the training data perfectly, driving the training error down toward zero. However, since the unrelated features do not generalize to new data, the model performs very poorly on the unseen test data, causing the test error MSE to rise dramatically. The other options are incorrect because adding noise does not improve generalization, nor does it cause both errors to rise or fall together in this manner. Training MSE decreases to zero while test MSE increases significantly. Training MSE increases and test set MSE decreases due to better generalization. Both training MSE and test set MSE decrease as more features improve the model. Training MSE and test set MSE both increase due to more complex modeling. Training MSE remains constant while test set MSE becomes zero.']"
20,70,20_scientific_applied science_deductive_inductive reasoning,"['scientific', 'applied science', 'deductive', 'inductive reasoning', 'research', 'deductive reasoning', 'scientific method', 'basic science', 'based science', 'hypothesis based science']","['How does applied science typically define the problem it addresses? Applied science is distinct from pure or basic science because its primary goal is to solve practical, real-world problems using scientific knowledge to achieve a specific outcome, such as developing a new medical treatment, improving technology, or addressing an environmental issue. Therefore, applied research must begin with a specific and clearly defined problem statement to guide its objectives, methodology, and ultimate solution. The other options are incorrect: applied science is characterized by its direct address of practical problems, and it certainly relies on well-defined problem statements to structure its research plan and allocate resources effectively. Applied science has a specific, pre-defined problem focus. Applied science avoids addressing practical problems directly. Applied science disregards problem statements in research planning. Applied science allows researchers complete freedom without problem definition. Applied science defers problem identification until after experiments.', 'What are characteristics of inductive reasoning? Inductive reasoning is the hallmark of descriptive science, as it involves analyzing large volumes of qualitative or quantitative data to identify patterns that lead to broader generalizations. Unlike deductive reasoning, which moves from a broad theory to a single observable event or uses general premises to deduce specific predictions, inductive reasoning works ""bottom-up"" by using specific instances to build a general theory. Consequently, it is not primarily employed in hypothesis-driven experimental science, which typically starts with a hypothesis (a general prediction) and tests it through specific experiments. Thus, the selected options accurately define the directionality and scientific application of the inductive process. It supports descriptive or discovery science methods. It infers general conclusions using many observations. It moves from specific cases to general principles. It is mainly employed in hypothesis-driven experimental science. It moves from a broad theory to a single observable event. It uses general premises to deduce specific predictions.', 'Which statements reflect valid reasons for defining science beyond just the scientific method? they acknowledge that science is a diverse endeavor not always confined to the rigid, laboratory-based ""scientific method."" Many legitimate fields, such as archaeology or astronomy, rely on observational data and inference where exact experimental replication is physically impossible, yet they still provide vital understanding of the natural world. In contrast, stating that science excludes study based on hypotheses or always requires exact repetition is incorrect as it ignores these observational disciplines and the evolving nature of scientific inquiry. Furthermore, non-testable supernatural explanations are universally excluded from science because they cannot be empirically observed or falsified. Hypotheses are sometimes supported without repetition. Fields like archaeology have trouble repeating experiments. Science seeks to broadly comprehend nature’s universe. Science excludes any study based on hypotheses or inference. Science always requires exact experimental repetition. Non-testable supernatural explanations are considered scientific.']"
21,66,21_shell_principal shell_hold maximum electrons_subshell,"['shell', 'principal shell', 'hold maximum electrons', 'subshell', 'amino acid', 'codons', 'genetic code', 'altering reading frame', 'charge open parenthesis delta', 'sickle cell disease caused']","[""A polar covalent bond forms when electrons are shared unequally between atoms, creating partial charges. This unequal sharing occurs because one atom has a higher electronegativity than the other, pulling the shared electrons closer to itself and creating a partial negative charge  open parenthesis  delta  minus  close parenthesis  on that atom, and a partial positive charge  open parenthesis  delta  plus  close parenthesis  on the less electronegative atom. Other types of bonds are incorrect: a nonpolar covalent bond involves equal sharing, an ionic bond involves the complete transfer of electrons, and a metallic bond involves a 'sea' of delocalized electrons."", 'If the insertion or deletion of bases is not a multiple of three, it causes a frame-shift mutation altering the reading frame. It describes a type of gene mutation caused by the insertion or deletion of base pairs in the DNA sequence where the number of bases added or removed is not a multiple of three. Since the genetic code is read in triplets (codons), adding or removing one or two bases shifts the entire sequence of codons downstream from the mutation, fundamentally altering the reading frame. This typically results in a completely new sequence of amino acids being coded for, often introducing a premature stop codon, leading to a non-functional or truncated protein, hence the name frame-shift mutation.', 'The substitution of a single amino acid at position 6 in β-globin causes red blood cells to have defective shape , leading to sickling. structure Sickle Cell disease is caused by a point mutation in the gene encoding the  beta -globin chain of hemoglobin, specifically resulting in the substitution of the amino acid valine for glutamic acid at position 6. This single change makes the hemoglobin molecules prone to clumping when oxygen levels are low, which in turn deforms the red blood cells, giving them a rigid, crescent, or sickle shape instead of the normal biconcave disc shape. This defective shape is what causes the cells to become stuck in capillaries, leading to the symptoms of the disease. While the hemoglobin itself has a defective structure (due to the amino acid change), the direct, visible effect on the red blood cell that leads to ""sickling"" is the change in its overall shape.']"
22,65,22_fibonacci_fibonacci number_fibonacci numbers_numbers,"['fibonacci', 'fibonacci number', 'fibonacci numbers', 'numbers', 'weights', 'binary', 'ij', 'function predictors', 'context', 'th fibonacci number']","[""What are the main processing steps in a two-layer feed-forward neural network? A two-layer feed-forward network, consisting of one hidden layer and one output layer, processes data sequentially. First, the input is transformed by a linear combination of inputs with weights and biases to calculate the net input for the hidden layer . This is immediately followed by a non-linear activation function (like ReLU or sigmoid), performing the nonlinear activation of hidden units. Finally, the hidden layer outputs are subjected to a final linear combination to calculate the output layer's net input, which is then typically followed by a final activation function (like softmax for classification) to produce the model's prediction Nonlinear activation of hidden units. Linear combinations of inputs with weights and biases. Final linear combination to output units followed by activation. Using fixed basis functions. Recurrent connections between layers. Only one linear transformation without nonlinearities."", 'What is an H-tree in recursive graphics? An H-tree is a specific type of geometric fractal constructed by repeating a base shape—a line segment with two shorter perpendicular segments at its ends—at progressively smaller scales. This recursive process ensures that each subsequent branch is a scaled-down version of the previous ""H"" shape, eventually creating a space-filling pattern . The other options are incorrect because a circle drawn inside itself describes a different recursive structure (like a nested circle fractal), while a linear sequence or a random scatter lacks the defining self-similarity and hierarchical ""H"" branching structure fundamental to this specific graphic. Because the algorithm requires each endpoint to become the center of a new, smaller ""H,"" it results in a highly ordered, multi-scale drawing rather than random lines. A recursive drawing made of shapes resembling letter H at different scales. A circle drawn repeatedly inside itself. A linear sequence of lines drawn in a row. A random scatter of lines on a page.', 'What does the equation \\Delta w h_{ij} = F(I h_i, O h_j, O h_{-1j}, w h_{ij}) describe? The weight change  Delta wh subscript ij depends on local information the input to the current neuron i (Ih subscript i), the current output of neuron j (Oh subscript j), the previous output of neuron j (Oh subscript  minus 1j), and the current weight itself (wh subscript ij). This dependency on the local pre- and post-synaptic activations and the current weight is characteristic of local learning rules like Hebbian learning, backpropagation, or their variations. The options suggesting weight decay, independence from activations, or use of only global outputs are incorrect as the formula explicitly uses local neuron activations (Ih subscript i, Oh subscript j) and the synaptic variable (wh subscript ij). A local learning rule using target and synaptic variables for updates. A formula for weight decay during unsupervised training. A fixed update rule independent of neuron activations. A rule that updates weights only using global network outputs.']"
23,63,23_question image_question_ant ant ant ant_white background,"['question image', 'question', 'ant ant ant ant', 'white background', 'hand shown', 'hand shown middle middle', 'middle hand shown middle', 'middle middle hand', 'middle hand', 'middle middle hand shown']","['solve the addition given below one hand has 3 fingers and other has 2 fingers. so it adds up to 6 6 7 5 3 Question image: a hand is shown in the middle and middle of a hand is shown in the middle, and middle of a hand is shown in the middle', 'Solve the addition below one hand has 4 fingers and other hand has 3 fingers, so it adds up to 7 7 8 6 5 Question image: a hand is shown in the middle and middle of a hand is shown in the middle and middle of a hand is shown in the middle of', 'solve the addition below one hand has 2 fingers while other hand also has two fingers, so it adds upto 4 4 5 3 2 Question image: a hand is shown in the middle and middle of a hand is shown in the middle and middle of a hand is shown in the middle and the middle of the middle of the middle of the middle of the middle of the middle of the middle of']"
24,61,24_atomic number highest lowest_number highest lowest correct_order atomic number highest_lowest correct order,"['atomic number highest lowest', 'number highest lowest correct', 'order atomic number highest', 'lowest correct order', 'lowest correct order atomic', 'following elements atomic number', 'correct order atomic', 'correct order atomic number', 'sort following elements atomic', 'order atomic number']","['Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nTh - Thorium (Atomic #: 90)\nNd - Neodymium (Atomic #: 60)\nSn - Tin (Atomic #: 50)\nV - Vanadium (Atomic #: 23)\nB - Boron (Atomic #: 5) Th - Thorium Nd - Neodymium Sn - Tin V - Vanadium B - Boron', 'Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nLv - Livermorium (Atomic #: 116)\nPa - Protactinium (Atomic #: 91)\nTc - Technetium (Atomic #: 43)\nNb - Niobium (Atomic #: 41)\nMn - Manganese (Atomic #: 25) Lv - Livermorium Pa - Protactinium Tc - Technetium Nb - Niobium Mn - Manganese', 'Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nCn - Copernicium (Atomic #: 112)\nTh - Thorium (Atomic #: 90)\nFe - Iron (Atomic #: 26)\nO - Oxygen (Atomic #: 8)\nLi - Lithium (Atomic #: 3) Cn - Copernicium Th - Thorium Fe - Iron O - Oxygen Li - Lithium']"
25,60,25_earthworms_earthworm_pores_nephridia,"['earthworms', 'earthworm', 'pores', 'nephridia', 'worm', 'clitellum', 'intestine', 'species', 'giant axons', 'sperm']","['What is the primary mechanism by which earthworms move through underground soil? Earthworms move via waves of muscular contractions (peristalsis) that work with setae anchoring most body segments, allowing effective underground locomotion. Peristalsis shortens and lengthens the body while setae anchor segments for underground movement. Undulating their whole bodies sideways like snakes as they move in loose soil conditions. Front body appendages provide digging ability to create earthworm tunnels below the surface. Mucus secretion is used alone to dissolve soils in front for simple forward progression.', 'In the classical taxonomic system, what justified placing earthworms in the order Opisthopora? The order Opisthopora is defined by the anatomical relationship in which the male pores open posterior to the female pores, even though the actual male segments are anterior internally. Earthworms have male pores opening behind female pores but males are anterior inside. Earthworms show the prostomium overhanging the mouth only during body movement. Earthworms have setae forming a circular arrangement around each body segment. Earthworms maintain a glandular clitellum located toward the body’s midsection part.', 'Which neural pathway in earthworms is primarily responsible for conducting the fastest emergency reflex escape signals, and what is its direction of conduction? The dorsal giant axon in earthworms conducts the fastest signals from the rear toward the front, initiating efficient emergency escape responses. The dorsal giant axon conducts emergency signals from rear to front rapidly. The lateral nerve plexus moves signals sideways across adjacent segments. The ventral nerve cord sends neural signals mostly from front to back. Medial giant axons transmit emergency signals mainly from rear to front.']"
26,57,26_protractor_triangle_equilateral_triangle sides,"['protractor', 'triangle', 'equilateral', 'triangle sides', 'trapezium', 'quadrilateral', 'right angled', 'parallel sides', 'equilateral triangle', 'equal angles']","['Which triangle has all sides equal? An equilateral triangle is a polygon in which all three sides are equal in length. As a result of this property, all three of its internal angles are also equal, each measuring 60°. This is a fundamental classification of triangles based on their side lengths. In contrast, an isosceles triangle has at least two sides of equal length, while a scalene triangle has all three sides of different lengths. A right-angled triangle is classified by having one angle that measures exactly 90°, and its side lengths may or may not be equal.  Isosceles Equilateral Scalene Right-angled', ""Which instrument is essential for accurately measuring an angle on paper? A protractor is a specialized tool designed specifically for measuring and drawing angles. It typically has a semicircular or circular shape with degree markings from 0° to 180° (or 360°). To measure an angle, a user places the protractor's center point on the angle's vertex and aligns one of the angle's arms with the 0° baseline. The measurement is then read where the other arm of the angle intersects the protractor's scale. Other instruments like a ruler measure length, a divider measures distance between two points, and a set square is used to draw specific angles like 90° or 45°, but none are universally capable of measuring any angle with accuracy like a protractor. Divider Protractor Ruler Set square"", 'To draw a perpendicular to a line from a point not on the line, you use: To construct a perpendicular line from a point not on a given line, you must use a compass and a straightedge (ruler). A compass is used to draw arcs that intersect the line at two points, and these intersections are then used to create two more intersecting arcs. A ruler is then used to connect the original point with the intersection of these new arcs. This method ensures that the constructed line is exactly perpendicular  open parenthesis 90 to the power of ∘  close parenthesis  to the original line, as it relies on a precise geometric construction. Protractor only Ruler only Compass and ruler Set square only']"
27,54,27_consonant vowel vowel vowel_argument_true true_argument structure,"['consonant vowel vowel vowel', 'argument', 'true true', 'argument structure', 'standard way notating argument', 'structure valid form argument', 'structure disjunctive', 'structure disjunctive syllogism', 'structure argument', 'structure valid form']","['Is U a consonant? U is not a consonant, it is a vowel.', 'Is A a consonant? A is not a consonant, it is a vowel.', 'Is O a consonant? O is not a consonant, it is a vowel.']"
28,53,28_recursion_modular programming_calls_modules,"['recursion', 'modular programming', 'calls', 'modules', 'memory', 'module', 'garbage', 'arrays objects', 'memory usage', 'javac']","[""After modular programming, object-oriented programming is the next step in modern Java programming models, extending capabilities further. The statement is generally considered true in the context of Java's evolution because while Object-Oriented Programming (OOP) is the fundamental paradigm of Java itself, the introduction of Modular Programming (the Java Platform Module System, or JPMS, in Java 9) represents a next step in organizing and scaling large applications. Modular programming extends the organizational benefits of OOP by structuring code into named, self-contained modules, improving security, maintainability, and reliability by strictly defining dependencies and encapsulated access, which is a modern enhancement to the core OOP model."", 'Java primitive types like int and double use fixed-size memory representations regardless of runtime environment. The statement is False because Java primitive types like  text  (32-bit signed integer) and  text  (64-bit double-precision float) are defined by the Java Language Specification to have a fixed, platform-independent size . This design choice ensures that Java code, when executed on any Java Virtual Machine (JVM), behaves consistently and predictably, adhering to the principle of ""Write once, run anywhere."" If their sizes were dependent on the runtime environment (like in languages such as C/C++), portability and predictability would be lost.', 'What is a key reason that older non-modular programs are difficult to maintain? Older, non-modular programs are difficult to maintain primarily due to high coupling and low cohesion. When a program lacks modularity, there are no clear boundaries between different parts of the code . This means that variables and data are globally accessible or extensively shared, resulting in a system where a change in one single statement or function can unexpectedly and uncontrollably affect variables and logic used throughout the entire codebase. This pervasive interdependence makes debugging and modifying the program an extremely complex, error-prone, and time-consuming process. Any statement can affect or be affected by any variable in a long sequence of code They are written in a modern language They avoid variables They use too many classes']"
29,50,29_rhymes_rhyme_rhyming_rhyming words,"['rhymes', 'rhyme', 'rhyming', 'rhyming words', 'similar sounds rhyming words', 'similar sounds rhyming', 'similar sounds rhyme', 'sounds rhyme', 'sounds rhyming words', 'sounds rhyming']","['What rhymes with “beep”?\n Sheep and beep both have similar sounds so they are rhyming words. Sheep Dog Wolf Cat', 'What rhymes with “fan”?\n Fan and Pan have similar sounds so they are rhyming words. Pan Light Cat Fat', 'Following the First World War, dress among Westerners became increasingly The late-twentieth-century phenomenon of people traveling, attending church, or even going shopping in casual clothing was unique. Into the 1950s it was unusual for a man to be seen in public in anything other than a coat and tie. The teen cultures of the 1920s and 1950s, which emphasized distinct dress for youth, promoted an increasingly casual approach to dress. casual formal monotone homemade dirty']"
30,48,30_response_deep neural network_learning_quantitative response,"['response', 'deep neural network', 'learning', 'quantitative response', 'language models', 'relationship response', 'task known', 'improved selecting', 'variance error terms', 'qualitative response']","['The formula SE subscript B open parenthesis  hat  close parenthesis  estimates the standard error of  hat  calculated from the original data set using bootstrap samples. initial first primary principal main ??', 'Linear regression assumes that there is a linear relationship between the response Y and the predictors X subscript 1, X subscript 2, ..., X subscript p.  This is correct because the entire premise of linear regression, as implied by its name, is to model the relationship between a response variable (Y) and one or more predictor variables (X subscript 1, X subscript 2,  dots , X subscript p) using a straight-line equation . The model is mathematically expressed as Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, where the predicted response is a linear combination of the predictors. If the true relationship were non-linear (e.g., quadratic or exponential), a linear model would be an inappropriate and inaccurate fit, justifying why ""linear"" is the essential, defining assumption and the only correct answer here.', 'We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as a classification problems. In the field of statistics and machine learning, problems are broadly categorized based on the nature of the output or response variable. When the desired output is a quantitative response (a continuous numerical value, like predicting a stock price or a temperature), the task is known as a regression problem. Conversely, when the output is a qualitative response (a categorical label, such as predicting whether an email is spam or not), the task is known as a classification problem. The answer provided accurately identifies the correct terminology used to distinguish these two fundamental types of predictive modeling problems.']"
31,47,31_naive bayes_naive_knn_high dimensional,"['naive bayes', 'naive', 'knn', 'high dimensional', 'larger margin', 'images', 'skin lesion', 'training', 'pcr ridge', 'regularization']","[""Why might naive Bayes outperform LDA or QDA when the number of predictors is large and data is limited? The Naive Bayes classifier often outperforms more complex models like  text  or  text  when the number of predictors (p) is large and the amount of data (N) is limited because it makes a strong, simplifying assumption: that all predictors are conditionally independent given the class. This assumption drastically reduces the number of parameters that need to be estimated for the class-conditional density functions, thereby reducing the model's variance. This regularization effect is particularly beneficial in high-dimensional, low-sample-size scenarios, preventing the model from overfitting where complex models like  text  would likely fail due to insufficient data to estimate their covariance matrices accurately. Because naive Bayes reduces variance by assuming independence, simplifying density estimation. Because naive Bayes ignores predictor values. Because naive Bayes always uses more data than other methods. Because naive Bayes uses more complex models."", 'Why is cross-validation essential when choosing the tuning parameter in models like the lasso for high-dimensional data? Cross-validation is necessary because the tuning parameter directly controls the bias-variance tradeoff; a value that is too small leads to overfitting (high variance), while one that is too large leads to underfitting (high bias). By evaluating model performance on held-out data, cross-validation identifies the optimal regularization amount that minimizes the expected test error rather than just the training error. Other options are incorrect because the goal of lasso is often to reduce, not maximize, the number of predictors, and cross-validation does not eliminate the need for standardization, which is a separate preprocessing step. Furthermore, it does not minimize training error—which is lowest when there is no regularization—nor does it inherently guarantee the selection of the fewest possible variables, as the ""optimal"" model might require several predictors to remain accurate. It helps select the regularization amount balancing bias and variance. Because it maximizes number of predictors included in the final model. Because it eliminates the need to standardize predictors. Because it always minimizes training error perfectly. Because it guarantees selection of the fewest variables possible.', 'Why is it often unnecessary to compute the constant factor c in the running time expression T(n) ~ c * f(n)? The primary goal of analyzing algorithm efficiency is to determine the asymptotic behavior—how the running time T(n) scales as the input size n approaches infinity, which is captured by the function f open parenthesis n close parenthesis  (the time complexity, e.g., O(n \\log n)). When comparing two algorithms, T subscript 1 open parenthesis n close parenthesis   sim c subscript 1  times f subscript 1 open parenthesis n close parenthesis  and T subscript 2 open parenthesis n close parenthesis   sim c subscript 2  times f subscript 2 open parenthesis n close parenthesis , the one with the slower growth rate (the smaller f open parenthesis n close parenthesis ) will eventually be superior, regardless of the constants c subscript 1 and c subscript 2. The constant factor c captures machine-specific details, such as hardware speed and implementation efficiency, which are ignored in Big O notation as it focuses on the dominant term f open parenthesis n close parenthesis . The other options are incorrect: constants do not dominate performance at large input sizes (the function f open parenthesis n close parenthesis  does); they do depend on algorithm design and implementation; constants are not always zero; and while constants vary with hardware, their irrelevance in Big O is due to the focus on the growth rate, not solely their dependence on hardware. Because constant factors cancel out in relative running time comparisons. Because constants dominate the performance at large input sizes. Because constants depend only on input data, not algorithm design. Because constants always equal zero in running time analyses. Because constants vary with hardware but not with input size.']"
32,44,32_perpendicular_straight line_angles equal_parallelogram,"['perpendicular', 'straight line', 'angles equal', 'parallelogram', 'circle', 'quadrilateral', 'right angles', 'diagonals equal', 'point straight', 'point straight line']","['The shortest distance between a point and a line is: The shortest distance between a point and a line in a two-dimensional space is always a line segment that is perpendicular to the original line. This principle is a fundamental concept in geometry. Think of it like this: if you were standing at a point and wanted to walk the shortest possible distance to a straight road, you would walk straight towards the road at a 90-degree angle. Any other path would be longer because it would involve moving along a hypotenuse of a right-angled triangle. This perpendicular line segment represents the unique shortest path from the point to the line. Any line from the point to the given line  The perpendicular from the point to the line A line parallel to the x-axis A line making 45° with the given line', 'What is the first step in bisecting a straight line segment? The first step in bisecting a line segment with a compass and straightedge is to set your compass to a width that is greater than half the length of the segment. This is essential because it ensures that when you draw arcs from both endpoints of the line, they will intersect at two distinct points. These intersection points are used to draw the perpendicular bisector, which cuts the original line segment into two equal halves. If the compass opening were less than half, the arcs would not cross, making it impossible to find the bisection point. Draw a perpendicular line Use a protractor to measure half the length Open the compass to more than half the length Draw a parallel line', ""Which construction is used to find the midpoint of a line segment? A perpendicular bisector is a line that intersects a line segment at its exact center, or midpoint, and forms a 90-degree angle with it. To construct this, you use a compass to draw intersecting arcs from each endpoint of the line segment. The line drawn through the two points where the arcs intersect is the perpendicular bisector, and where it crosses the original line segment is the midpoint. The other options are incorrect: a tangent touches a circle at one point, a simple circle doesn't find a midpoint, and a parallel line never intersects the original line. Drawing a perpendicular bisector  Drawing a tangent Drawing a circle Drawing a parallel line""]"
33,43,33_memory_recollection_ebbinghaus_items,"['memory', 'recollection', 'ebbinghaus', 'items', 'strength', 'participants', 'indirect', 'repetitions', 'memory tasks', 'memory research']","['What are the key differences between the variable-recollection model and traditional unidimensional strength theory? this model challenges the ""all-or-none"" assumption of traditional dual-process theories. By treating recollection as a continuous, graded signal, it can more accurately model the variability in the quality of retrieved details, which manifests as a bimodal distribution of memory strength. Other options are incorrect because the variable-recollection model does not assume that recollection always fails, but rather that it occurs with varying degrees of success. Furthermore, the model is designed specifically to differentiate between the structures of recollection and familiarity, so it does not treat them identically. Finally, it is the multidimensional variable-recollection model, not the unidimensional strength theory, that accounts for multiple peaks in memory strength distributions. Variable-recollection models graded recollection, not all-or-none It represents recollection as graded like familiarity Explains bimodal target strength with two peaks Variable-recollection model assumes recollection always fails Both models treat recollection and familiarity identically Unidimensional model requires multiple strength peaks', ""What was a critical feature that distinguished Müller's laboratory experiments from Ebbinghaus's? While Ebbinghaus pioneered the experimental study of memory, he primarily served as his own participant, focusing on a single case study. Georg Elias Müller, by contrast, introduced greater experimental control and laboratory rigor to memory studies, including the use of controlled apparatus (like the memory drum for presenting stimuli) and, most critically, testing larger groups of participants. This shift from self-experimentation to studying groups with controlled methods allowed for the calculation of mean effects and enhanced the generalizability and statistical validity of memory research, moving the field towards modern psychological methods. Müller studied large groups with controlled apparatus, improving rigor Müller avoided timing controls and allowed self-paced learning sessions Müller only tested a single participant repeatedly under similar conditions Müller relied solely on introspection without experimental equipment"", ""What role does the standard deviation of strength distributions play in strength theory's predictions for recognition memory? Equal standard deviations predict a symmetric ROC (Receiver Operating Characteristic), as this equal variance assumption simplifies the model and results in an ROC curve that is symmetric around the negative diagonal when plotted on standard axes. However, empirical data often show a non-symmetric ROC, which is why the model is often refined: Unequal variance better models ROC curves observed in real memory experiments, where the distribution for targets is often broader (more variable) than for lures. This leads to the third correct point: More target variability produces zROC slopes less than one when the ROC is plotted in z-score space, because the slope of the zROC is the ratio of the lure standard deviation ( sigma  subscript  text ) to the target standard deviation ( sigma  subscript  text ), and if  sigma  subscript  text   greater than   sigma  subscript  text , the ratio is less than one. The other options are incorrect because standard deviation does have a critical impact on accuracy (it affects overlap); decreased variance narrows the distribution and steepens the ROC curve; and standard deviations do not always match between targets and lures, as the unequal variance model is often preferred. Equal standard deviations predict a symmetric ROC Unequal variance better models ROC curves More target variability produces zROC slopes less than one Standard deviation has no impact on recognition accuracy Decreased variance broadens the ROC curve Standard deviations always match between targets and lures""]"
34,33,34_noble_called deep_previously unseen_helium,"['noble', 'called deep', 'previously unseen', 'helium', 'training mse', 'complete outer', 'learning method', 'outer shell', 'shell', 'membrane proteins']","['The problem with selecting a statistical learning method by minimizing the training MSE is that there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE ??', 'Many RNAs are only a few thousand base pairs long and their errors are less problematic than DNA replication errors partly because RNA errors are not passed to the next generation of daughter cells ??', 'Atoms with a complete outer shell of electrons, like helium and neon, are called inert gases noble gases Atoms like helium and neon are classified as Noble Gases (Group 18 of the periodic table) because they possess a complete outer shell of valence electrons (eight for most, two for helium). This full shell, known as an octet (or duet), makes these elements exceptionally stable and chemically unreactive. Historically, they were called Inert Gases because they were thought to be completely unreactive; although some heavier noble gases can form compounds, both terms correctly describe this group distinguished by their lack of chemical eagerness.']"
35,33,35_class_ignoring smaller_polynomial regression_applying,"['class', 'ignoring smaller', 'polynomial regression', 'applying', 'minority', 'biases', 'largest disc', 'transformations', 'performance', 'running time']","[""What is the purpose of subsampling layers in convolutional neural networks? Subsampling layers, typically implemented as pooling layers (e.g., max pooling or average pooling) , serve two key purposes in a CNN. First, they reduce the spatial dimensions (width and height) of the feature maps, which cuts down the number of parameters and computation required in the network. Second, by summarizing the features over a small area, they introduce a degree of translational invariance (or shift invariance). This means the network's final output will be less sensitive to minor shifts or distortions in the input image, helping the model generalize better. The layers are not for full connectivity, expansion of resolution, or complete removal of the effect of input pixels. To reduce spatial resolution and increase invariance to small input shifts. To fully connect all neurons making the network denser. To completely remove the effect of any input pixel irrespective of location. To expand the spatial resolution increasing sensitivity to minute details."", 'What is the core objective of Support Vector Machines (SVM) in supervised learning? The core objective of a Support Vector Machine (SVM) in supervised learning is to construct a hyperplane (or set of hyperplanes) in a high-dimensional space that is used for classification or regression. Specifically, for classification, the goal is to find the optimal separating hyperplane that has the largest margin —the greatest distance between the hyperplane and the nearest data point of any class. These nearest data points are called the support vectors. Maximizing this margin ensures that the classifier has the best generalization ability. The other options describe different machine learning techniques (like clustering, decision trees, or dimensionality reduction) or incorrect objectives. To find an optimal hyperplane that maximally separates classes. To reduce dimensionality by projecting data into fewer features for easier classification. To cluster data points into multiple groups without supervision using the smallest margin. To generate decision trees that recursively partition data for classification tasks. To identify the mean point in a dataset to determine class boundaries adaptively.', 'What does Knuth suggest programmers need to know to make back-of-the-envelope performance estimates? Donald Knuth, a pioneer in algorithm analysis, advocates for a pragmatic approach to performance estimation, especially for rapid, ""back-of-the-envelope"" analysis. He suggests that programmers should rely on empirical data (such as rough constants or observed times) combined with basic mathematical tools (like polynomial or logarithmic growth) to quickly hypothesize the asymptotic running time growth of an algorithm. This method prioritizes understanding the function\'s scaling behavior O(f(n)) over complex, fine-grained details. Detailed knowledge of assembly-level programming, processor architecture, or operating system internals is necessary for precise runtime analysis, but it is too complex for the quick, estimative nature of back-of-the-envelope calculations. Use empirical data and basic math to hypothesize running time growth. Detailed assembly-level programming skills. Extensive knowledge of processor architecture specifications. Complex real-time hardware simulation outputs. Complete knowledge of the operating system internals.']"
36,30,36_fibonacci_fibonacci numbers_numbers_fibonacci number,"['fibonacci', 'fibonacci numbers', 'numbers', 'fibonacci number', 'fibonacci primes', 'fibonacci sequence', 'strings', 'prime indices', 'modulo', 'indices fibonacci']","[""What does Zeckendorf's theorem state about positive integers and Fibonacci numbers? Zeckendorf's theorem is a fundamental result in number theory stating that any positive integer can be written as the sum of one or more non-consecutive Fibonacci numbers, and this representation is unique . The requirement that the Fibonacci numbers are non-consecutive is crucial for uniqueness. The other options are incorrect statements about the theorem: it applies to all positive integers, not just primes or multiples, and it confirms a unique representation exists. Every positive integer can be uniquely represented as a sum of non-consecutive Fibonacci numbers. Fibonacci numbers can represent only prime integers uniquely. Every positive integer is a multiple of some Fibonacci number. There is no representation of integers using Fibonacci numbers."", 'What is the defining rule of the Fibonacci sequence recurrence relation for n > 1?  The Fibonacci sequence (F subscript n) is defined by the recurrence relation for n  greater than  1 as: F subscript n  equals  F subscript n minus 1  plus  F subscript n minus 2. This means that any number in the sequence (after the first two starting terms, typically F subscript 1 equals 1 and F subscript 2 equals 1 or F subscript 0 equals 0 and F subscript 1 equals 1) is generated by adding the two terms immediately before it. For example, the third term (F subscript 3) is F subscript 2  plus  F subscript 1  equals  1  plus  1  equals  2. The other options describe different types of sequences: a geometric sequence (product of terms), a simple arithmetic sequence (adding 1), or a multiplicative sequence (doubling the previous term), none of which define the Fibonacci sequence. Each term is the sum of the two preceding terms. Each term is the product of the two preceding terms, Fn = Fn−1 × Fn−2. Each term is one more than the previous term, Fn = Fn−1 + 1. Each term is double the previous term, Fn = 2 × Fn−1.', 'How does the Fibonacci sequence relate to the golden ratio as n increases? The Fibonacci sequence, defined by F subscript n  equals  F subscript n minus 1  plus  F subscript n minus 2 with starting values F subscript 1 equals 1 and F subscript 2 equals 1, exhibits a property where the ratio of any term to its immediately preceding term, represented by the limit  limit  subscript n  approaches  infinity   F subscript n divided by F subscript n minus 1 , converges to the Golden Ratio ( phi ) . The Golden Ratio is an irrational number approximately equal to 1.618. The other options are incorrect: the ratio tends towards  phi  (not zero or infinity); and the ratio of alternate terms converges to  phi  to the power of 2  approximately equal to 2.618, not the square root of two ( approximately equal to 1.414). The ratio of two consecutive Fibonacci numbers tends towards the golden ratio. The ratio of two consecutive Fibonacci numbers tends towards zero rapidly. The ratio of alternate Fibonacci numbers tends towards the square root of two. The ratio of two consecutive Fibonacci numbers tends towards infinity as n increases.']"
37,29,37_sum interior angles_interior angles_exterior_360 540,"['sum interior angles', 'interior angles', 'exterior', '360 540', 'exterior angles', 'angles polygon', 'sum exterior angles', 'sum exterior', 'angles sum', 'angles regular']","['What is the sum of the exterior angles of any triangle (one at each vertex)? The sum of the exterior angles of any convex polygon, including a triangle, is always 360 to the power of ∘. An exterior angle is formed by extending one side of the polygon and the adjacent side. This principle holds true regardless of the number of sides the polygon has. For a triangle specifically, the three exterior angles, one at each vertex, will always add up to 360 to the power of ∘. 180°  270° 360° 540°', 'What do we call two angles that sum to 180°? The correct answer is Supplementary angles. Two angles are defined as supplementary if their measures add up to exactly 180 to the power of ∘. A simple way to remember this is that a straight line forms a 180 to the power of ∘ angle, and two angles that form a straight line together are supplementary. In contrast, complementary angles sum to 90 to the power of ∘, while adjacent and vertical angles describe a relationship in position rather than a sum. Supplementary angles Complementary angles Adjacent angles Vertical angles', 'What is the sum of the measures of the four angles of a rectangle? A rectangle is a special type of quadrilateral, which is a polygon with four sides. It has two key properties:\r\n\r\n    All four of its angles are right angles, meaning each angle measures exactly 90 degrees.\r\n\r\n    Its opposite sides are parallel and equal in length.\r\n\r\nSince a rectangle has four right angles, the sum of their measures is:\r\n\r\n90 to the power of ∘ plus 90 to the power of ∘ plus 90 to the power of ∘ plus 90 to the power of ∘ equals 360 to the power of ∘\r\n\r\nThis principle also applies to all quadrilaterals, as the sum of their interior angles is always 360 degrees. 180°  270° 360° 540°']"
