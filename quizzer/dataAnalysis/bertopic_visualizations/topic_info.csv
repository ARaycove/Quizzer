Topic,Count,Name,Representation,Representative_Docs
-1,1012,-1_subscript_dna_cells_parenthesis close,"['subscript', 'dna', 'cells', 'parenthesis close', 'equals', 'genetic', 'rna', 'open parenthesis close', 'networks', 'proteins']","['If\nNumber of hypothesizes |H| = 973\nprobability = 95\nerror  less than  0.1.\n\nFind the minimum number of training examples, m required. m = 99 The probability is given by 1  minus   delta  equals  0.95, so  delta  equals  0.05.\n\nThe error is given by  epsilon  less than  0.1, so  epsilon  equals  0.1.\n\nThe number of hypotheses is |H| = 973.\n\nWe use the PAC learning bound for the number of training examples, which is given by the formula: m \\geq \\frac{1}{\\epsilon} \\left( \\ln|H| + \\ln\\left(\\frac{1}{\\delta}\\right) \\right)\n\nSubstituting the given values:\nm  greater than or equal to  1 divided by 0.1   left  open parenthesis   natural logarithm  open parenthesis 973 close parenthesis   plus   natural logarithm  left  open parenthesis  1 divided by 0.05  right  close parenthesis   right  close parenthesis \nm  greater than or equal to 10  left  open parenthesis   natural logarithm  open parenthesis 973 close parenthesis   plus   natural logarithm  open parenthesis 20 close parenthesis   right  close parenthesis \nm  greater than or equal to 10  open parenthesis 6.88  plus  2.99 close parenthesis \nm  greater than or equal to 10  open parenthesis 9.87 close parenthesis \nm  greater than or equal to 98.7\n\nSince the number of training examples, m, must be an integer, we must round up to the next whole number.\nm  equals  99', ""Two rectangles are similar. One has an area of 20 and the other an area of 80. If the first has a base length of 5, what is the height of the second rectangle? The goal is to solve for the height of the second rectangle.\r\nSimilar rectangles function on proportionality - that is, the ratios of the sides between two rectangles will be the same. In order to determine the height, we will be using this concept of ratios through solving for variables from the area.\r\nFirst, it's helpful to achieve full dimensions for the first rectangle.\r\nIt is given that its base length is 5, and it has an area of 20.\r\nArea  equals  b  times h\r\n20  equals  5  times h\r\nh  equals  4\r\nThis means the first rectangle has the dimensions 5  times 4.\r\nNow, we may utilize the concept of ratios for similarity. The side lengths of the first rectangle is 5  times 4, so the second rectangle must have sides that are proportional to the first's.\r\n base subscript 1 divided by height subscript 1   equals   base subscript 2 divided by height subscript 2 \r\nWe have the information for the first rectangle, so the data may be substituted in.\r\n 4 divided by 5   equals   b subscript 2 divided by h subscript 2 \r\n5b subscript 2  equals  4h subscript 2\r\nb subscript 2  equals   4 divided by 5 h subscript 2\r\nb subscript 2  equals   4 divided by 5 h subscript 2 is the ratio factor that will be used to solve for the height of the second rectangle. This may be substituted into the area formula for the second rectangle.\r\nArea  equals  b  times h\r\n80  equals   left  open parenthesis  4 divided by 5 h right  close parenthesis   times h\r\n 5 divided by 4   times 80  equals  h  times h\r\n100  equals  h to the power of 2\r\n10  equals  h\r\nTherefore, the height of the second rectangle is 10. 10 20 5 30"", 'There are two rectangles. One has a perimeter of 30 and the second one has a perimeter of 50. The first rectangle has a height of 10. If the two rectangles are similiar, what is the base of the second rectangle? The goal of this problem is to figure out what base length of the second rectangle will make it similar to the first rectangle.\r\nSimilar rectangles function on proportionality - that is, the ratios of the sides between two rectangles will be the same. In order to determine the base, we will be using this concept of ratios through solving for variables from the perimeter.\r\nFirst, all the dimensions of the first rectangle must be calculated.\r\nThis can be accomplished through using the perimeter equation: P = 2h + 2b\r\n30  equals  2 open parenthesis 10 close parenthesis   plus  2b\r\n\n30  equals  20  plus  2b\r\n\n10  equals  2b\r\n\n5  equals  b\r\nThis means the dimensions of the first rectangle are 10  times 5. We will use this information for the ratios to calculate dimensions that would yield the second rectangle similar because of proportions.\r\n\n b subscript 1 divided by h subscript 1   equals   b subscript 2 divided by h subscript 2 \r\n\n 5 divided by 10   equals   b subscript 2 divided by h subscript 2 \r\n\n10b subscript 2  equals  5h subscript 2\r\n\nh subscript 2  equals  2b subscript 2\r\n\nh subscript 2  equals  2b subscript 2 is the ratio factor we will use to solve for the base of the second rectangle.\r\nThis will require revisiting the perimeter equation for the second rectangle.\r\n\nP = 2h + 2b\r\n\n50  equals  2 open parenthesis 2b close parenthesis   plus  2b\r\n\n50  equals  4b  plus  2b\r\n\n50  equals  6b\r\n\n 50 divided by 6   equals  b\r\n\nb  equals  8.3 8.3 8 9 9.3']"
0,1027,0_cell_dna_division_proteins,"['cell', 'dna', 'division', 'proteins', 'replication', 'mitosis', 'meiosis', 'cell division', 'carbon', 'chromatids']","['Which statements describe the Warburg effect and its enzymatic regulation in cancer cells? they accurately describe the metabolic reprogramming of cancer cells to favor aerobic glycolysis. The Warburg effect is defined by high rates of glucose-to-lactate conversion despite the presence of oxygen, a process mediated by HIF1 triggering PDK1 to block the pyruvate dehydrogenase (PDH) complex and divert pyruvate away from the mitochondria. Additionally, the low-activity PKM2 dimer allows glycolytic intermediates to accumulate for biosynthetic pathways rather than being fully oxidized for energy. In contrast, cancer cells do not rely primarily on oxidative phosphorylation under hypoxia, as they shift toward anaerobic processes, and p53 actually suppresses glycolysis by activating TIGAR rather than enhancing it. Glucose is metabolized to lactate even with oxygen (Warburg effect) HIF1 upregulates PDK1, inhibiting the PDH complex PKM2 dimer reduces pyruvate kinase activity, supports biosynthesis Cancer cells rely primarily on oxidative phosphorylation under hypoxia p53 enhances glycolysis by suppressing TIGAR LDHA downregulation supports lactate production in tumors', ""Why can't blending inheritance explain Mendel's observed results? \r\nBlending inheritance, the pre-Mendelian idea that offspring traits are simply an average of the parents' traits, cannot explain Mendel's observed results because his experiments showed that Traits reappear after missing in F1, Dominant traits mask the recessive ones, and Traits segregate in predictable ratios. If blending were correct, the F1 generation from a cross between pure purple and pure white flowers would only produce an intermediate, light purple shade, and the original traits would be lost permanently, which contradicts the reappearance of white flowers in the F2 generation. Mendel's results, instead, support the idea of particulate inheritance, where hereditary factors (genes/alleles) remain discrete, explaining why the dominant trait only masks the recessive one in F1, and why traits then segregate (separate) into predictable 3:1 ratios in F2. The options about random change or offspring traits being averages directly contradict the Mendelian principles and the observed data, while the first three options are the very observations that disproved the blending hypothesis. Traits reappear after missing in F1 Dominant traits mask the recessive ones Traits segregate in predictable ratios Traits change randomly over generations Offspring traits are averages of parents All traits mix to intermediate form"", 'Which of the following are required for a cell to divide? For a cell to successfully divide into two viable daughter cells, several key events are absolutely required. First, cell division signals must be present to initiate the process, ensuring division happens at the right time. Then, the cell must undergo DNA replication to accurately duplicate its genetic material, ensuring both new cells receive a complete copy. Following replication, the duplicated chromosomes must be carefully and equally distributed to opposite poles through DNA segregation (mitosis or meiosis). Finally, the cell must divide the rest of its contents, including the cytoplasm and organelles, through cytokinesis. The unchecked options, cell wall thickening (only relevant to certain plant/fungal cells) and mitochondrial division (an important but separate event) are not the four fundamental processes necessary for all cell division. Cytokinesis to divide cytoplasm Cell division signals must initiate division DNA replication to duplicate genetic material DNA segregation to distribute chromosomes Cell wall thickening Mitochondrial division Protein synthesis only Nutrient absorption']"
1,917,1_data_regression_trees_variable,"['data', 'regression', 'trees', 'variable', 'classification', 'decision', 'overfitting', 'statements true', 'bias', 'models']","['What practical advantage does k-fold CV have when fitting computationally intensive models? k-fold cross-validation divides the dataset into a fixed number of subsets (k), necessitating only k model fits (typically 5 or 10), whereas Leave-One-Out Cross-Validation (LOOCV) must fit the model n times for a dataset of size n. For computationally intensive models, this reduction in the number of training cycles is a critical practical advantage. In contrast, it does not result in perfect predictions, as all cross-validation methods provide estimates of generalization error rather than eliminating error entirely. It definitely needs validation data, as the entire purpose is to test the model on held-out ""folds"" to prevent overfitting. Finally, it actually uses smaller training sets than LOOCV, since LOOCV trains on n minus 1 points while k-fold trains on approximately [ open parenthesis k minus 1 close parenthesis  divided by k]  times n points. It requires fitting the model fewer times than LOOCV, saving time It results in perfect predictions It needs no validation data It uses larger training sets than LOOCV', 'Relating to Machine Learning - Which of the following statements is not true about boosting? Boosting does not increase the bias and variance but it mainly reduces the bias and the variance. It is a technique for solving two-class classification problems. And it tries to generate complementary base-learners by training the next learner (by increasing the weights) on the mistakes (misclassified data) of the previous learners It mainly increases the bias and the variance It tries to generate complementary base-learning by training the next learner on the mistakes of the previous learners It is a technique for solving two-class classification problems it uses the mechanism of increasing the weights of misclassified data in preceding classifiers', 'What is the role of cross-validation in determining the optimal tree size during pruning? Cross-validation is utilized in decision tree pruning to provide a statistically sound estimate of the model\'s performance on unseen data, allowing the algorithm to identify a subtree that minimizes error without overfitting. This highlights the necessary trade-off between a tree that is too complex (high variance) and one that is too simple (high bias). While cross-validation does not eliminate qualitative variables, nor does it increase tree size without limit, which would exacerbate overfitting. Additionally, it does not remove all internal nodes, as that would result in a useless model, nor can it compute the ""exact"" test error on future data, only an estimate based on available samples. Consequently, cross-validation serves as a critical tuning mechanism to ensure the resulting pruned tree generalizes effectively to new observations. It estimates test error to select a subtree balancing accuracy and complexity It eliminates qualitative variables from the predictor set It increases the tree size without limit for better training fit It removes all internal nodes set by the splitting function It directly computes the exact test error on unseen data']"
2,412,2_times equals_times equals times equals_equals times equals_times equals times,"['times equals', 'times equals times equals', 'equals times equals', 'times equals times', 'times 13 equals', 'times 12 equals', 'times 13', 'equals times equals times', '13 equals', '19 times']","['1 times 4  equals  4 1 times 4  equals 4', '1  times 2  equals  2 two 1  times 2  equals  2', '4 times 2  equals  8 4 times 2  equals  8']"
3,394,3_bonds_atom_covalent_bond,"['bonds', 'atom', 'covalent', 'bond', 'hydrogen', 'covalent bonds', 'phospholipids', 'water molecules', 'phosphate', 'ions']","[""The hydrophobic lipid groups insert into the phospholipid bilayer and hold membrane proteins in association with the membrane. A phospholipid bilayer is a double layer of lipid molecules that forms the cell membrane. It's composed of amphipathic molecules, meaning each has a hydrophilic (water-loving) head and two hydrophobic (water-fearing) tails. In the watery environment inside and outside the cell, these molecules naturally arrange themselves to protect the hydrophobic tails from water. They form a double layer where the hydrophilic heads face outward towards the water, and the hydrophobic tails face inward, packed against each other. This structure is the most stable and energetically favorable arrangement, creating a critical barrier for the cell."", 'Hydrogen bonding in DNA base pairs is not essential for specific binding of proteins to DNA sequences. hydrogen bonds formed between a protein\'s amino acid side chains and the exposed functional groups (donors and acceptors) in the major and minor grooves of the DNA helix are absolutely essential for the specificity of DNA-binding proteins. These weak, non-covalent interactions, along with van der Waals forces and ionic bonds, allow the protein to ""read"" the unique chemical pattern of a specific base sequence, ensuring it only binds to its designated target site, like a transcription factor binding to a specific promoter or operator sequence. The stability of the overall protein-DNA complex relies on the sum of these many specific hydrogen bonds.', 'Which chemical components and structure characterize DNA? DNA is a macromolecule defined by its components and structure . It is a polymer of nucleotide monomers where each monomer consists of a deoxyribose sugar, a phosphate group, and one of four nitrogenous bases (adenine, thymine, cytosine, or guanine). The individual nucleotides are covalently joined by phosphodiester bonds, which link the phosphate group of one nucleotide to the sugar of the next, forming the sugar-phosphate backbone, meaning the phosphate and nitrogen bases covalently link nucleotides (via the sugar-phosphate backbone). Phosphate and nitrogen bases covalently link nucleotides. DNA is a polymer of nucleotide monomers with deoxyribose. DNA contains nitrogenous bases: adenine, thymine, cytosine, guanine. DNA uses uracil instead of thymine as a base. DNA contains ribose sugars instead of deoxyribose sugars. DNA is composed chiefly of amino acids joined by peptide bonds.']"
4,211,4_sort following elements atomic_atomic number highest_correct order atomic_correct order atomic number,"['sort following elements atomic', 'atomic number highest', 'correct order atomic', 'correct order atomic number', 'following elements atomic', 'following elements atomic number', 'order atomic', 'order atomic number', 'elements atomic number', 'elements atomic']","['Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nDs - Darmstadtium (Atomic #: 110)\nMt - Meitnerium (Atomic #: 109)\nBi - Bismuth (Atomic #: 83)\nSm - Samarium (Atomic #: 62)\nPm - Promethium (Atomic #: 61) Ds - Darmstadtium Mt - Meitnerium Bi - Bismuth Sm - Samarium Pm - Promethium', 'Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nDs - Darmstadtium (Atomic #: 110)\nRf - Rutherfordium (Atomic #: 104)\nLr - Lawrencium (Atomic #: 103)\nGd - Gadolinium (Atomic #: 64)\nPm - Promethium (Atomic #: 61) Ds - Darmstadtium Rf - Rutherfordium Lr - Lawrencium Gd - Gadolinium Pm - Promethium', 'Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nHs - Hassium (Atomic #: 108)\nBi - Bismuth (Atomic #: 83)\nPt - Platinum (Atomic #: 78)\nGd - Gadolinium (Atomic #: 64)\nPm - Promethium (Atomic #: 61) Hs - Hassium Bi - Bismuth Pt - Platinum Gd - Gadolinium Pm - Promethium']"
5,198,5_java_operator_println_python,"['java', 'operator', 'println', 'python', 'my_list', 'public static void main', 'main string', 'public static', 'void main string', 'static void main string']","['What is the output of the following Java program? class variable_scope \n        {\n            public static void main(String args[]) \n            {\n                int x;\n                x = 5;\n                {\n    \t               int y = 6;\n    \t               System.out.print(x + "" "" + y);\n                }\n                System.out.println(x + "" "" + y);\n            } \n        }  Compilation Error exception in thread ""main"" java.lang.error: unresolved compilation problem: y cannot be resolved to a variable The second print statement doesn\'t have access to y, scope y was limited to the block defined after initialization of x. Output:\nException in thread ""main"" java.lang.Error: Unresolved compilation problem: y cannot be resolved to a variable', 'What will be the output of the following Java code snippet? import java.util.*;\n       class Arraylists\n       {\n           public static void main(String args[])\n           {\n               ArrayList obj = new ArrayList();\n               obj.add(""A"");\n               obj.add(""B"");\n               obj.add(""C"");\n               obj.add(1, ""D"");\n               System.out.println(obj);\n           }\n       } obj is an object of class ArrayLists hence it is a dynamic array which can increase and decrease its size. obj.add(""X"") adds to the array element X and obj.add(1, ""X"") add element x at index position 1 in the list, Hence obj.add(1, ""D"") stores D at position 1 of obj and shifts the previous value stored at that position by 1 [A, D, C] [A, B, C] [A, B, C, D] [A, D, B, C]', 'What will be the output of the following Java code? class newthread extends Thread\n       {\n    \tThread t1, t2;\n    \tnewthread()\n           {\n    \t    t1 = new Thread(this,""Thread_1"");\n    \t    t2 = new Thread(this,""Thread_2"");\n    \t    t1.start();\n    \t    t2.start();\n    \t}\n    \tpublic void run()\n           {\n    \t           t2.setPriority(Thread.MAX_PRIORITY);\t\n    \t           System.out.print(t1.equals(t2));\n           }   \n       }\n       class multithreaded_programing\n       {\n           public static void main(String args[])\n           {\n               new newthread();       \n           }\n       } Threads t1 and t2 are created by the NewThread class, which implements the Runnable interface. therefore, both threads have their own run() methods defining the actions to be performed. When the constructor of the NewThread class is invoked, the run() method of t1 executes first, followed by the run() method of t2. Each execution prints false because the two threads are not equal - one has a different priority than the other. As a result, the output is falsefalse truetrue falsefalse true false']"
6,196,6_question image_question_text equals_area,"['question image', 'question', 'text equals', 'area', 'equals text', 'equals square root', 'equals square', 'question image diagram', 'image diagram', 'question image image']","['Find the circumference of a circle inscribed in a square that has a diagonal of 32 square root of  . When you draw out the circle that is inscribed in a square, you should notice two things. The first thing you should notice is that the diagonal of the square is also the hypotenuse of a right isosceles triangle that has the side lengths of the square as its legs. The second thing you should notice is that the diameter of the circle has the same length as the length of one side of the square.\r\n\r\nFirst, use the Pythagorean theorem to find the length of a side of the square.\r\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\r\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\r\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \r\n\n text   equals   square root of    equals    text  square root of   divided by 2 \r\n\nSubstitute in the length of the diagonal to find the length of the square.\r\n\n text   equals   32 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \r\n\nSimplify.\r\n\n text   equals  32\r\n\nNow, recall the relationship between the diameter of the circle and the side of the square.\r\n text   equals   text   equals  32\r\n\nNow, recall how to find the circumference of a circle.\r\n\n text   equals   pi  times  text \r\n\nSubstitute in the diameter you just found to find the circumference.\r\n\n text   equals  32 pi  32 pi  8 pi  12 pi  4 pi  a circle with an orange line in the middle', 'Find the length of the radius of a circle inscribed in a square that has a diagonal of 10 square root of  . Notice that the diagonal of the square is also the hypotenuse of a right isosceles triangle whose legs are also the sides of the square. You should also notice that the diameter of the circle has the same length as that of a side of the square.\n\nIn order to find the radius of the circle, we need to first use the Pythagorean theorem to find the length of the side of the square.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \n\n text   equals   square root of    equals    text  square root of   divided by 2 \n\nNow, substitute in the value of the diagonal to find the length of a side of the square.\n\n text   equals   10 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \n\nSimplify.\n\n text   equals  10\n\nNow keep in mind the following relationship between the diameter and the side of the square:\n\n text   equals   text   equals  10\n\nRecall the relationship between the diameter and the radius.\n\n text   equals   1 divided by 2  open parenthesis  text  close parenthesis \n\nSubstitute in the value of the radius by plugging in the value of the diameter.\n\n text   equals   1 divided by 2  open parenthesis 10 close parenthesis \n\nSolve.\n\n text   equals  5 10 square root of   5 square root of   5 2 an image of a circle with a yellow line in the middle', 'Find the radius of a circle inscribed in a square with a diagonal of 6 square root of  . Notice that the diagonal of the square is also the hypotenuse of a right isosceles triangle whose legs are also the sides of the square. You should also notice that the diameter of the circle has the same length as that of a side of the square.\n\nIn order to find the radius of the circle, we need to first use the Pythagorean theorem to find the length of the side of the square.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \n\n text   equals   square root of    equals    text  square root of   divided by 2 \n\nNow, substitute in the value of the diagonal to find the length of a side of the square.\n\n text   equals   6 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \n\nSimplify.\n\n text   equals  6\n\nNow keep in mind the following relationship between the diameter and the side of the square:\n\n text   equals   text   equals  6\n\nRecall the relationship between the diameter and the radius.\n\n text   equals   1 divided by 2  open parenthesis  text  close parenthesis \n\nSubstitute in the value of the radius by plugging in the value of the diameter.\n\n text   equals   1 divided by 2  open parenthesis 6 close parenthesis \n\nSolve.\n text   equals  3 6 square root of  . 3 3 square root of  . 6 an image of a circle with a line going through it']"
7,191,7_nerve_lesion_aphasia_year old,"['nerve', 'lesion', 'aphasia', 'year old', 'medulla', 'spinal', 'brain', 'artery', 'trigeminal', 'cerebellum']","['A 30-year-old man presents with loss of pain and temperature in the right face and left body. The lesion is most likely in the: The most likely location of the lesion is the right lateral medulla. This is a classic presentation of Wallenberg syndrome, or lateral medullary syndrome. The loss of pain and temperature on the right side of the face and left side of the body is caused by damage to the spinothalamic tract and the spinal trigeminal tract in the lateral medulla. These two tracts are responsible for pain and temperature sensation. This specific pattern of crossed sensory loss is a hallmark of a lesion in the right lateral medulla. Right lateral medulla  Left lateral medulla Right medial medulla  Left medial medulla', ""A 45-year-old woman presents with right-sided hemianesthesia and left-sided tongue deviation. The lesion is most likely in the: The left medial medulla is the correct answer. This is because the patient's symptoms point to a lesion in this specific area. The right-sided hemianesthesia (loss of sensation) indicates damage to the medial lemniscus, a sensory pathway that crosses over in the brainstem. The left-sided tongue deviation is a classic sign of damage to the hypoglossal nerve (CN XII), which originates in the left medial medulla. The combination of these two findings is characteristic of medial medullary syndrome, also known as Dejerine syndrome. This syndrome is caused by an infarction (stroke) in the territory of the anterior spinal artery or a penetrating branch of the vertebral artery, which supplies the medial medulla. Left medial medulla  Right medial medulla Left lateral medulla Right lateral medulla"", ""Which structure is essential for consolidation of procedural (implicit) memory? The basal ganglia handles procedural learning and habit formation - skills acquired through practice without conscious awareness. The hippocampus manages declarative memory - facts and events we can consciously recall. These systems work independently but can interact during learning.\n Further Reading:\nThe basal ganglia primarily supports non-declarative memory processes, particularly procedural learning and habit formation. This system enables the gradual acquisition of skills and behaviors through repeated practice, often without conscious awareness of the learning process itself. The basal ganglia is heavily dependent on dopaminergic signaling and specializes in reward-based learning, helping organisms learn to predict outcomes and develop automatic behavioral patterns. When you learn to ride a bicycle, play a musical instrument, or develop diagnostic skills as a radiologist, the basal ganglia is primarily responsible for encoding these procedural memories. In contrast, the hippocampus is essential for declarative memory, enabling the formation of explicit memories for facts and events that can be consciously recalled and verbally expressed. The hippocampus creates rich, contextual representations that allow for flexible memory retrieval and the ability to generalize across different situations. When you remember what you had for breakfast, recall historical facts, or remember the details of a conversation, the hippocampus is primarily responsible for encoding and retrieving these declarative memories. These systems operate largely independently but can interact during certain learning tasks. The functional separation is evident in patients with selective damage to either system - those with hippocampal damage can still learn new procedures despite severe amnesia for facts and events, while those with basal ganglia dysfunction (such as in Parkinson's disease) show deficits in habit learning while maintaining declarative memory abilities. This dissociation demonstrates that the brain has evolved specialized neural circuits optimized for different types of learning and memory demands. Hippocampus Amygdala Basal ganglia  Prefrontal cortex""]"
8,184,8_ridge_ridge regression_lasso_regression,"['ridge', 'ridge regression', 'lasso', 'regression', 'cross validation', 'overfitting', 'regularization', 'selection', 'high dimensional', 'high variance']","['The bias–variance problem describes the difficulty in minimizing both bias and variance errors simultaneously in supervised learning. The bias-variance tradeoff is a fundamental concept in supervised machine learning that describes the difficulty in simultaneously minimizing two sources of error that prevent a model from generalizing well to new data: bias and variance. Bias is the error introduced by approximating a real-world problem, which may be complicated, by a much simpler model (underfitting). Variance is the error introduced by the model\'s excessive sensitivity to small fluctuations in the training data (overfitting). As model complexity is increased, bias typically decreases while variance increases, and vice versa; thus, a successful model requires finding the optimal balance or ""sweet spot"" between these two competing sources of error. The difficulty in achieving this simultaneous minimization is precisely what the ""bias-variance problem"" or ""tradeoff"" refers to, validating the statement.', ""The out-of-bag (OOB) error estimate provides a convenient way to evaluate the test error of a bagged model without cross-validation. The out-of-bag (OOB) error estimate is a distinctive and highly efficient feature of bagging algorithms, such as Random Forests. Bagging involves training each base learner on a bootstrap sample (sampling with replacement) of the training data. Consequently, for any given base learner, approximately one-third of the original training data samples are left out of its training set; these are the OOB samples. The OOB error is calculated by using each data point's OOB samples to obtain an aggregated prediction, and then calculating the error based on these predictions.  This effectively provides a cross-validated-like estimate of the model's generalization error without the need for setting aside a separate validation set or performing explicit k-fold cross-validation, thus making it a convenient and cost-effective method for estimating the test error."", ""In the context of Partial Least Squares (PLS), what determines the number M of PLS components used in prediction? The number of Partial Least Squares (PLS) components, M, is a hyperparameter that controls the complexity of the model. Choosing the optimal M is critical to achieving a good balance between model bias and variance, avoiding both underfitting and overfitting. This optimal value is not arbitrary and is typically determined using an external validation technique like cross-validation (e.g., K-fold cross-validation or Leave-One-Out Cross-Validation). Cross-validation estimates the prediction error for different values of M, and the value that minimizes this error is selected. The number M is not always fixed to the number of observations or predictors, and while PLS is related to Principal Component Regression (PCR), M is not determined by Principal Component Analysis (PCA) alone, but by optimizing the model's predictive performance. M is chosen by cross-validation as a tuning parameter. The number M is chosen arbitrarily by the researcher. The number M is always equal to the number of observations. The number M is fixed based on the number of predictors. The number M is determined by the principal components analysis.""]"
9,173,9_familiarity_items_yes_strength theory,"['familiarity', 'items', 'yes', 'strength theory', 'memories', 'yonelinas model', 'remember', 'recollection model', 'familiarity recollection', 'response']","['According to the Yonelinas model, how do recollection and familiarity influence recognition responses? The Dual-Process Model of Recognition Memory, proposed by Yonelinas, posits that recognition is based on two independent processes: recollection (slow, effortful retrieval of contextual details) and familiarity (fast, effortless feeling of knowing). A recognition response (""yes, I saw that before"") is triggered if either the recollection process is successful or the familiarity signal exceeds a threshold. Crucially, if recollection occurs, it provides a high-confidence, definitive response that overrides or bypasses the need for the slower familiarity judgment, leading immediately to a ""yes"" response. This is often referred to as a high-threshold process for recollection, which takes precedence over the continuous familiarity signal. Recollection responses override familiarity responses to produce a \'yes.\' Recollection and familiarity signals are averaged before deciding. Recollection is used only if familiarity is below a certain threshold. Only familiarity determines responses; recollection is ignored. Familiarity always overrides recollection in recognition judgments.', 'What does the Yonelinas familiarity-recollection model assume about the relationship between recollection and familiarity during recognition? The Yonelinas Familiarity-Recollection Model (often called the Dual-Process Model of Recognition Memory) proposes that recognition is based on two distinct processes: a fast, automatic process called familiarity (a feeling of knowing), and a slower, effortful process called recollection (retrieval of specific details). The model assumes a threshold, all-or-none role for recollection; if recollection succeeds, it is sufficient for the ""yes"" response and overrides any familiarity signal. If recollection fails, the decision falls back to a familiarity-based signal-detection process. Thus, the three selected options correctly capture the core tenets of the model: recollection is all-or-none and overrides familiarity, when recollection fails, the decision uses familiarity, and consequently, if recollection occurs, the participant answers ""yes"". The other options, like familiarity always overriding recollection or the processes combining into a single scalar value, are contrary to the distinct, hierarchical nature of this model. If recollection occurs, participant answers ""yes"" When recollection fails, decision uses familiarity Recollection is all-or-none and overrides familiarity Familiarity always overrides recollection in responses Participants respond ""no"" when recollection and familiarity conflict Recollection and familiarity are combined into a single scalar value', 'How do confidence judgments differ from R-K judgments regarding sources of evidence? The Dual-Process Theory of Recognition Memory, posits that recognition is based on two processes: Familiarity (a fast, context-free feeling of knowing) and Recollection (a slower retrieval of specific contextual details). R-K (Remember-Know) judgments are specifically designed to separate these two, with ""Remember"" mapping onto Recollection and ""Know"" mapping onto Familiarity. Confidence judgments, in contrast, are typically holistic ratings of certainty that are influenced by both familiarity and recollection, often weighting familiarity heavily, especially for rapid or low-signal responses. The other options are incorrect: R-K judgments are not random guesses, they are highly related to memory; both judgments do not rely exclusively on familiarity; they are not identical; and confidence judgments reflect more than just recollection signal strength. Confidence judgments may weight familiarity more Confidence judgments only reflect recollection signal strength. R-K judgments are random guesses unrelated to memory. Both judgments rely exclusively on familiarity evidence. Confidence and R-K judgments are identical in evidence weighting.']"
10,158,10_open parenthesis_power_parenthesis power_parenthesis plus close parenthesis,"['open parenthesis', 'power', 'parenthesis power', 'parenthesis plus close parenthesis', 'open parenthesis plus close', 'parenthesis minus close parenthesis', 'parenthesis minus close', 'open parenthesis minus close', 'open parenthesis minus', 'open parenthesis plus']","['Factor completely:\r\nx to the power of 4  minus  16 Difference of squares: x to the power of 4  minus  16  equals   open parenthesis x to the power of 2 close parenthesis  to the power of 2  minus  4 to the power of 2  open parenthesis x to the power of 2  minus  4 close parenthesis  open parenthesis x to the power of 2  plus  4 close parenthesis   open parenthesis x  minus  4 close parenthesis  open parenthesis x  plus  4 close parenthesis  open parenthesis x to the power of 2  plus  4 close parenthesis   open parenthesis x  minus  2 close parenthesis  open parenthesis x  plus  2 close parenthesis  open parenthesis x to the power of 2  plus  4 close parenthesis   Prime', 'Solve for y.\r\n\ny  minus  7  equals   4x  plus  6y divided by 3  y  minus  7  equals   4x  plus  6y divided by 3 \r\nMultiply both sides by 3:\r\n\n 3 divided by 1  open parenthesis y  minus  7 close parenthesis   equals   open parenthesis  4x  plus  6y divided by 3  close parenthesis  3 divided by 1 \r\n\n3 open parenthesis y  minus  7 close parenthesis   equals  4x  plus  6y\r\n\nDistribute:\r\n\n3y  minus  21  equals  4x  plus  6y\r\n\nSubtract 3y from both sides:\r\n\n minus 21  equals  4x  plus  6y  minus  3y\r\n\nAdd the y terms together, and subtract 4x from both sides:\r\n\n minus 4x  minus  21  equals  3y\r\n\nDivide both sides by 3:\r\n\n  minus 4x  minus  21 divided by 3   equals   3y divided by 3 \n\nSimplify:\r\n\ny  equals   minus  4 divided by 3 x  minus  7 y  equals   minus  4 divided by 3 x  minus  7 y  equals   minus  4 divided by 3 x y  equals    minus  7 y  equals   minus  2 divided by 6 x  minus  7', 'Evaluate  integral  subscript 0 to the power of 3  open parenthesis 2x open parenthesis x to the power of 2  plus  1 close parenthesis  to the power of 3 close parenthesis dx  integral  subscript 0 to the power of 3  open parenthesis 2x open parenthesis x to the power of 2  plus  1 close parenthesis  to the power of 3 close parenthesis dx\n\nIntegration by substitution\n\nu  equals  x to the power of 2  plus  1\n\ndu  equals  2xdx\n\nnew endpoints:\n\nu  equals   open parenthesis 3 close parenthesis  to the power of 2  plus  1  equals  10\n\nu  equals   open parenthesis 0 close parenthesis  to the power of 2  plus  1  equals  1\n\nNew Equation:\n\n integral  subscript 1 to the power of 10  open parenthesis u to the power of 3 close parenthesis du\n\n u to the power of 4 divided by 4  at [1, 10]\n\n equals    open parenthesis 10 close parenthesis  to the power of 4 divided by 4   minus    open parenthesis 1 close parenthesis  to the power of 4 divided by 4 \n\n equals    open parenthesis 10000 close parenthesis  divided by 4   minus    open parenthesis 1 close parenthesis  divided by 4 \n\n equals   9999 divided by 4   81 divided by 4   10000 divided by 4   10001 divided by 4   9999 divided by 4 ']"
11,146,11_war_russia_government_britain,"['war', 'russia', 'government', 'britain', 'british', 'world war', 'national', 'political', 'american revolution', 'nations']","['""We [are] determined to save succeeding generations from the scourge of war, which twice in our lifetime has brought untold sorrow to man kind . . ."" this statement comes from which of the following texts? The United Nations was formed at the end of the Second World War and was designed to accomplish what the earlier League of Nations had failed to accomplish: to prevent another world war. the ""Declaration of the Rights of Man and Citizen"" was the founding document of the French Revolution, and Burke\'s work was a critique of that revolution. Bossuet\'s book promoted the theory of ""divine right"". the U.S. Constitution is not explicitly interested in preventing future wars. Charter of the United Nations National Assembly\'s ""Declaration of the Rights of Man and Citizen"" U.S. Constitution Edmund Burke\'s Reflections on the Revolution in France Bishop bossuet\'s Politics Drawn from the Very Words of Holy Scripture', 'During the American Revolution, unanimous approval of the Articles of Confederation was achieved when: The Articles of Confederation, America’s first governing document post-independence, required unanimous consent for amendments and significant decisions. Initially, states held substantial power, especially regarding western lands.\nSignificance of Land Surrender:\n\n1. Centralization of Authority: By surrendering western lands, states acknowledged the need for a centralized government to manage these territories.\n2. Facilitating Expansion: This act helped the U.S. govern newly acquired lands effectively and allowed for future state admissions.\n3. Promoting Unity: Surrendering land interests fostered cooperation among states, enhancing national cohesion and addressing regional rivalries. All states claiming western lands surrendered them to the national government A compromise on slavery was reached States gave up their right to establish tariffs The states gave up their power to print money Three co-equal branches of government were established', 'The phrase ""Caesar non est supra grammaticos"", translating roughly to ""Caesar/The Emperor is not above grammarians"", originated in an incident involving the Holy Roman Emperor Sigismund at the Council of Constance in 1414 While not believed to be a direct quote from the word of the monks, we do have this source:\n\n""…A similar anecdote is told of the German Emperor Sigismund. When presiding at the Council of Constance, he addressed the assembly in a Latin speech, exhorting them to eradicate the schism of the Hussites. \'Videte Patres,\' he said, \'ut eradicetis schismam Hussitarum.\' He was very unceremoniously called to order by a monk, who called out \'Serenissime Rex, schisma est generis neutri.\' The emperor, however, without losing his presence of mind, asked the impertinent monk, \'How do you know it?\' The old Bohemian schoolmaster replied, \'Alexander Gallus says so.\' \'And who is Alexander Gallus?\' the emperor rejoined. The monk replied, \'He was a monk.\' \'Well,\' said the emperor, \'and I am emperor of Rome; and my word, I trust, will be as good as the word of any monk.\' No doubt the laughers were with the emperor; but for all that, schisma remained a neuter, and not even an emperor could change its gender or termination.""\n\n    Source: The Science of Language, Founded on Lectures Delivered at the Royal Institution in 1861 and 1863 by F. Max Müller, K.M., pp. 39–40. (1899)']"
12,145,12_voting_hyperplane_hard svm_votes,"['voting', 'hyperplane', 'hard svm', 'votes', 'multiclass classification', 'decision', 'linearly separable', 'ensemble', 'classifiers', 'majority']","['Different algorithms make different assumptions about the data and lead to different classifiers in generating diverse learners. Different algorithms make different assumptions about the data and lead to different classifiers. For example one base – learner may be parametric and another may be nonparametric. When we decide on a single algorithm, we give importance to a single method and ignore all others.', 'Different algorithms make different assumptions about the data and lead to different classifiers in generating diverse learners. Different algorithms make different assumptions about the data and lead to different classifiers. For example one base – learner may be parametric and another may be nonparametric. When we decide on a single algorithm, we give importance to a single method and ignore all others.', 'Given a two-class classification problem with data points x subscript 1  equals   minus 5, x subscript 2  equals  3, x subscript 3  equals  5, having class label +1 and x subscript 4  equals  2 with class label -1. The problem can never be solved using Hard SVM. The given problem is a one dimensional two-class classification problem and the data points are non-linearly separable. So the problem cannot be solved by the Hard SVM directly. But it can be solved using Hard SVM if the one dimensional data set is transformed into a 2-dimensional dataset using some function like (x, x2). Then the problem is linearly separable and can be solved by Hard SVM.']"
13,123,13_fibonacci_predictors_weights_outputs,"['fibonacci', 'predictors', 'weights', 'outputs', 'ij', 'learning', 'using', 'performance', 'recursive', 'dynamic programming']","['What is the primary cause of performance slowdown for ThreeSum with very large arrays, as observed in empirical tests? In empirical tests of the ThreeSum algorithm, performance often slows down significantly more than predicted by its theoretical time complexity once arrays exceed a certain size due to hardware limitations. Specifically, as the dataset grows beyond the capacity of the CPU cache, the system must frequently fetch data from the much slower main memory (RAM), a phenomenon known as a cache miss . This memory hierarchy bottleneck makes the first option correct, while other choices like network latency or compiler bugs are incorrect as they do not scale fundamentally with array size in a local execution environment. Furthermore, ""application-layer design"" is too vague to be the primary hardware-level cause, and faster processors would decrease execution time rather than causing a performance slowdown relative to the input size. Caching effects increase time to access memory with large arrays. Inherent limitations in application-layer design. Network latency during execution. Compiler bugs that intensify with input size. Faster processors that reduce the time too much.', 'What does Knuth suggest programmers need to know to make back-of-the-envelope performance estimates? Donald Knuth, a pioneer in algorithm analysis, advocates for a pragmatic approach to performance estimation, especially for rapid, ""back-of-the-envelope"" analysis. He suggests that programmers should rely on empirical data (such as rough constants or observed times) combined with basic mathematical tools (like polynomial or logarithmic growth) to quickly hypothesize the asymptotic running time growth of an algorithm. This method prioritizes understanding the function\'s scaling behavior O(f(n)) over complex, fine-grained details. Detailed knowledge of assembly-level programming, processor architecture, or operating system internals is necessary for precise runtime analysis, but it is too complex for the quick, estimative nature of back-of-the-envelope calculations. Use empirical data and basic math to hypothesize running time growth. Detailed assembly-level programming skills. Extensive knowledge of processor architecture specifications. Complex real-time hardware simulation outputs. Complete knowledge of the operating system internals.', 'What does the equation \\Delta w h_{ij} = F(I h_i, O h_j, O h_{-1j}, w h_{ij}) describe? The weight change  Delta wh subscript ij depends on local information the input to the current neuron i (Ih subscript i), the current output of neuron j (Oh subscript j), the previous output of neuron j (Oh subscript  minus 1j), and the current weight itself (wh subscript ij). This dependency on the local pre- and post-synaptic activations and the current weight is characteristic of local learning rules like Hebbian learning, backpropagation, or their variations. The options suggesting weight decay, independence from activations, or use of only global outputs are incorrect as the formula explicitly uses local neuron activations (Ih subscript i, Oh subscript j) and the synaptic variable (wh subscript ij). A local learning rule using target and synaptic variables for updates. A formula for weight decay during unsupervised training. A fixed update rule independent of neuron activations. A rule that updates weights only using global network outputs.']"
14,105,14_rna_earthworm_membrane proteins_mrna,"['rna', 'earthworm', 'membrane proteins', 'mrna', 'translation', 'rna polymerases', 'anchored membrane proteins', 'codon', 'polymerases', 'acids lipid']","['Promoters vary in sequence and determine where RNA polymerase binds, influenced by proteins called sigma factors and transcription factors that regulate gene expression timing. ??', ""During translation, an mRNA molecule has a codon with the triplet sequence 5'-AUC-3'. Which anticodon sequence in a tRNA molecule would pair with this codon? ?? 3'-UAG-5' 3'-CUA-5' 3'-GAT-5' 3'-TAG-5'"", 'Ligand binding is specific and can involve several weak bonds, which together make for a relatively strong interaction. Binding can cause a change in the shape of a protein, a so called conformational change , which can affect the proteins function. ??']"
15,105,15_darwin_dna_evolution_watson,"['darwin', 'dna', 'evolution', 'watson', 'natural selection', 'mendel', 'genetic', 'ray diffraction', 'inheritance', 'genetics']","[""Why were dyes developed in the early 20th century important to DNA studies? Early \\text{DNA}-specific dyes, like the Feulgen stain, were crucial because they directly bind to \\text{DNA}, enabling scientists to visualize it under a microscope within the cell nucleus. . This staining clearly showed that chromosomes, the carriers of hereditary information, were composed largely of this material. Furthermore, the intensity of the dye uptake was observed to correlate directly with the quantity of \\text{DNA} present in a cell's nucleus, which was essential evidence supporting \\text{DNA} as the genetic material. The other options are incorrect as \\text{DNA} staining was relevant, it focused on the nucleus, and the dyes bound to \\text{DNA}, not exclusively to proteins Staining revealed chromosomes that contain most DNA. Dyes bind DNA, allowing visualization under microscope. Dye intensity correlated with DNA quantity in cells. Staining technology was irrelevant to genetic material identification. Early dyes stained cytoplasm more than nuclear material. Dyes bind exclusively to proteins, not DNA, in the nucleus."", ""What role did geology play in Darwin's evolutionary thinking? This is based on the geological theory of Uniformitarianism, championed by Charles Lyell (whose book Darwin read), which proposed that the Earth was shaped by the same slow, continuous processes operating today over vast spans of time. This idea of deep time and gradual change provided the necessary framework for Darwin's theory of gradual evolution through natural selection, suggesting that the small changes observed in individuals could accumulate over immense timescales to produce new species. The other options are incorrect because geology provided the foundation for an old Earth, supported a relationship between changes in the environment and organisms, and Darwin integrated geological principles into his theory. Geology showed Earth's slow changes, supporting Darwin's view of gradual evolution. Earth's geological changes implied no relation to living organisms. Geology disproved slow Earth changes, favoring sudden events. Geology showed Earth was too young for evolution to occur. Darwin rejected geology as irrelevant to biological processes."", ""What experiment provided clear evidence that DNA replication follows a semiconservative mechanism? The Meselson-Stahl experiment provided the definitive evidence for the semiconservative model of DNA replication. They grew E. coli in a heavy nitrogen isotope (\\text{}^{15}\\text{N}) and then transferred them to a light nitrogen isotope (\\text{}^{14}\\text{N}). Using density gradient centrifugation , they showed that after one generation, DNA had an intermediate density, and after two generations, it had two bands (intermediate and light), exactly as predicted by the semiconservative model, where each new DNA molecule consists of one old strand and one new strand. The other experiments, while significant, established different facts: Hershey-Chase identified DNA as the genetic material; Watson-Crick determined the structure; and Avery and Griffith studied bacterial transformation. The Meselson-Stahl experiment used nitrogen isotopes to prove semiconservative replication. Hershey-Chase used bacteriophage labeling to confirm DNA as genetic material. The Watson-Crick model demonstrated DNA forms a double helix structure. Avery's work identified DNA as the molecule responsible for transformation. Griffith's experiment showed that genetic material could transform bacteria types.""]"
16,101,16_odd numbers_select odd numbers_odd numbers odd numbers_select odd,"['odd numbers', 'select odd numbers', 'odd numbers odd numbers', 'select odd', 'select numbers numbers divisible', 'select odd numbers odd', 'odd numbers odd', 'numbers odd numbers', 'odd numbers divisible', 'numbers odd numbers divisible']","['Select all the ODD numbers. Odd numbers are not divisible by 2. 14 1 39 73 29 41', 'Select all the ODD numbers. Odd numbers are not divisible by 2. 52 5 62 6 7 91', 'Select all the ODD numbers. Odd numbers are not divisible by 2. 82 39 61 7 43 5']"
17,100,17_game_china_dynasty_players,"['game', 'china', 'dynasty', 'players', 'century', 'hon inbō', 'played', 'edo period', 'greek mythology', 'shogunate']","[""How did Go's cultural role differ in China compared to Tibet and Mongolia? In China, Go (Weiqi) was linked with the scholar arts and self-cultivation, forming one of the Four Arts essential for the educated elite, where its strategic depth was valued for cultivating intellect and discipline. In contrast, its role in neighboring regions was often more spiritual or practical: in Tibet, Go was part of shamanic and ritual traditions, and Mongols used Go stones for divination and vision. The other options are incorrect because Go's role in China was profoundly intellectual and esteemed, not only for gambling; it was not solely a competitive sport in Tibet and Mongolia, as the spiritual uses show; and Go has a much longer history in China, predating the 19th century by millennia, so the last option is historically false. Tibet: Go part of shamanic and ritual traditions Mongols used Go stones for divination and vision China: Go linked with scholar arts, self-cultivation China used Go only for gambling and fortune-telling In Tibet and Mongolia, Go was solely played as competitive sport Mongolia introduced Go to China in the 19th century"", 'The Tokugawa shogunate in Japan established four official Go schools and patronized formal competitive play starting from 1700 onwards. The Tokugawa shogunate provided essential official patronage to Go, institutionalizing it through the four official schools—Hon’inbo, Inoue, Yasui, and Hayashi—each led by a hereditary master or iemoto. This system emerged after Tokugawa Ieyasu, an avid Go player, granted salaries to top experts, eventually formalizing their status as government-supported professionals. While the schools were active earlier, the system matured into its most prestigious form by the early 18th century (1700 onwards), characterized by the famous Oshirogo (Castle Go) matches played annually in the presence of the shogun at Edo Castle. Other options claiming Go was only a casual pastime or lacked official structure are incorrect because the shogunate specifically integrated these schools into the national administrative framework, governed under the Magistrate of Temples and Shrines. This high-level patronage transformed Go from a simple game into a formalized, competitive art with deep political and social significance in Edo Japan.', ""How did the opening strategies in Japanese Go evolve during the Edo period? The Edo period (1603-1868) was the Golden Age of Go in Japan, characterized by intense study under the patronage of the Shogunate. This environment fostered great theoretical advancements, including the development of numerous fuseki (whole-board opening patterns) beyond simple corner-first play. Go strategy evolved from older practices, like handicap games with preset stone placements, toward a modern game played on an empty board, requiring more strategic depth. Furthermore, new josekis (corner sequences) and advanced concepts like sacrifice tactics emerged as masters innovated and refined the game's opening theory. Various opening patterns (fuseki) then developed Shifted from preset to empty board starts New josekis and sacrifice tactics emerged Opening strategies remained unchanged from Chinese traditions Players started with all stones placed for gameplay The Edo period discouraged opening strategy experimentation""]"
18,98,18_perimeter_area rectangle_perimeter square_equals open parenthesis,"['perimeter', 'area rectangle', 'perimeter square', 'equals open parenthesis', 'width rectangle', 'perimeter equals', 'perimeter equals open', 'perimeter equals open parenthesis', 'diameter', 'perimeter rectangle']","['If the perimeter of a rectangle is 50, and the length of the rectangle is 13, what is the area of the rectangle? First, use the information given about the perimeter to find the width of the rectangle.\n\nRecall how to find the perimeter of a rectangle:\n\n text   equals  2 open parenthesis  text   plus   text  close parenthesis \n\nFrom this equation, we can solve for the width.\n\n text   plus   text   equals    text  divided by 2 \n\n text   equals    text  divided by 2   minus   text \n\nSubstitute in the information from the question to find the width of the rectangle.\n\n text   equals   50 divided by 2   minus  13\n\nSimplify.\n\n text   equals  12\n\nNow, recall how to find the area of a rectangle:\n\n text   equals   text   times  text \n\nSubstitute in the information about the length and width to find the area for the rectangle in question.\n\n text   equals  13  times 12\n\nSolve.\n\n text   equals  156 156 168 172 120', 'If the area of a rectangle is 36, and the length of the rectangle is 4, what is the perimeter of the rectangle? Recall how to find the area of a rectangle:\r\nArea  equals   text   times  text \r\nNow, we can use this equation to find the width of the rectangle.\r\nwidth  equals    text  divided by  text  \r\nPlug in the information from the question to find the width of the rectangle.\r\nWidth  equals   36 divided by 4   equals  9\r\nNow, recall how to find the perimeter of a rectangle.\r\nPerimeter  equals  2 open parenthesis  text   plus   text  close parenthesis \r\nPlug in the length and width values to find the perimeter.\r\nPerimeter  equals  2 open parenthesis 4  plus  9 close parenthesis   equals  26 26 24 22 20', 'If the area of a rectangle is 35, and the length of the rectangle is 5, what is the perimeter of the rectangle? Recall how to find the area of a rectangle:\r\nArea  equals   text   times  text \r\nNow, we can use this equation to find the width of the rectangle.\r\nwidth  equals    text  divided by  text  \r\nPlug in the information from the question to find the width of the rectangle.\r\nWidth  equals   35 divided by 5   equals  7\r\nNow, recall how to find the perimeter of a rectangle.\r\nPerimeter  equals  2 open parenthesis  text   plus   text  close parenthesis \r\nPlug in the length and width values to find the perimeter.\r\nPerimeter  equals  2 open parenthesis 5  plus  7 close parenthesis   equals  24 24 12 36 48']"
19,86,19_power circ_circ_power_power equals,"['power circ', 'circ', 'power', 'power equals', 'pi text', 'divided pi', 'perimeter', 'equals square root', 'equals square', 'square root square root']","['Find the area of a circle that has a radius of  1 divided by 5 .   1 divided by 25  pi  Use the following formula to find the area of a circle:\r\n\r\nArea  equals   pi  times  text  to the power of 2\r\n\r\nFor the circle in question, plug in the given radius to find the area.\r\n\r\nIn our particular case the radius is  1 divided by 5 .\r\n\r\nArea  equals   pi  times  left  open parenthesis   1 divided by 5   right  close parenthesis  to the power of 2  equals   1 divided by 25  pi \r\n\r\nWhen squaring a fraction we need to square both the numerator and the denominator.\r\n\r\n pi  times  left  open parenthesis   1 divided by 5   right  close parenthesis  to the power of 2  equals   pi  1 to the power of 2 divided by 5 to the power of 2   equals   pi  1  times 1 divided by 5  times 5   equals   1 divided by 25  pi ', 'Using the Pythagorean Theorem, calculate the length of the hypotenuse of  triangle ABC. The Pythagorean Theorem holds that, in a right triangle, the square of the hypotenuse equals the sum of the squares of the other two sides. \n\nThat is,\r\n\na to the power of 2  plus  b to the power of 2  equals  c to the power of 2\r\n\nwhere c is the hypotenuse and a and b are the other two sides.\r\n\nHere, c corresponds to  overline , and, since this is a 45 to the power of  circ   minus  45 to the power of  circ   minus  90 to the power of  circ  triangle, the length of a equals the \n\nlength of b, so we know that  overline   equals   overline   equals  3.\r\n\nApply the Pythagorean Theorem.\r\n\n|AB| to the power of 2  plus  |BC| to the power of 2  equals  |AC| to the power of 2\r\n\n3 to the power of 2  plus  3 to the power of 2  equals  |AC| to the power of 2\r\n\n9  plus  9  equals  |AC| to the power of 2\r\n\n18  equals  |AC| to the power of 2\r\n\n square root of    equals  AC\r\n\n square root of    equals  AC\r\n\n3 square root of    equals  AC\r\n\nSo the length of the hypotenuse is 3 square root of  . 3 square root of   5 square root of   3 square root of   2 square root of  ', 'Find the area of a circle that has a radius of  1 divided by 5 . Use the following formula to find the area of a circle:\r\n\nArea  equals   pi  times  text  to the power of 2\r\n\nFor the circle in question, plug in the given radius to find the area.\r\n\nIn our particular case the radius is  1 divided by 5 .\r\n\nArea  equals   pi  times  left  open parenthesis   1 divided by 5   right  close parenthesis  to the power of 2  equals   1 divided by 25  pi \r\n\nWhen squaring a fraction we need to square both the numerator and the denominator.\r\n\n pi  times  left  open parenthesis   1 divided by 5   right  close parenthesis  to the power of 2  equals   pi  1 to the power of 2 divided by 5 to the power of 2   equals   pi  1  times 1 divided by 5  times 5   equals   1 divided by 25  pi   2 divided by 5  pi   1 divided by 25  pi   3 divided by 25  pi   1 divided by 12  pi ']"
20,84,20_antigen_blood_gene_blood group,"['antigen', 'blood', 'gene', 'blood group', 'blood type', 'abo blood', 'abo blood group', 'multiple alleles', 'inheritance', 'group']","['What test helps determine if two mutations are allelic or non-allelic? The test used to determine if two mutations are allelic (on the same gene) or non-allelic (on different genes) is the Complementation Test (or cis-trans test). This test involves crossing two individuals that are homozygous for each different mutation. If the resulting F1 generation exhibits the wild-type phenotype (meaning the function is restored), the mutations are non-allelic because each mutant provided a functional copy of the gene the other lacked, demonstrating complementation. If the F1 generation still displays the mutant phenotype, the mutations are allelic (on the same gene) because the mutations could not complement each other to restore the wild-type function. The other methods (counting chromosomes, observing traits, sequencing, or measuring enzymes) are not the definitive method for assessing allelic relationship. Cross mutants and see if wild type is restored. Counting chromosomes in mutant cells. Observing physical traits without crossing. Sequencing the entire genome of mutants. Measuring enzyme levels in mutant tissues.', ""What does independent assortment fail to explain about genes on the same chromosome? Independent assortment fails to explain that Genes close together do not assort independently due to linkage. Mendel's Law of Independent Assortment states that alleles for different traits separate independently of each other during gamete formation, which is true only for genes located on different chromosomes or those located far apart on the same chromosome. However, genes that are physically close together on the same chromosome are often inherited together, a phenomenon called genetic linkage. This linkage prevents their independent assortment, leading to a higher frequency of parental allele combinations in the offspring than expected by Mendel's law. The other options are incorrect: Genes on the same chromosome do not always show complete independent assortment; the whole point of linkage is that not all genes, regardless of location, assort independently; and genes on different chromosomes always assort independently, which is the exact condition where the law holds true. Genes close together do not assort independently due to linkage. Genes on the same chromosome always show complete independent assortment. All genes, regardless of location, always assort independently. Genes on different chromosomes never assort independently."", 'What was the biochemical basis for Garrod coining the term \'inborn error of metabolism\'? Archibald Garrod proposed that certain inherited diseases, such as alkaptonuria, resulted from a genetically determined enzyme deficiency that prevented the completion of a specific metabolic pathway . This established the first direct link between genes and the production of functional enzymes, making the first option correct. The other options are incorrect because Garrod’s work specifically highlighted heritable traits rather than environmental toxins or transient dietary malfunctions, and he identified deficiencies in enzymes (proteins) rather than general overproduction or random RNA mutations. By observing that these ""errors"" followed Mendelian inheritance patterns, he laid the foundation for the ""one gene, one enzyme"" hypothesis, which excludes non-genetic or non-protein-based explanations for these metabolic conditions. Genetically determined enzyme deficiency causing symptoms. Exposure to environmental toxins causing genetic mutations in enzymes. Random mutations in RNA sequences with no inheritance pattern. Enzyme overproduction leading to metabolic imbalances. A transient enzyme malfunction from dietary deficiencies.']"
21,82,21_probability_joint probability_event_square method,"['probability', 'joint probability', 'event', 'square method', 'probability density', 'marginal probability', 'probability theory', 'best fit', 'beta_0 beta_1', 'linear relationship']","['A probability space in which every outcome has the same probability is called a uniform probability space A uniform probability space is defined as any sample space where every possible outcome has the exact same probability of occurring . For a finite space with N outcomes, the probability of any single outcome E is P(E) = 1/N. This is the simplest and most fundamental type of probability space, often used as a starting point for teaching probability theory, where examples like rolling a fair six-sided die or flipping a fair coin are classic demonstrations of uniformity. Terms like ""marginal"" or ""conditional"" describe specific types of probabilities, not the inherent nature of the entire space.', 'Linear regression assumes that there is a linear relationship between the response Y and the predictors X subscript 1, X subscript 2, ..., X subscript p.  This is correct because the entire premise of linear regression, as implied by its name, is to model the relationship between a response variable (Y) and one or more predictor variables (X subscript 1, X subscript 2,  dots , X subscript p) using a straight-line equation . The model is mathematically expressed as Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, where the predicted response is a linear combination of the predictors. If the true relationship were non-linear (e.g., quadratic or exponential), a linear model would be an inappropriate and inaccurate fit, justifying why ""linear"" is the essential, defining assumption and the only correct answer here.', ""The equation p(X = x_i, Y = y_j) = p(Y = y_j | X = x_i)p(X = x_i) represents the product rule The product rule defines the joint probability of two events, P(A, B), as the probability of the second event given the first, multiplied by the probability of the first event, i.e., P(A, B) = P(B|A)P(A). The given equation exactly follows this structure for discrete random variables X and Y: the joint probability of X=x_i and Y=y_j, P(X=x_i, Y=y_j), is calculated by multiplying the conditional probability P(Y=y_j | X=x_i) by the marginal probability P(X=x_i). This rule is fundamental for calculating the intersection of dependent events, and it is a rearrangement of the conditional probability formula. Other probability rules like the addition rule or Bayes' theorem have different forms.""]"
22,82,22_meiosis_gametes_independent assortment_daughter cells,"['meiosis', 'gametes', 'independent assortment', 'daughter cells', 'diploid', 'chromosome pairs', 'haploid', 'gamete', 'genetically identical', 'chromatids']","['Homologous chromosome pairs do not align together on the metaphase I plate during meiosis. Metaphase I in meiosis is the precise alignment of homologous chromosome pairs (bivalents) at the metaphase plate, in contrast to mitosis or Meiosis II where individual chromosomes align. This paired alignment is essential for the subsequent separation of the homologous chromosomes during Anaphase I. If the homologous pairs did not align together, the meiotic process would fail to properly reduce the chromosome number, leading to daughter cells with an incorrect chromosome complement (aneuploidy).', ""Alleles of genes located close together on the same chromosome assort independently according to Mendel’s law of independent assortment. The statement is incorrect because Mendel's Law of Independent Assortment only applies to genes located on different chromosomes or those located very far apart on the same chromosome. Genes that are located close together on the same chromosome are physically linked and tend to be inherited together; this phenomenon is called genetic linkage. Linkage violates the law of independent assortment because the close proximity prevents independent separation during meiosis, resulting in non-Mendelian inheritance ratios."", 'Recombinant gametes are produced when homologous chromosomes exchange corresponding segments during prophase I of meiosis. The statement that recombinant gametes are produced when homologous chromosomes exchange corresponding segments during prophase I of meiosis is correct. This process, known as crossing over , involves the physical exchange of \\text{DNA} between non-sister chromatids of homologous chromosomes. This exchange shuffles alleles, creating new combinations of genes on the chromatids. When these chromatids are eventually packaged into gametes, the resulting gametes are recombinant, ensuring genetic diversity in the offspring.']"
23,81,23_11_11 13_12 11_15 10,"['11', '11 13', '12 11', '15 10', '15 13', '17 10', '15 16', '12 10', '11 10', '10 11 12']","['8 + 5 = ? 8+5 = 13 11 13 12 22 85 14', '9 + 3 = ? 9+3 =12 12 13 93 18 14 11', '3 + 8 = ? 3+8 = 11 11 38 21 13 12 10']"
24,71,24_polymerase_dna_replication_dna polymerase,"['polymerase', 'dna', 'replication', 'dna polymerase', 'rna polymerase', 'template strand', 'polymerases', 'dna strand', 'dna strands', 'dna template']","[""Why must the RNA primers be removed and replaced during lagging strand synthesis? During DNA replication, RNA primers provide the initial 3'-OH group needed to start the synthesis of Okazaki fragments on the lagging strand. However, RNA is inherently less stable and more susceptible to degradation than DNA, and if left in the final molecule, it would introduce errors and greatly increase the risk of mutations in the replicated chromosome. Therefore, enzymes (like DNA Polymerase I in E. coli or various nucleases/polymerases in eukaryotes) must excise the RNA and replace it with deoxyribonucleotides to create a complete, stable, and continuous DNA strand, ensuring the integrity of the genetic material. RNA primers are replaced by DNA to ensure genomic stability after replication. RNA primers block DNA polymerase activity during strand elongation. RNA primers are toxic and induce apoptosis if not excised. RNA primers cause breaks in the DNA backbone if left unrepaired. RNA primers degrade spontaneously and are removed to avoid DNA damage."", ""What critical feature distinguishes leading and lagging strand synthesis? This is the critical distinguishing feature of \\text{DNA} replication . \\text{DNA} polymerase can only synthesize new strands in the 5'  approaches 3' direction. On the leading strand, this allows continuous synthesis towards the replication fork as the \\text{DNA} unwinds. On the lagging strand, however, the 5'  approaches 3' direction is away from the fork, forcing the polymerase to synthesize the new strand discontinuously in short fragments called Okazaki fragments. The other options are incorrect descriptions of the fundamental mechanism of \\text{DNA} polymerase activity. Leading strand is synthesized continuously, lagging strand discontinuously. Leading strand synthesis requires RNA primers; lagging strand does not need them. Leading strand is synthesized discontinuously; lagging strand is a single continuous strand. Both strands are synthesized continuously, but lagging strand forms RNA segments. Neither strand uses DNA polymerase; instead, helicase completes synthesis."", ""Which is a reason that explains why DNA replication occurs in a continuous fashion on one parental DNA strand and in a discontinuous fashion on the other parental DNA strand? This enzyme can only synthesize DNA in the 5' to 3' direction, requiring a free hydroxyl group to attach new nucleotides. Since the two parental strands are antiparallel, one strand (the leading strand) can be synthesized continuously toward the replication fork, while the other (the lagging strand) must be built in short, discontinuous Okazaki fragments to maintain that specific chemical orientation. While it is true that initiation requires a primer, base pairing is essential, and enzymes ensure speed, these facts apply equally to both strands and do not explain the directional discrepancy. Therefore, the physical constraint of the polymerase's active site is the sole reason for the asymmetric nature of the replication process. DNA polymerase only works to add nucleotides in one direction of the growing chain. Initiation of a new DNA strand requires the use of a primer. DNA replication depends on base pairing between a template and nucleotides being added to the new strand. A polymerase enzyme is needed to ensure that the replication process is both accurate and rapid""]"
25,69,25_carbon_carbon fixation_calvin cycle_carbon dioxide,"['carbon', 'carbon fixation', 'calvin cycle', 'carbon dioxide', 'photosynthesis', 'hydroxypropionate', 'thylakoid', 'carbon fixation reactions', 'fixation reactions', 'bacteria']","['ATP and NADPH are required for the carbon-fixation reactions that produce carbohydrates from CO₂. the Calvin Cycle, are entirely dependent on the energy and reducing power supplied by the light reactions of photosynthesis. ATP provides the necessary chemical energy to drive the endergonic reactions within the cycle, while NADPH supplies the high-energy electrons (reducing power) required to convert the fixed carbon dioxide  open parenthesis CO subscript 2 close parenthesis  into the carbohydrate product (G3P), which is then used to synthesize sugars like glucose. Without both ATP and NADPH, the cycle cannot proceed to produce carbohydrates.', 'Biological carbon fixation is the process by which living organisms convert inorganic carbon, mainly carbon dioxide, into organic compounds. Biological carbon fixation is precisely the process where living organisms (primarily photoautotrophs like plants, algae, and cyanobacteria, as well as chemoautotrophs) convert atmospheric or dissolved inorganic carbon (chiefly carbon dioxide,  text  subscript 2) into organic compounds.  This conversion is fundamental to all life on Earth, forming the base of most food webs by generating the energy-rich, carbon-based molecules (like glucose) that are essential for growth and energy storage within the organisms. The process effectively transfers carbon from the atmosphere/hydrosphere into the biosphere.', ""An important feature of the 3-hydroxypropionate bicycle is its ability to co-assimilate numerous compounds, making it suitable for mixotrophic organisms. The 3-hydroxypropionate bicycle (also called the 3-hydroxypropionate/4-hydroxybutyrate cycle) is a highly efficient metabolic pathway primarily found in certain archaea and photosynthetic bacteria. Its crucial feature is its versatility in carbon fixation and metabolism, allowing it to co-assimilate numerous organic and inorganic compounds, including  text  subscript 2 and various organic substrates. This metabolic flexibility makes the cycle particularly well-suited for mixotrophic organisms, which are those that can use both inorganic compounds (like  text  subscript 2) for carbon and organic compounds for energy, giving them a survival advantage in diverse environments. Therefore, the cycle's ability to co-assimilate compounds is directly linked to its importance in mixotrophic metabolism.""]"
26,68,26_scientific_applied science_inductive reasoning_scientific method,"['scientific', 'applied science', 'inductive reasoning', 'scientific method', 'deductive reasoning', 'natural sciences', 'hypothesis based science', 'based science', 'descriptive science', 'basic science']","['Which statements reflect valid reasons for defining science beyond just the scientific method? they acknowledge that science is a diverse endeavor not always confined to the rigid, laboratory-based ""scientific method."" Many legitimate fields, such as archaeology or astronomy, rely on observational data and inference where exact experimental replication is physically impossible, yet they still provide vital understanding of the natural world. In contrast, stating that science excludes study based on hypotheses or always requires exact repetition is incorrect as it ignores these observational disciplines and the evolving nature of scientific inquiry. Furthermore, non-testable supernatural explanations are universally excluded from science because they cannot be empirically observed or falsified. Hypotheses are sometimes supported without repetition. Fields like archaeology have trouble repeating experiments. Science seeks to broadly comprehend nature’s universe. Science excludes any study based on hypotheses or inference. Science always requires exact experimental repetition. Non-testable supernatural explanations are considered scientific.', 'What factors differentiate descriptive science from hypothesis-based science? Descriptive science is primarily concerned with observing and recording phenomena, often using inductive reasoning to form generalizations from specific observations. In contrast, hypothesis-based science starts with a proposed explanation (hypothesis) and uses deductive reasoning to make and test specific predictions. While distinct in their core methodology, the line between the two is often blurred, as descriptive discoveries frequently lead to testable hypotheses, and hypothesis testing relies on careful observation and data collection. Therefore, the differences lie in their primary reasoning and goal (discovery versus explanation), not in being entirely independent or mutually exclusive. Hypothesis-based science tests predictions deductively. Descriptive science uses inductive reasoning. The line between them is sometimes blurred. Hypothesis-based science never involves observation data collection. Both forms are entirely independent and never combined in research. Descriptive science always tests hypotheses with experiments.', 'What are characteristics of inductive reasoning? Inductive reasoning is the hallmark of descriptive science, as it involves analyzing large volumes of qualitative or quantitative data to identify patterns that lead to broader generalizations. Unlike deductive reasoning, which moves from a broad theory to a single observable event or uses general premises to deduce specific predictions, inductive reasoning works ""bottom-up"" by using specific instances to build a general theory. Consequently, it is not primarily employed in hypothesis-driven experimental science, which typically starts with a hypothesis (a general prediction) and tests it through specific experiments. Thus, the selected options accurately define the directionality and scientific application of the inductive process. It supports descriptive or discovery science methods. It infers general conclusions using many observations. It moves from specific cases to general principles. It is mainly employed in hypothesis-driven experimental science. It moves from a broad theory to a single observable event. It uses general premises to deduce specific predictions.']"
27,67,27_deep_deep learning_local learning_synaptic,"['deep', 'deep learning', 'local learning', 'synaptic', 'learning rules', 'local deep', 'local deep learning', 'learning channel', 'deep learning channel', 'neural networks']","['Local deep learning rules can be written as a function of target information, postsynaptic and presynaptic activities, and existing weight values. Local deep learning rules, such as those derived from backpropagation or its biologically plausible approximations, define the weight change ( Delta w) based on information locally available to the synapse . This information includes the postsynaptic activity (e.g., the output of the current neuron), the presynaptic activity (the input from the previous neuron), the existing weight values themselves, and the target information (e.g., the error signal or desired output that is backpropagated). These four components are essential for calculating the gradient used to update the weight, making the statement correct.', ""What physical characteristic distinguishes the ideal Deep Learning Channel in terms of feedback? This characteristic is essential because the primary learning algorithm in deep neural networks is backpropagation, which works by calculating the error at the output and then recursively transmitting the error signals backward through the layers. This backward flow of the error gradient is used to adjust the weights and biases to minimize the error. The other options are incorrect because simply transmitting signals forward is the feedforward pass, and blocking the backward signal would prevent learning, while chemical diffusion is a biological analogy that doesn't describe the computational process. It must allow backward error signal communication through the network. It only transmits signals forward from input to output. It blocks all backward signal transmission. It uses only chemical diffusion without wired pathways."", 'How does Deep Learning Channel relate to synaptic physical structures? The Deep Learning Channel (DLC) is a theoretical concept proposing how biological synapses, which are physical structures, can implement error backpropagation-like mechanisms required for deep learning . This mechanism is thought to involve backward signaling pathways (retrograde signaling) that convey information about error or target signals from the postsynaptic side back to the presynaptic terminal, allowing for the precise adjustment of synaptic weights. The other options are incorrect as the DLC is not purely abstract, and it specifically involves more than just a forward signal across the cleft or non-specific global neurotransmitters. It may use backward pathways to convey error or target signals to synapses. It operates only through the synaptic cleft without backward signals. It uses global neuromodulators shared across the whole brain. It is an abstract concept without physical correspondence.']"
28,64,28_predictors_predictor_covariance_parametric methods,"['predictors', 'predictor', 'covariance', 'parametric methods', 'regression', 'variables', 'logistic regression', 'effect', 'interaction term', 'assumes predictors']","['How does Quadratic Discriminant Analysis (QDA) differ from LDA in assumptions about covariance matrices? QDA models the covariance as class-specific, leading to quadratic boundaries, whereas LDA uses a shared covariance matrix, resulting in linear boundaries. QDA assumes each class has its own covariance matrix, while LDA assumes a common covariance matrix across all classes. QDA assumes covariance matrices are diagonal, LDA assumes they are identical and full. QDA assumes zero covariance, LDA assumes full covariance. QDA ignores covariance matrices, LDA models them fully.', 'What happens to the training mean squared error (MSE) and test set MSE as the number of unrelated features increases in a model? This is the classic symptom of overfitting, where the model becomes overly complex and starts to fit the noise in the training data perfectly, driving the training error down toward zero. However, since the unrelated features do not generalize to new data, the model performs very poorly on the unseen test data, causing the test error MSE to rise dramatically. The other options are incorrect because adding noise does not improve generalization, nor does it cause both errors to rise or fall together in this manner. Training MSE decreases to zero while test MSE increases significantly. Training MSE increases and test set MSE decreases due to better generalization. Both training MSE and test set MSE decrease as more features improve the model. Training MSE and test set MSE both increase due to more complex modeling. Training MSE remains constant while test set MSE becomes zero.', 'What consequence does collinearity have on t-statistics of predictor coefficients? Collinearity, or multicollinearity, refers to a high correlation between two or more predictor variables in a regression model. This correlation causes the standard errors of the corresponding predictor coefficient estimates ( hat ) to increase. Since the t-statistic is calculated as the coefficient divided by its standard error (t  equals   hat   divided by  SE open parenthesis  hat  close parenthesis ), an increased standard error directly reduces the magnitude of the t-statistic. A smaller t-statistic leads to a higher p-value, making it harder to reject the null hypothesis (H subscript 0:  beta  equals  0) and thus harder to detect a statistically significant effect for the individual predictor, even if the overall model is significant. It reduces t-statistics, making it harder to detect significant effects. It makes t-statistics invalid. It has no effect on t-statistics. It increases t-statistics.']"
29,63,29_pyruvate_acetyl coa_glycolysis_pyruvate dehydrogenase,"['pyruvate', 'acetyl coa', 'glycolysis', 'pyruvate dehydrogenase', 'mitochondrial', 'lactate', 'catalyzes', 'citric acid cycle', 'acid cycle', 'citric acid']","['How does pyruvate regulate the E. coli pyruvate dehydrogenase complex differently from the mammalian enzyme? The regulation of the Pyruvate Dehydrogenase Complex (PDC) differs between bacteria (like E. coli) and mammals primarily because E. coli lacks the dedicated regulatory kinases and phosphatases used by mammals . In mammalian cells, high pyruvate inhibits the kinase (preventing phosphorylation) to activate the enzyme. In E. coli, however, high pyruvate directly influences the cofactors: it binds to the E1 subunit, increasing the binding of the essential cofactor thiamine pyrophosphate (TPP), which directly stimulates enzyme activity. The regulation in E. coli is thus more reliant on substrate availability and allosteric cofactor binding rather than complex phosphorylation/dephosphorylation cycles. High pyruvate increases TPP binding to stimulate enzyme activity. High pyruvate inhibits complex phosphorylation to regulate activity. High pyruvate reduces lipoamide content to decrease enzyme function. High pyruvate activates NADH production to inhibit enzyme activity.', 'Which enzyme in the complex transfers the acetyl group to Coenzyme A to form acetyl-CoA? The conversion of pyruvate to acetyl-CoA is catalyzed by the Pyruvate Dehydrogenase Complex (PDC), a multi-enzyme system composed of three main enzymes: E1, E2, and E3 . The second enzyme, Dihydrolipoyl transacetylase (E2), contains a lipoamide cofactor which accepts the acetyl group from E1. E2 then catalyzes the final, crucial step: the transfer of this two-carbon acetyl group directly to Coenzyme A (CoA), forming the high-energy product acetyl-CoA. E1 (Pyruvate dehydrogenase) initiates the decarboxylation, and E3 (Dihydrolipoyl dehydrogenase) regenerates the complex, making E2 the sole transferase. Dihydrolipoyl transacetylase (E2) transfers acetyl groups to CoA. Pyruvate dehydrogenase (E1) transfers acetyl groups to CoA. Protein X transfers acetyl groups to Coenzyme A during catalysis. Dihydrolipoyl dehydrogenase (E3) transfers acetyl groups to CoA.', 'What is one irreversible reaction performed by the pyruvate dehydrogenase complex in animals? The pyruvate dehydrogenase (PDH) complex catalyzes the oxidative decarboxylation of pyruvate to form acetyl-CoA, releasing carbon dioxide ( text  subscript 2) and reducing \\text{NAD}^+ to \\text{NADH}. This reaction is highly exergonic and is considered physiologically irreversible in animals. This irreversibility is a crucial metabolic control point, ensuring that once glucose is converted to pyruvate and then to acetyl-CoA, the acetyl-CoA must either enter the citric acid cycle or be used for fatty acid synthesis; it cannot be converted back to glucose. The other options are incorrect because the PDH complex only catalyzes the conversion of pyruvate to acetyl-CoA (eliminating options 2, 3, and 4 as its products/substrates), and specifically, the conversion in the reverse direction (acetyl-CoA to pyruvate) is impossible for the PDH complex and generally not done by a single enzyme in animals. Conversion of pyruvate to acetyl-CoA is irreversible. Conversion of acetyl-CoA to pyruvate is irreversible in animals. Conversion of pyruvate to oxaloacetate is irreversible in animals. Conversion of alanine to pyruvate is irreversible in animals.']"
30,63,30_question image_ant ant_question_ant ant ant,"['question image', 'ant ant', 'question', 'ant ant ant', 'ant ant ant ant', 'shown middle', 'white background', 'white background count', 'shown middle middle hand', 'middle hand shown middle']","['solve the addition given below one hand has 3 fingers and other has 2 fingers. so it adds up to 6 6 7 5 3 Question image: a hand is shown in the middle and middle of a hand is shown in the middle, and middle of a hand is shown in the middle', 'Solve the addition below one hand has 4 fingers and other hand has 3 fingers, so it adds up to 7 7 8 6 5 Question image: a hand is shown in the middle and middle of a hand is shown in the middle and middle of a hand is shown in the middle of', 'solve the addition below one hand has 2 fingers while other hand also has two fingers, so it adds upto 4 4 5 3 2 Question image: a hand is shown in the middle and middle of a hand is shown in the middle and middle of a hand is shown in the middle and the middle of the middle of the middle of the middle of the middle of the middle of the middle of']"
31,59,31_earthworms_earthworm_soil_setae,"['earthworms', 'earthworm', 'soil', 'setae', 'nephridia', 'worm', 'clitellum', 'intestine', 'giant axons', 'species']","['What is the primary mechanism by which earthworms move through underground soil? Earthworms move via waves of muscular contractions (peristalsis) that work with setae anchoring most body segments, allowing effective underground locomotion. Peristalsis shortens and lengthens the body while setae anchor segments for underground movement. Undulating their whole bodies sideways like snakes as they move in loose soil conditions. Front body appendages provide digging ability to create earthworm tunnels below the surface. Mucus secretion is used alone to dissolve soils in front for simple forward progression.', 'Which neural pathway in earthworms is primarily responsible for conducting the fastest emergency reflex escape signals, and what is its direction of conduction? The dorsal giant axon in earthworms conducts the fastest signals from the rear toward the front, initiating efficient emergency escape responses. The dorsal giant axon conducts emergency signals from rear to front rapidly. The lateral nerve plexus moves signals sideways across adjacent segments. The ventral nerve cord sends neural signals mostly from front to back. Medial giant axons transmit emergency signals mainly from rear to front.', 'In the classical taxonomic system, what justified placing earthworms in the order Opisthopora? The order Opisthopora is defined by the anatomical relationship in which the male pores open posterior to the female pores, even though the actual male segments are anterior internally. Earthworms have male pores opening behind female pores but males are anterior inside. Earthworms show the prostomium overhanging the mouth only during body movement. Earthworms have setae forming a circular arrangement around each body segment. Earthworms maintain a glandular clitellum located toward the body’s midsection part.']"
32,59,32_living_organisms_amino acids_rna,"['living', 'organisms', 'amino acids', 'rna', 'transcription', 'chemical', 'answer', 'text rna', 'living organisms', 'physics']","['The term dipole describes a separation of opposite electric charges, such as those occurring in a polar covalent bond. The term dipole is the correct answer because its definition literally means ""two poles,"" referring to a system of two equal and oppositely charged or magnetized poles separated by a distance. In the context of chemistry and physics, a molecular dipole specifically describes the separation of opposite electric charges, exactly as stated in the sentence. This charge separation commonly arises in a polar covalent bond between atoms with different electronegativities, like in a water molecule (\\text{H}_2\\text{O}), where the oxygen atom pulls the electrons closer, becoming partially negative ( delta  to the power of  minus ), and the hydrogen atoms become partially positive ( delta  to the power of  plus ). No other single term accurately and concisely captures this fundamental concept of separated opposite charges.', 'Ligand binding is specific and can involve several weak bonds, which together make for a relatively strong interaction. The strength of the interaction is termed the binding affinity ; The higher values are associated with more specific binding. The Binding affinity is defined precisely by the collective effect of the numerous weak, non-covalent bonds (like hydrogen bonds, ionic bonds, and van der Waals forces) that form between the two molecules, resulting in a stable, relatively strong interaction. Therefore, the justification is that binding affinity quantitatively measures how tightly and specifically a ligand binds to a macromolecule; a higher binding affinity means a lower dissociation constant  open parenthesis K subscript d close parenthesis , which directly corresponds to a stronger, more specific interaction, as noted in the image itself. Other terms like binding specificity only describe the selectivity, while reaction rate or thermodynamic stability describe kinetics or overall stability, not the strength of the non-covalent interaction itself.', 'What are commonalities observed among all known living organisms? These choices represent the core biological principles that define life, as all living organisms must maintain a stable internal state, possess high levels of structural organization from cells to organ systems, and undergo genetic change over time to survive in shifting environments . These options are correct because they identify universal traits, whereas the other choices are factually incorrect: no organism ""exists indefinitely"" without reproduction, and many life forms rely on organic matter rather than ""wholly inorganic materials"" for energy. Furthermore, all living things are fundamentally affected by environmental changes, making the claim that they are ""unaffected"" biologically impossible They regulate internal environments for homeostasis. They display organized, ordered structures. They can adapt and evolve over generations. They exist indefinitely with no reproduction required. They generate energy wholly from inorganic materials. They are unaffected by environmental changes.']"
33,57,33_sud_sud sud_sud sud sud_sud sud sud sud,"['sud', 'sud sud', 'sud sud sud', 'sud sud sud sud', 'power minus', 'parenthesis open parenthesis', 'divided power', 'close parenthesis open', 'lambda', 'open parenthesis power']","[""Derive the following function:\nf(x) = f open parenthesis x close parenthesis   equals    square root of 3  divided by x to the power of 3   -\\frac{7}{3\\sqrt[3]{{x}^{10}}} Step 1: Identify which is the numerator/dividend and which is the denominator/divisor. The numerator/dividend will be denoted as u while the denominator/divisor will be v.\n\nTherefore, we have\n\nu  equals   square root of 3 \nv  equals  x to the power of 3 Step 2: Derive u and v individually.\n\nThen, we have\n\nu  equals   square root of 3 \nu  equals  x to the power of  2 divided by 3 \nu'  equals   2 divided by 3 x to the power of  minus  1 divided by 3 \n\nv  equals  x to the power of 3\nv'  equals  3x to the power of 2 Step 3: Substitute u, u', v and v' into the quotient rule formula.\n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals   vu'  minus  uv' divided by v to the power of 2 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    left  open parenthesis x to the power of 3 right  close parenthesis   times  left  open parenthesis  2 divided by 3 x to the power of  minus  1 divided by 3  right  close parenthesis   minus   left  open parenthesis x to the power of  2 divided by 3  right  close parenthesis   times  left  open parenthesis 3x to the power of 2 right  close parenthesis  divided by  left  open parenthesis x to the power of 3 right  close parenthesis  to the power of 2  Step 4: Simplify algebraically.\n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    left  open parenthesis x to the power of 3 right  close parenthesis   times  left  open parenthesis  2 divided by 3 x to the power of  minus  1 divided by 3  right  close parenthesis   minus   left  open parenthesis x to the power of  2 divided by 3  right  close parenthesis   times  left  open parenthesis 3x to the power of 2 right  close parenthesis  divided by  left  open parenthesis x to the power of 3 right  close parenthesis  to the power of 2 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    2 divided by 3 x to the power of  8 divided by 3   minus  3x to the power of  8 divided by 3  divided by x to the power of 6 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals    minus  7 divided by 3 x to the power of  8 divided by 3  divided by x to the power of 6 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals   minus   7 divided by 3 x to the power of  8 divided by 3  divided by x to the power of 6 \n\n d divided by dx  left  open parenthesis  u divided by v  right  close parenthesis   equals   minus  7 divided by 3x to the power of  10 divided by 3  \n\nThe final answer is:\n\nf' open parenthesis x close parenthesis   equals   minus  7 divided by 3 square root of 3  "", ""Find the eigenvalues and set of mutually orthogonal eigenvectors for the following matrix.\nA = \\begin{bmatrix} 3 & 2 & 4 \\\\ 2 & 0 & 2 \\\\ 4 & 2 & 3 \\end{bmatrix} In this problem, we will get three eigen values and eigen vectors since it's a symmetric matrix.\nTo find the eigenvalues, we need to minus lambda along the main diagonal and then take the determinant, then solve for lambda.\n|A - I\\lambda| = \\begin{vmatrix} 3-\\lambda & 2 & 4 \\\\ 2 & 0-\\lambda & 2 \\\\ 4 & 2 & 3-\\lambda \\end{vmatrix}\n equals   open parenthesis 3 minus  lambda  close parenthesis    minus  2   plus  4 \n equals   open parenthesis  open parenthesis 3 minus  lambda  close parenthesis  open parenthesis  open parenthesis  minus  lambda  close parenthesis  open parenthesis 3 minus  lambda  close parenthesis   minus   open parenthesis 2 close parenthesis  open parenthesis 2 close parenthesis  close parenthesis  close parenthesis   minus  2 open parenthesis  open parenthesis 2 close parenthesis  open parenthesis 3 minus  lambda  close parenthesis   minus   open parenthesis 2 close parenthesis  open parenthesis 4 close parenthesis  close parenthesis   plus  4 open parenthesis  open parenthesis 2 close parenthesis  open parenthesis 2 close parenthesis   minus   open parenthesis  minus  lambda  open parenthesis 4 close parenthesis  close parenthesis  close parenthesis \n equals   open parenthesis 3 minus  lambda  close parenthesis  open parenthesis  lambda  to the power of 2  minus  3 lambda  minus  4 close parenthesis   minus  2 open parenthesis  minus 2  minus  2 lambda  close parenthesis   plus  4 open parenthesis 4  plus  4 lambda  close parenthesis \n equals   minus  lambda  to the power of 3  plus  3 lambda  to the power of 2  plus  4 lambda  plus  3 lambda  to the power of 2  minus  9 lambda  minus  12  plus  4  plus  4 lambda  plus  16  plus  16 lambda \n equals   minus  lambda  to the power of 3  plus  6 lambda  to the power of 2  plus  15 lambda  plus  8\nThis can be factored to\n minus  open parenthesis  lambda  plus  1 close parenthesis  to the power of 2 open parenthesis  lambda  minus  8 close parenthesis   equals  0\nThus our eigenvalues are at  lambda  equals   minus 1, 8\nNow we need to substitute  lambda  into or matrix in order to find the eigenvectors.\nFor  lambda  equals   minus 1.\n(A + I)v = \\begin{bmatrix} 3-(-1) & 2 & 4 & | & 0 \\\\ 2 & 0-(-1) & 2 & | & 0 \\\\ 4 & 2 & 3-(-1) & | & 0 \\end{bmatrix}\n(A + I)v = \\begin{bmatrix} 4 & 2 & 4 & | & 0 \\\\ 2 & 1 & 2 & | & 0 \\\\ 4 & 2 & 4 & | & 0 \\end{bmatrix}\nNow we need to get the matrix into reduced echelon form.\nR subscript 1  minus  2R subscript 2  approaches R subscript 2\nR subscript 1  minus  R subscript 3  approaches R subscript 3\n\nThis can be reduced to\n\nThis is in equation form is 2x  plus  y  plus  2z  equals  0, which can be rewritten as y  equals   minus 2x  minus  2z. In vector form it looks like,  less than x,  minus 2x  minus  2z, z greater than .\nWe need to take the dot product and set it equal to zero, and pick a value for x, and z.\nLet x  equals  1, and z  equals  0.\n less than 1,  minus 2, 0 greater than   times  less than x,  minus 2x  minus  2z, z greater than   equals  0\nx  plus  4x  plus  4z  plus  0  equals  0\n5x  plus  4z  equals  0\nNow we pick another value for x, and z so that the result is zero. The easiest ones to pick are x  equals  4, and z  equals   minus 5.\nSo the orthogonal vectors for  lambda  equals   minus 1 are  less than 1,  minus 2, 0 greater than , and  less than 4, 2,  minus 5 greater than .\nNow we need to get the last eigenvector for  lambda  equals  8.\n(A + I)v = \\begin{bmatrix} 3-(8) & 2 & 4 & | & 0 \\\\ 2 & 0-(8) & 2 & | & 0 \\\\ 4 & 2 & 3-(8) & | & 0 \\end{bmatrix}\n(A + I)v = \\begin{bmatrix} -5 & 2 & 4 & | & 0 \\\\ 2 & -8 & 2 & | & 0 \\\\ 4 & 2 & -5 & | & 0 \\end{bmatrix}\nAfter row reducing, the matrix looks like\n\nSo our equations are then\nx  minus  z  equals  0, and 2y  minus  z  equals  0, which can be rewritten as x  equals  z, z  equals  2y.\nThen eigenvectors take this form,  less than 2y, y, 2y greater than . This will be orthogonal to our other vectors, no matter what value of y, we pick. For convenience, let's pick y  equals  1, then our eigenvector is  less than 2, 1, 2 greater than .  lambda  equals   minus 1,  minus 1, 8\n less than 1, 2, 0 greater than ,  less than 4, 2,  minus 5 greater than ,  less than 2, 1, 2 greater than   lambda  equals  8\n less than 1, 2, 0 greater than ,  less than 2, 1, 2 greater than  No eigenvalues or eigenvectors exist  lambda  equals  1, 8\n less than 4, 2,  minus 5 greater than ,  less than 2, 1, 2 greater than   lambda  equals   minus 1, 8\n less than 1, 2, 0 greater than ,  less than 4, 2,  minus 5 greater than "", 'Suppose we have an equality optimization problem as follows: Minimize f open parenthesis x, y close parenthesis   equals  x  plus  2y subject to x to the power of 2  plus  y to the power of 2  minus  4  equals  0. While solving the above equation we get x  equals   plus or minus  2 divided by  square root of   , y  equals   plus or minus  4 divided by  square root of   ,  lambda  equals   plus or minus   square root of   divided by 4 . At what value of x and y does the function f open parenthesis x, y close parenthesis  has its minimum value? When x  equals   minus  2 divided by  square root of   , y  equals   minus  4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 ,\n\nf open parenthesis x, y,  lambda  close parenthesis   equals  x  plus  2y  plus   lambda  open parenthesis x to the power of 2  plus  y to the power of 2  minus  4 close parenthesis \n\n equals   minus  2 divided by  square root of     plus  2 open parenthesis  minus  4 divided by  square root of    close parenthesis   plus or minus   square root of   divided by 4  open parenthesis  4 divided by 5   plus   16 divided by 5   minus  4 close parenthesis \n\n equals   minus  2 divided by  square root of     minus   8 divided by  square root of     plus or minus   square root of   divided by 4  open parenthesis  20 divided by 5   minus  4 close parenthesis \n\n equals   minus  10 divided by  square root of     plus or minus   square root of   divided by 4  open parenthesis 4  minus  4 close parenthesis \n\n equals   minus  10 divided by  square root of     plus or minus   square root of   divided by 4   times 0\n\n equals   minus  10 divided by  square root of   \n\nSimilarly when x  equals   2 divided by  square root of   , y  equals   4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 ,\n\nf open parenthesis x, y,  lambda  close parenthesis   equals   10 divided by  square root of   \n\nWhen x  equals   minus  2 divided by  square root of   , y  equals   4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 \n\nf open parenthesis x, y,  lambda  close parenthesis   equals   6 divided by  square root of   \n\nWhen x  equals   2 divided by  square root of   , y  equals   minus  4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 \n\nf open parenthesis x, y,  lambda  close parenthesis   equals   minus  6 divided by  square root of   \n\nSo the function f open parenthesis x, y close parenthesis  has its minimum value  open parenthesis  minus  10 divided by  square root of    close parenthesis  at x  equals   minus  2 divided by  square root of    and y  equals   minus  4 divided by  square root of   .  minus  2 divided by  square root of   ,  minus  4 divided by  square root of     2 divided by  square root of   ,  minus  4 divided by  square root of     minus  2 divided by  square root of   ,  4 divided by  square root of     2 divided by  square root of   ,  4 divided by  square root of   ']"
34,55,34_triangle_equilateral_trapezium_quadrilateral,"['triangle', 'equilateral', 'trapezium', 'quadrilateral', 'triangle sides', 'scalene', 'right angled', 'parallel sides', 'equal angles', 'equilateral triangle']","['To draw a perpendicular to a line from a point not on the line, you would use: You would use a compass to construct a perpendicular line from a point not on the line. The process involves drawing two arcs that intersect the line, and then drawing two more intersecting arcs from those points. The ruler is used to draw the final straight line connecting the original point to the intersection of the final arcs, which will be the perpendicular line. While other tools like a set square or protractor can be used to draw perpendicular lines, the standard geometric construction method relies on a compass and ruler. A protractor only A ruler only A compass and ruler A set square only', 'To draw a perpendicular to a line from a point not on the line, you use: To construct a perpendicular line from a point not on a given line, you must use a compass and a straightedge (ruler). A compass is used to draw arcs that intersect the line at two points, and these intersections are then used to create two more intersecting arcs. A ruler is then used to connect the original point with the intersection of these new arcs. This method ensures that the constructed line is exactly perpendicular  open parenthesis 90 to the power of ∘  close parenthesis  to the original line, as it relies on a precise geometric construction. Protractor only Ruler only Compass and ruler Set square only', ""Which instrument is essential for accurately measuring an angle on paper? A protractor is a specialized tool designed specifically for measuring and drawing angles. It typically has a semicircular or circular shape with degree markings from 0° to 180° (or 360°). To measure an angle, a user places the protractor's center point on the angle's vertex and aligns one of the angle's arms with the 0° baseline. The measurement is then read where the other arm of the angle intersects the protractor's scale. Other instruments like a ruler measure length, a divider measures distance between two points, and a set square is used to draw specific angles like 90° or 45°, but none are universally capable of measuring any angle with accuracy like a protractor. Divider Protractor Ruler Set square""]"
35,54,35_consonant vowel vowel vowel_argument_true true_argument structure,"['consonant vowel vowel vowel', 'argument', 'true true', 'argument structure', 'standard way notating argument', 'structure valid form argument', 'structure disjunctive', 'structure disjunctive syllogism', 'structure argument', 'structure valid form']","['Is I a consonant? I is not a consonant, it is a vowel.', 'Is U a consonant? U is not a consonant, it is a vowel.', 'Is O a consonant? O is not a consonant, it is a vowel.']"
36,53,36_programming_recursion_modular programming_modules,"['programming', 'recursion', 'modular programming', 'modules', 'calls', 'module', 'arrays objects', 'garbage collection', 'javac', 'memory usage']","[""After modular programming, object-oriented programming is the next step in modern Java programming models, extending capabilities further. The statement is generally considered true in the context of Java's evolution because while Object-Oriented Programming (OOP) is the fundamental paradigm of Java itself, the introduction of Modular Programming (the Java Platform Module System, or JPMS, in Java 9) represents a next step in organizing and scaling large applications. Modular programming extends the organizational benefits of OOP by structuring code into named, self-contained modules, improving security, maintainability, and reliability by strictly defining dependencies and encapsulated access, which is a modern enhancement to the core OOP model."", 'Java primitive types like int and double use fixed-size memory representations regardless of runtime environment. The statement is False because Java primitive types like  text  (32-bit signed integer) and  text  (64-bit double-precision float) are defined by the Java Language Specification to have a fixed, platform-independent size . This design choice ensures that Java code, when executed on any Java Virtual Machine (JVM), behaves consistently and predictably, adhering to the principle of ""Write once, run anywhere."" If their sizes were dependent on the runtime environment (like in languages such as C/C++), portability and predictability would be lost.', 'What is a key reason that older non-modular programs are difficult to maintain? Older, non-modular programs are difficult to maintain primarily due to high coupling and low cohesion. When a program lacks modularity, there are no clear boundaries between different parts of the code . This means that variables and data are globally accessible or extensively shared, resulting in a system where a change in one single statement or function can unexpectedly and uncontrollably affect variables and logic used throughout the entire codebase. This pervasive interdependence makes debugging and modifying the program an extremely complex, error-prone, and time-consuming process. Any statement can affect or be affected by any variable in a long sequence of code They are written in a modern language They avoid variables They use too many classes']"
37,53,37_phagosome_phagosomes_phagocytosis_phagocytes,"['phagosome', 'phagosomes', 'phagocytosis', 'phagocytes', 'pinocytosis', 'lysosomes', 'plasmodesmata', 'professional phagocytes', 'phagolysosomes', 'early phagosomes']","['How do plasmodesmata-located proteins (PDLPs) regulate plasmodesmata during stress? Plasmodesmata-located proteins (PDLPs) are cell-wall proteins in plants that manage the size exclusion limit of plasmodesmata, the cytoplasmic channels connecting adjacent cells . During conditions of biotic or abiotic stress, the cell needs to restrict intercellular transport to isolate damaged areas. PDLPs achieve this by actively recruiting or stabilizing callose synthase at the plasmodesmata aperture. This leads to the accumulation of callose (a  beta -1,3-glucan) in the cell wall surrounding the channel, which causes the plasmodesmata to narrow or close, thereby stopping the movement of molecules and pathogens between cells.  They promote plasmodesmata closure through callose accumulation. They inhibit protein synthesis in plant cells. They degrade callose to open plasmodesmata. They transport hormones across plasmodesmata.', ""What happens to matter incorporated into phagosomes? The primary fate of matter incorporated into a phagosome (a vesicle formed during phagocytosis) is fusion with a lysosome. This fusion creates a phagolysosome, where the lysosome delivers a potent mixture of hydrolytic enzymes into the vesicle. These enzymes are specifically designed to break down macromolecules—such as bacteria, cellular debris, or consumed particles—into their basic components (e.g., amino acids, sugars, nucleotides). Once broken down, these useful components are transported out of the phagolysosome and into the cytosol for the cell's reuse or energy production. The other options are incorrect as they describe either pinocytosis/export, an incorrect fusion process, or release of contents after digestion but miss the crucial fusion step. The phagosome fuses with a lysosome, which contains enzymes that break down consumed material for use by the cell The phagosome fuses with a vesicle resulting from pinocytosis in a pathway leading to export of waste matter out of the cell The phagosome fuses with the cell membrane and releases the material into the external environment The phagosome itself and the matter inside, releasing the products of digestion in the cell interior"", ""What is the relationship between the surface area-to-volume ratio and cell size? As a cell increases in size, its volume increases at a much faster rate (cubed) than its surface area (squared). This means the surface area-to-volume ratio (SA/V) decreases as the cell gets larger. A high SA/V ratio is essential for efficient material exchange, as it allows nutrients and waste to quickly cross the cell membrane and reach the cell's interior. Therefore, smaller cells maintain a high SA/V ratio, which is why most cells are microscopic, ensuring metabolic efficiency. Smaller cells have higher surface area-to-volume ratios than larger cells. The ratio only matters for plant cells, not animal cells. Larger cells have higher surface area-to-volume ratios than smaller cells. Cell size does not affect the surface area-to-volume ratio.""]"
38,50,38_rhymes_rhyme_rhyming_rhyming words,"['rhymes', 'rhyme', 'rhyming', 'rhyming words', 'sounds rhyming', 'sounds rhyming words', 'similar sounds rhyming', 'similar sounds rhyme', 'sounds rhyme', 'similar sounds rhyming words']","['What rhymes with “beep”?\n Sheep and beep both have similar sounds so they are rhyming words. Sheep Dog Wolf Cat', 'What rhymes with “fan”?\n Fan and Pan have similar sounds so they are rhyming words. Pan Light Cat Fat', 'Following the First World War, dress among Westerners became increasingly The late-twentieth-century phenomenon of people traveling, attending church, or even going shopping in casual clothing was unique. Into the 1950s it was unusual for a man to be seen in public in anything other than a coat and tie. The teen cultures of the 1920s and 1950s, which emphasized distinct dress for youth, promoted an increasingly casual approach to dress. casual formal monotone homemade dirty']"
39,49,39_deep neural_language_improved selecting_language models,"['deep neural', 'language', 'improved selecting', 'language models', 'neural', 'variance error terms', 'example supervised', 'deep', 'large language models', 'large language']","['The 3D structure of a protein can be predicted from its amino acid sequence using a deep learning model trained on known sequences and structures, which is an example of supervised learning. ??', 'The training approach where labelled outputs are obtained automatically from the input training data without needing separate human-derived labels is called self-supervised learning. ??', 'The formula SE subscript B open parenthesis  hat  close parenthesis  estimates the standard error of  hat  calculated from the original data set using bootstrap samples. initial first primary principal main ??']"
40,48,40_naive bayes_naive_models_high dimensional,"['naive bayes', 'naive', 'models', 'high dimensional', 'regression', 'images', 'skin lesion', 'pcr ridge', 'number predictors', 'generative models']","[""Why might naive Bayes outperform LDA or QDA when the number of predictors is large and data is limited? The Naive Bayes classifier often outperforms more complex models like  text  or  text  when the number of predictors (p) is large and the amount of data (N) is limited because it makes a strong, simplifying assumption: that all predictors are conditionally independent given the class. This assumption drastically reduces the number of parameters that need to be estimated for the class-conditional density functions, thereby reducing the model's variance. This regularization effect is particularly beneficial in high-dimensional, low-sample-size scenarios, preventing the model from overfitting where complex models like  text  would likely fail due to insufficient data to estimate their covariance matrices accurately. Because naive Bayes reduces variance by assuming independence, simplifying density estimation. Because naive Bayes ignores predictor values. Because naive Bayes always uses more data than other methods. Because naive Bayes uses more complex models."", 'Why is cross-validation essential when choosing the tuning parameter in models like the lasso for high-dimensional data? Cross-validation is necessary because the tuning parameter directly controls the bias-variance tradeoff; a value that is too small leads to overfitting (high variance), while one that is too large leads to underfitting (high bias). By evaluating model performance on held-out data, cross-validation identifies the optimal regularization amount that minimizes the expected test error rather than just the training error. Other options are incorrect because the goal of lasso is often to reduce, not maximize, the number of predictors, and cross-validation does not eliminate the need for standardization, which is a separate preprocessing step. Furthermore, it does not minimize training error—which is lowest when there is no regularization—nor does it inherently guarantee the selection of the fewest possible variables, as the ""optimal"" model might require several predictors to remain accurate. It helps select the regularization amount balancing bias and variance. Because it maximizes number of predictors included in the final model. Because it eliminates the need to standardize predictors. Because it always minimizes training error perfectly. Because it guarantees selection of the fewest variables possible.', 'Why is it often unnecessary to compute the constant factor c in the running time expression T(n) ~ c * f(n)? The primary goal of analyzing algorithm efficiency is to determine the asymptotic behavior—how the running time T(n) scales as the input size n approaches infinity, which is captured by the function f open parenthesis n close parenthesis  (the time complexity, e.g., O(n \\log n)). When comparing two algorithms, T subscript 1 open parenthesis n close parenthesis   sim c subscript 1  times f subscript 1 open parenthesis n close parenthesis  and T subscript 2 open parenthesis n close parenthesis   sim c subscript 2  times f subscript 2 open parenthesis n close parenthesis , the one with the slower growth rate (the smaller f open parenthesis n close parenthesis ) will eventually be superior, regardless of the constants c subscript 1 and c subscript 2. The constant factor c captures machine-specific details, such as hardware speed and implementation efficiency, which are ignored in Big O notation as it focuses on the dominant term f open parenthesis n close parenthesis . The other options are incorrect: constants do not dominate performance at large input sizes (the function f open parenthesis n close parenthesis  does); they do depend on algorithm design and implementation; constants are not always zero; and while constants vary with hardware, their irrelevance in Big O is due to the focus on the growth rate, not solely their dependence on hardware. Because constant factors cancel out in relative running time comparisons. Because constants dominate the performance at large input sizes. Because constants depend only on input data, not algorithm design. Because constants always equal zero in running time analyses. Because constants vary with hardware but not with input size.']"
41,47,41_perpendicular_straight line_angles equal_circle,"['perpendicular', 'straight line', 'angles equal', 'circle', 'parallelogram', 'quadrilateral', 'diagonals equal', 'right angles', 'point straight line', 'point straight']","['The shortest distance between a point and a line is: The shortest distance between a point and a line in a two-dimensional space is always a line segment that is perpendicular to the original line. This principle is a fundamental concept in geometry. Think of it like this: if you were standing at a point and wanted to walk the shortest possible distance to a straight road, you would walk straight towards the road at a 90-degree angle. Any other path would be longer because it would involve moving along a hypotenuse of a right-angled triangle. This perpendicular line segment represents the unique shortest path from the point to the line. Any line from the point to the given line  The perpendicular from the point to the line A line parallel to the x-axis A line making 45° with the given line', ""Which construction is used to find the midpoint of a line segment? A perpendicular bisector is a line that intersects a line segment at its exact center, or midpoint, and forms a 90-degree angle with it. To construct this, you use a compass to draw intersecting arcs from each endpoint of the line segment. The line drawn through the two points where the arcs intersect is the perpendicular bisector, and where it crosses the original line segment is the midpoint. The other options are incorrect: a tangent touches a circle at one point, a simple circle doesn't find a midpoint, and a parallel line never intersects the original line. Drawing a perpendicular bisector  Drawing a tangent Drawing a circle Drawing a parallel line"", 'What is the first step in bisecting a straight line segment? The first step in bisecting a line segment with a compass and straightedge is to set your compass to a width that is greater than half the length of the segment. This is essential because it ensures that when you draw arcs from both endpoints of the line, they will intersect at two distinct points. These intersection points are used to draw the perpendicular bisector, which cuts the original line segment into two equal halves. If the compass opening were less than half, the arcs would not cross, making it impossible to find the bisection point. Draw a perpendicular line Use a protractor to measure half the length Open the compass to more than half the length Draw a parallel line']"
42,42,42_memory_recollection_items_participants,"['memory', 'recollection', 'items', 'participants', 'indirect', 'repetitions', 'memory tasks', 'memory research', 'trials', 'incidental learning']","[""What problem did Müller's memory drum address in the presentation of memory items? The Müller memory drum was an early experimental device in psychology designed to address a critical methodological problem: controlling the exact timing and presentation of memory stimuli. Its primary purpose was to present one syllable (or item) at a time, for a precise, brief duration, which effectively prevented participants from engaging in uncontrolled covert rehearsal of previously seen items while the list was still being presented. By strictly regulating the item-by-item exposure and interval, Müller's apparatus ensured that all participants received the same controlled conditions for initial encoding, which was necessary for rigorous memory research. It prevented participants from rehearsing items during presentation It produced auditory cues to support encoding through phonetic repetition It allowed participants to see entire lists at once to improve rehearsal ability It extended exposure time for each syllable to enhance memorization"", ""What was a critical feature that distinguished Müller's laboratory experiments from Ebbinghaus's? While Ebbinghaus pioneered the experimental study of memory, he primarily served as his own participant, focusing on a single case study. Georg Elias Müller, by contrast, introduced greater experimental control and laboratory rigor to memory studies, including the use of controlled apparatus (like the memory drum for presenting stimuli) and, most critically, testing larger groups of participants. This shift from self-experimentation to studying groups with controlled methods allowed for the calculation of mean effects and enhanced the generalizability and statistical validity of memory research, moving the field towards modern psychological methods. Müller studied large groups with controlled apparatus, improving rigor Müller avoided timing controls and allowed self-paced learning sessions Müller only tested a single participant repeatedly under similar conditions Müller relied solely on introspection without experimental equipment"", ""What role does the standard deviation of strength distributions play in strength theory's predictions for recognition memory? Equal standard deviations predict a symmetric ROC (Receiver Operating Characteristic), as this equal variance assumption simplifies the model and results in an ROC curve that is symmetric around the negative diagonal when plotted on standard axes. However, empirical data often show a non-symmetric ROC, which is why the model is often refined: Unequal variance better models ROC curves observed in real memory experiments, where the distribution for targets is often broader (more variable) than for lures. This leads to the third correct point: More target variability produces zROC slopes less than one when the ROC is plotted in z-score space, because the slope of the zROC is the ratio of the lure standard deviation ( sigma  subscript  text ) to the target standard deviation ( sigma  subscript  text ), and if  sigma  subscript  text   greater than   sigma  subscript  text , the ratio is less than one. The other options are incorrect because standard deviation does have a critical impact on accuracy (it affects overlap); decreased variance narrows the distribution and steepens the ROC curve; and standard deviations do not always match between targets and lures, as the unequal variance model is often preferred. Equal standard deviations predict a symmetric ROC Unequal variance better models ROC curves More target variability produces zROC slopes less than one Standard deviation has no impact on recognition accuracy Decreased variance broadens the ROC curve Standard deviations always match between targets and lures""]"
43,41,43_cell cycle_mitosis_cell division_cycle,"['cell cycle', 'mitosis', 'cell division', 'cycle', 'daughter cells', 'interphase', 'eukaryotic cell', 'phase mitosis', 'chromatids', 'cell cytokinesis']","[""Mitochondria and chloroplasts must be inherited by daughter cells through mitosis and cytokinesis. Mitochondria and chloroplasts are semi-autonomous organelles essential for cellular function (energy production and photosynthesis, respectively). Unlike nuclear DNA which is precisely divided by mitosis, these organelles replicate independently within the parent cell. During the final stage of cell division, cytokinesis, the parent cell's cytoplasm, which contains the replicated organelles, is partitioned into the two new daughter cells. Since these organelles cannot be synthesized de novo by the cell, a sufficient number of them must be inherited by each daughter cell through this physical division process of mitosis and cytokinesis to ensure the viability and proper functioning of the new cells."", 'The unfolded protein response (UPR) consists of three parallel branches activated upon stress, including inositol requiring enzyme 1 PERK, and ATF6 pathways. ire1 The Unfolded Protein Response (UPR) is a crucial cellular stress pathway that is activated when unfolded proteins accumulate in the Endoplasmic Reticulum (ER). It is defined by three main, parallel signaling branches: the pathways mediated by IRE1 (inositol requiring enzyme 1), PERK (PKR-like ER kinase), and ATF6 (activating transcription factor 6). These three components act as sensors that initiate distinct corrective transcriptional and translational responses to restore ER homeostasis.', 'The ER contains several calcium channels, ryanodine receptors and inositol 1,4,5-trisphosphate (IP3) receptors (IP3R) that are responsible for releasing Ca2+ from the ER into the cytosol when intracellular levels are low. ip3   text  subscript 3 receptors (\\text{IP}_3\\text{R}) are ligand-gated calcium channels located on the Endoplasmic Reticulum (ER) membrane, alongside ryanodine receptors (RyR), which are another type of calcium release channel.  When a signaling pathway activates the production of  text  subscript 3 in the cytosol, it binds to the \\text{IP}_3\\text{R} on the ER. This binding causes the receptor to open, leading to the rapid and massive release of \\text{Ca}^{2+} from the ER into the cytosol, which acts as a crucial second messenger for numerous cellular processes.']"
44,40,44_research_scientific_item_ethical,"['research', 'scientific', 'item', 'ethical', 'memory strength', 'ignoring', 'recall', 'plagiarism', 'strength theory', 'commercial gain']","['Which are important goals a biologist might pursue given the broad scope of biology? Biology ranges from the microscopic scale (cell biology and molecular structures) to the macroscopic scale (ecology and planetary dynamics). Furthermore, a central application of biology is solving real-world problems, such as finding disease cures like those for cancer or AIDS. The remaining options are incorrect because biology is not limited to human anatomy, it explicitly addresses environmental issues, and successful modern biological research increasingly relies on interdisciplinary approaches (like biochemistry, biophysics, and bioinformatics). Studying cellular structures at microscopic scale. Finding disease cures, such as cancer or AIDS. Investigating ecosystem and planetary dynamics. Limiting study only to human anatomy and physiology. Ignoring environmental issues and focusing solely on labs. Avoiding interdisciplinary approaches in research fields.', 'Which of these statements capture reasons why scientific writing follows set guidelines? These guidelines are critical to maintain logic and rigor in research and to avoid plagiarism by properly citing sources. They also ensure the clear, concise sharing of findings, which is vital for effective communication within the scientific community. Most importantly, standardized reporting allows other researchers to reproduce the work, which is a cornerstone of the scientific method for validating results. The unchecked options describing eliminating references, using a verbose style, or allowing creative expression without accuracy contradict the core principles of scientific integrity and reproducibility. To keep logic, rigor, and avoid plagiarism. To ensure clear, concise sharing of findings. To allow other researchers to reproduce work. To eliminate references to past scientific work. To present scientific results in a verbose, narrative style. To allow creative expression without data accuracy.', ""What ethical questions arise from the Henrietta Lacks case? These selections address the core bioethical violations surrounding the HeLa cell line, which was established without her permission or her family's awareness. These options highlight the modern standards of informed consent, equitable compensation, and proper attribution that were ignored during her treatment and subsequent decades of commercial research. In contrast, accepting tissue use without review, prioritizing commercial gain over ethics, and ignoring patient identity are the very practices that created the controversy and are universally rejected by current ethical frameworks. Consequently, the case serves as a landmark lesson in the necessity of protecting patient rights and bodily autonomy in scientific exploration. Sharing financial benefits from cell research. Consent and knowledge before taking tissue samples. Recognizing Henrietta Lacks in publications, awards. Accepting tissue use without any ethical review. Prioritizing commercial gain over ethical considerations. Ignoring patient identity when conducting research.""]"
45,32,45_fibonacci_fibonacci numbers_numbers_fibonacci number,"['fibonacci', 'fibonacci numbers', 'numbers', 'fibonacci number', 'fibonacci primes', 'fibonacci sequence', 'strings', 'prime indices', 'modulo', 'indices fibonacci']","[""What does Zeckendorf's theorem state about positive integers and Fibonacci numbers? Zeckendorf's theorem is a fundamental result in number theory stating that any positive integer can be written as the sum of one or more non-consecutive Fibonacci numbers, and this representation is unique . The requirement that the Fibonacci numbers are non-consecutive is crucial for uniqueness. The other options are incorrect statements about the theorem: it applies to all positive integers, not just primes or multiples, and it confirms a unique representation exists. Every positive integer can be uniquely represented as a sum of non-consecutive Fibonacci numbers. Fibonacci numbers can represent only prime integers uniquely. Every positive integer is a multiple of some Fibonacci number. There is no representation of integers using Fibonacci numbers."", 'What is the defining rule of the Fibonacci sequence recurrence relation for n > 1?  The Fibonacci sequence (F subscript n) is defined by the recurrence relation for n  greater than  1 as: F subscript n  equals  F subscript n minus 1  plus  F subscript n minus 2. This means that any number in the sequence (after the first two starting terms, typically F subscript 1 equals 1 and F subscript 2 equals 1 or F subscript 0 equals 0 and F subscript 1 equals 1) is generated by adding the two terms immediately before it. For example, the third term (F subscript 3) is F subscript 2  plus  F subscript 1  equals  1  plus  1  equals  2. The other options describe different types of sequences: a geometric sequence (product of terms), a simple arithmetic sequence (adding 1), or a multiplicative sequence (doubling the previous term), none of which define the Fibonacci sequence. Each term is the sum of the two preceding terms. Each term is the product of the two preceding terms, Fn = Fn−1 × Fn−2. Each term is one more than the previous term, Fn = Fn−1 + 1. Each term is double the previous term, Fn = 2 × Fn−1.', 'How does the Fibonacci sequence relate to the golden ratio as n increases? The Fibonacci sequence, defined by F subscript n  equals  F subscript n minus 1  plus  F subscript n minus 2 with starting values F subscript 1 equals 1 and F subscript 2 equals 1, exhibits a property where the ratio of any term to its immediately preceding term, represented by the limit  limit  subscript n  approaches  infinity   F subscript n divided by F subscript n minus 1 , converges to the Golden Ratio ( phi ) . The Golden Ratio is an irrational number approximately equal to 1.618. The other options are incorrect: the ratio tends towards  phi  (not zero or infinity); and the ratio of alternate terms converges to  phi  to the power of 2  approximately equal to 2.618, not the square root of two ( approximately equal to 1.414). The ratio of two consecutive Fibonacci numbers tends towards the golden ratio. The ratio of two consecutive Fibonacci numbers tends towards zero rapidly. The ratio of alternate Fibonacci numbers tends towards the square root of two. The ratio of two consecutive Fibonacci numbers tends towards infinity as n increases.']"
46,30,46_trisomy_aneuploidy_polyploidy_aneuploid,"['trisomy', 'aneuploidy', 'polyploidy', 'aneuploid', 'copies chromosome', 'trisomy 21', 'tetrasomy', 'autosomal aneuploidy', 'trisomy 16', 'abnormal chromosome']","[""How is trisomy 21 related to human genetic disorders? Trisomy 21 is the genetic term for Down syndrome and is defined by the presence of three copies of chromosome 21 instead of the usual two , making the second option correct. This extra genetic material is the direct cause of the syndrome, which presents with a characteristic set of physical and intellectual features, thus validating the first option. Statistically, Trisomy 21 is a common chromosomal disorder, affecting approximately 1 in every 1,000 live births, making the third option correct. The incorrect options state a missing chromosome (Monosomy 21), that it's unrelated to aneuploidy (it is a form of aneuploidy), or that it guarantees disability with no variability (intellectual disability varies widely). It causes Down syndrome with distinctive characteristics. It results from an extra copy of chromosome 21. It affects about 1 in 1,000 live births. It involves missing chromosome 21. It is unrelated to aneuploidy. It guarantees intellectual disability with no variability."", 'What is the most common autosomal aneuploidy among live human births? it is the most frequent chromosomal abnormality compatible with survival to birth, occurring in approximately 1 in 700 live births. While Trisomy 16 is actually the most common autosomal aneuploidy overall, it is invariably fatal in the first trimester and never survives to a live birth, making that option incorrect. Trisomy 18 (Edwards syndrome) and Trisomy 13 (Patau syndrome) are also incorrect because they occur much less frequently than Trisomy 21 and have significantly lower survival rates beyond infancy . Additionally, Monosomy X (Turner syndrome) is a sex chromosome aneuploidy rather than an autosomal one, excluding it from the category requested by the question. Therefore, Trisomy 21 stands as the only correct choice for the most prevalent autosomal aneuploidy found among live-born human populations. Trisomy 21 is the most common autosomal aneuploidy. Trisomy 16 is the most common and survives until birth. Trisomy 18, Edwards syndrome, is the most frequent. Monosomy X, Turner syndrome, is the predominant autosomal aneuploidy. Trisomy 13, Patau syndrome, is the most common autosomal aneuploidy.', 'Which types of chromosome number abnormalities are described and their characteristics? Trisomy is defined as the presence of three copies of a chromosome instead of the normal two, and this includes both autosomes (like Down Syndrome, Trisomy 21) and sex chromosomes (like Klinefelter Syndrome, XXY). Similarly, Tetrasomy and Pentasomy are correctly described as having four or five copies of a chromosome, respectively, which are most commonly observed in sex chromosome aneuploidies. Monosomy is the accurate term for the condition of losing one entire chromosome, resulting in one copy instead of two, and this term is also sometimes used to describe the loss of a part of a chromosome. The unselected options are incorrect: Pentasomy involves extra chromosomes, not their absence (Turner syndrome is a monosomy); partial tetrasomy refers to a segment, and the loss of entire chromosome sets is polyploidy, not partial tetrasomy; and Disomy refers to the normal state of having two copies, not four. Trisomy is three copies, including sex chromosome types. Tetrasomy and pentasomy are four or five copies, mainly in sex chromosomes. Monosomy is losing one chromosome, sometimes only part. Pentasomy includes absence of chromosomes, leading to Turner syndrome. Partial tetrasomy involves loss of entire chromosome sets, rare in humans. Disomy refers to four copies of each chromosome, typical in diploid organisms.']"
47,29,47_sum interior angles_interior angles_interior_exterior,"['sum interior angles', 'interior angles', 'interior', 'exterior', 'exterior angles', 'angles polygon', 'sum exterior angles', 'sum exterior', 'angles sum', '270 360 540']","['What is the sum of the exterior angles of any triangle (one at each vertex)? The sum of the exterior angles of any convex polygon, including a triangle, is always 360 to the power of ∘. An exterior angle is formed by extending one side of the polygon and the adjacent side. This principle holds true regardless of the number of sides the polygon has. For a triangle specifically, the three exterior angles, one at each vertex, will always add up to 360 to the power of ∘. 180°  270° 360° 540°', 'What do we call two angles that sum to 180°? The correct answer is Supplementary angles. Two angles are defined as supplementary if their measures add up to exactly 180 to the power of ∘. A simple way to remember this is that a straight line forms a 180 to the power of ∘ angle, and two angles that form a straight line together are supplementary. In contrast, complementary angles sum to 90 to the power of ∘, while adjacent and vertical angles describe a relationship in position rather than a sum. Supplementary angles Complementary angles Adjacent angles Vertical angles', 'What is the sum of the measures of the four angles of a rectangle? A rectangle is a special type of quadrilateral, which is a polygon with four sides. It has two key properties:\r\n\r\n    All four of its angles are right angles, meaning each angle measures exactly 90 degrees.\r\n\r\n    Its opposite sides are parallel and equal in length.\r\n\r\nSince a rectangle has four right angles, the sum of their measures is:\r\n\r\n90 to the power of ∘ plus 90 to the power of ∘ plus 90 to the power of ∘ plus 90 to the power of ∘ equals 360 to the power of ∘\r\n\r\nThis principle also applies to all quadrilaterals, as the sum of their interior angles is always 360 degrees. 180°  270° 360° 540°']"
48,28,48_aneuploidy_sperm_sickle cell_cell disease,"['aneuploidy', 'sperm', 'sickle cell', 'cell disease', 'sickle cell disease', 'chromosome 13 disomy', '13 disomy', 'beta globin', 'ldha deficiency', 'mitotic checkpoints']","['How can aneuploidy contribute to tumorigenesis and cancer progression? Aneuploidy (having an abnormal number of chromosomes) creates genomic instability by altering gene dosage, which increases the mutation rate and aids in tumor evolution and resistance to therapy . Specifically, trisomy 12 is a known common finding in certain cancers, such as chronic lymphocytic leukemia. Finally, defects or alterations in mitotic checkpoints (which normally ensure correct chromosome segregation) are a direct cause of the unequal chromosome partitioning that produces aneuploid cells in a tumor. Aneuploidy creates instability, aiding tumor evolution. Trisomy 12 is found in chronic lymphocytic leukemia. Altered mitotic checkpoints in cancer produce aneuploid cells. Aneuploidy exclusively arises from inherited germline mutations. Aneuploidy always prevents cancer cell proliferation. Mitotic checkpoints accelerate proper chromosome segregation in tumors.', ""Why is sickle-cell disease associated with a mutation in the β-globin gene? Sickle-cell disease is a classic example of a genetic disorder caused by a single point mutation in the  beta -globin gene (HBB) . Specifically, a change from an adenine (A) to a thymine (T) in the DNA sequence for codon 6 of the  beta -globin chain changes the mRNA codon from GAG to GUG. This substitution results in the amino acid glutamic acid (a hydrophilic residue) being incorrectly replaced by valine (a hydrophobic residue) at the sixth position. This small change in the protein's primary structure causes the hemoglobin to polymerize under low oxygen conditions, leading to the characteristic sickle shape of red blood cells. Point mutation causes glutamic acid replaced by valine at position 6. A deletion mutation removes the entire β-globin gene from chromosome 11. The mutation substitutes valine with lysine at a non-functional site. The disease results from an insertion adding extra amino acids. The β-globin mutation only affects RNA stability, not protein."", ""Why do substitutions at position 6 of the β-globin chain in hemoglobin cause sickle-cell disease? Sickle-cell disease (SCD) is caused by a point mutation in the gene encoding the  beta -globin chain of hemoglobin. This mutation results in the substitution of the charged, hydrophilic amino acid Glutamic acid (Glu) with the non-polar, hydrophobic amino acid Valine (Val) at the sixth position of the chain . This change creates a hydrophobic pocket on the surface of the hemoglobin molecule (HbS). In low-oxygen conditions, this altered charge and structure cause the  text  molecules to aggregate and polymerize into long fibers, deforming the red blood cells into a characteristic sickle shape, which leads to the symptoms of the disease. Glutamic acid replaced by valine changes charge and structure. Valine substitution increases hemoglobin's oxygen binding affinity. Position 6 mutations enhance red blood cell flexibility. Glutamic acid replaced by lysine eliminates all sickling features. Charge changes prevent hemoglobin from binding carbon dioxide.""]"
49,28,49_basis functions_nonlinear_neural networks_neural,"['basis functions', 'nonlinear', 'neural networks', 'neural', 'nonlinear activation', 'nonlinear activation functions', 'deep', 'svms', 'hidden units', 'backpropagation']","[""How does local deep learning differ from standard backpropagation? Standard backpropagation relies on a chain rule calculation that propagates the error gradient from the output layer all the way back through the network, requiring a global error signal and synchronized updates across all layers. Local deep learning, in contrast, is an approach designed to mitigate this non-local dependency by allowing each layer (or module) to be trained using a locally available signal, such as a target or error computed only on that layer's output, thereby omitting the need for the global gradient that passes through all preceding weights. This local update rule contrasts sharply with backpropagation, which inherently requires the knowledge of the final network loss. Local deep learning omits global gradients and uses only local info. Local deep learning ignores output layer supervision. Local deep learning calculates full gradients explicitly. Backpropagation uses only local neuron data."", ""What does the fundamental limitation of deep local learning imply about its practical usage? Deep local learning (like purely Hebbian learning) relies only on information available locally at a synapse, typically the activity of the pre- and post-synaptic neurons, to update weights. The fundamental limitation is that it cannot effectively utilize the global error signal (the network's final output error) to adjust weights throughout the network, particularly in deeper layers. In contrast, backpropagation propagates this global error backward. Therefore, deep local learning methods may not find the globally or even locally optimal set of weights to minimize the overall error, making them less effective than backpropagation for training complex, deep neural networks in real-world applications where optimal error minimization is essential. It may not find weights to minimize error optimally in deep networks. It does not depend on local information at synapses. It is the perfect method for all deep networks. It can always replace backpropagation without issues."", 'What main advantage does a multilayer network have compared to a single hidden layer network? The main advantage of a multilayer (deep) neural network lies in its hierarchical structure, allowing it to compose complex functions from simpler, intermediate representations . While the Universal Approximation Theorem states that a single hidden layer can theoretically model any function, it often requires an exponentially large number of neurons for complex problems, making training computationally intractable and leading to poor generalization. Multilayer networks, by contrast, can achieve the same complexity with fewer overall parameters spread across several modest-sized layers, which simplifies the learning process, improves feature extraction, and makes the model easier to train efficiently. Multiple layers allow building complex functions from modest-sized layers, simplifying learning compared to one very large layer. Multilayers improve interpretability by reducing the number of parameters required. A single hidden layer can model any function exactly but requires exponential computation time. Single-layer networks can only perform linear regression, unable to fit nonlinear data. Multilayers always guarantee perfect fitting whereas single layers do not approximate functions well.']"
50,26,50_memory_mental_internal context_internal mental,"['memory', 'mental', 'internal context', 'internal mental', 'early memory', 'cognitive approach', 'cognitivism emphasized', 'internal mental states', 'mental processes', 'mental states']","['Ebbinghaus adhered to an experimental approach focusing on complex real-life memory phenomena rather than simplified controlled studies. Hermann Ebbinghaus pioneered the use of highly controlled, simplified studies to investigate memory, which is the opposite of focusing on complex, real-life phenomena. To ensure precise measurement and minimize prior knowledge influence, he famously invented nonsense syllables (CVCs like DAX or KEF) and systematically studied their memorization and forgetting. His entire approach was based on isolating memory as a psychological process under strict experimental conditions, moving away from complex, ecologically valid memory tasks.', 'How did early learning and memory research differ from the cognitive approach developed in the late 1950s? Early research emphasized the relationship between learning experiences and recall, whereas cognitivism focused on internal latent processes that mediate learning beyond observable behaviors. Early focus was on recall; cognitivism emphasized internal processes. Early memory research centered on unobservable cognitive stages instead of recall. Early research emphasized abstract theories, while cognitive approach ignored internal processes. Early research prioritized computational models over observable learning behaviors.', ""Early behavioristic learning scholars embraced internal context as a measurable construct for memory studies. early behaviorism, championed by figures like Watson and Skinner, explicitly rejected the study of internal mental states, including internal context, which they considered unobservable and non-measurable. Behaviorists focused strictly on observable stimuli and responses (S-R) and external, environmental factors to explain learning and memory. The concept of internal context, which suggests that a person's mood, state, or thoughts influence memory, belongs to the later school of cognitive psychology, which arose largely as a reaction against behaviorism.""]"
51,25,51_hidden units_differentiable_neural network_units,"['hidden units', 'differentiable', 'neural network', 'units', 'derivatives', 'input vector', 'interpolation', 'spaces', 'weights biases', 'equivalent weight vectors']","['A feed-forward neural network adapts both the coefficients and basis function parameters during training. a feed-forward neural network is a flexible parametric model that learns complex patterns by adjusting multiple sets of internal parameters. During the training process, the network optimizes the coefficients (weights) connecting different layers, which determine the strength of the signal between neurons. Simultaneously, it adapts the basis function parameters—such as the weights and biases within hidden layers—which essentially define the shape and location of the non-linear transformations applied to the input data.', 'Two-layer neural networks with tanh activation and linear output units can approximate functions including step and absolute value functions. A two-layer neural network with a non-linear, non-constant activation function in the hidden layer (like  hyperbolic tangent  or sigmoid) and a linear output layer is theoretically capable of approximating any continuous function to an arbitrary degree of accuracy, provided it has a sufficient number of hidden units. Since the step function (which is piecewise continuous) and the absolute value function are relatively simple functions, they can certainly be approximated by a network with  hyperbolic tangent  hidden units by cleverly combining the shifted and scaled  hyperbolic tangent  responses.', 'The outer product approximation of the Hessian involves summing outer products of gradient vectors for each data point. The Hessian matrix contains the second-order partial derivatives and is computationally expensive to calculate for large models. The statement describes the method used in the Gauss-Newton method (or related quasi-Newton methods like BFGS), where the Hessian is approximated by the sum of outer products of the gradient vector of the error (residual) for each individual data point . This approximation is derived by ignoring the second-order derivative term of the cost function, which is valid if the model is close to the true minimum, thereby providing an efficient, positive semi-definite estimate of the Hessian for use in optimization algorithms.']"
52,25,52_stop codon_protein function_amino acid sequence_mutations change,"['stop codon', 'protein function', 'amino acid sequence', 'mutations change', 'missense', 'loss stop mutations', 'stop mutations', 'change amino', 'alter protein', 'function mutations']","['Duplication mutations do not change the number of gene copies in a chromosome segment. A duplication mutation is defined as a type of chromosomal rearrangement where a segment of the chromosome is repeated, leading to extra copies of one or more genes in that segment. This means the number of gene copies does change, as the affected chromosome now has more genetic material than its homolog. For example, a chromosome segment  text  times  text  could become  text  times  text  (tandem duplication), clearly increasing the copy number of the  text  region. Gene duplication is a major source of copy number variation (CNV) and is often a raw material for evolution precisely because it increases the genetic material. ', 'If the insertion or deletion of bases is not a multiple of three, it causes a frame-shift mutation altering the reading frame. It describes a type of gene mutation caused by the insertion or deletion of base pairs in the DNA sequence where the number of bases added or removed is not a multiple of three. Since the genetic code is read in triplets (codons), adding or removing one or two bases shifts the entire sequence of codons downstream from the mutation, fundamentally altering the reading frame. This typically results in a completely new sequence of amino acids being coded for, often introducing a premature stop codon, leading to a non-functional or truncated protein, hence the name frame-shift mutation.', ""Mutations that convert a stop codon to a sense codon causing addition of amino acids at the protein's end are called loss-of-stop mutations. A loss-of-stop mutation is a type of point mutation where a normal stop codon (UAA, UAG, or UGA) is converted into a sense codon (one that codes for an amino acid) . Since the ribosome no longer recognizes the signal to terminate translation, it continues translating the  text  until it reaches the next downstream stop codon or the end of the  text . This results in the addition of extra amino acids at the C-terminus of the protein, potentially altering its function, folding, or stability, and is the precise definition of this mutation type.""]"
