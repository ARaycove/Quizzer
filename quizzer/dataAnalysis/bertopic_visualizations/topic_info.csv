Topic,Count,Name,Representation,Representative_Docs
-1,508,-1_time_learning_text_phi phi,"['time', 'learning', 'text', 'phi phi', 'linear', 'regression', 'curve', 'fibonacci', 'running time', 'output']","['Find the circumference of a circle inscribed in a square that has a diagonal of 32 square root of  . When you draw out the circle that is inscribed in a square, you should notice two things. The first thing you should notice is that the diagonal of the square is also the hypotenuse of a right isosceles triangle that has the side lengths of the square as its legs. The second thing you should notice is that the diameter of the circle has the same length as the length of one side of the square.\r\n\r\nFirst, use the Pythagorean theorem to find the length of a side of the square.\r\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\r\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\r\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \r\n\n text   equals   square root of    equals    text  square root of   divided by 2 \r\n\nSubstitute in the length of the diagonal to find the length of the square.\r\n\n text   equals   32 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \r\n\nSimplify.\r\n\n text   equals  32\r\n\nNow, recall the relationship between the diameter of the circle and the side of the square.\r\n text   equals   text   equals  32\r\n\nNow, recall how to find the circumference of a circle.\r\n\n text   equals   pi  times  text \r\n\nSubstitute in the diameter you just found to find the circumference.\r\n\n text   equals  32 pi  32 pi  8 pi  12 pi  4 pi  a circle with an orange line in the middle', ""Find the circumference of the circle if the lengths of the legs of the inscribed isosceles triangle are 5. Notice that the hypotenuse of the triangle in the figure is also the diameter of the circle.\n\nUse the Pythagorean theorem to find the length of the hypotenuse.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n text  to the power of 2  equals  2 open parenthesis  text  to the power of 2 close parenthesis \n\n text   equals   square root of    equals   text  square root of  \n\nSubstitute in the length of triangle's legs to find the missing length of the hypotenuse.\n\n text   equals  5 square root of  \n\nNow, recall that the hypotenuse of the triangle and the diameter of the circle are the same:\n\n text   equals   text   equals  5 square root of  \n\nNow, recall how to find the circumference of a circle:\n\n text   equals   text   times  pi \n\nSubstitute in the value for the diameter to find the circumference of the circle.\n\n text   equals  5 square root of   pi  5 square root of   pi  2 square root of   pi   square root of   pi  5 pi  Question image: an image of a circle with the center of the circle, and the center of the circle with the center of the circle an image of a circle with the center of the circle, and the center of the circle with the center of the circle"", ""Find the circumference of the circle if the lengths of the legs of the inscribed isosceles triangle are 2. Notice that the hypotenuse of the triangle in the figure is also the diameter of the circle.\n\nUse the Pythagorean theorem to find the length of the hypotenuse.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n text  to the power of 2  equals  2 open parenthesis  text  to the power of 2 close parenthesis \n\n text   equals   square root of    equals   text  square root of  \n\nSubstitute in the length of triangle's legs to find the missing length of the hypotenuse.\n\n text   equals  2 square root of  \n\nNow, recall that the hypotenuse of the triangle and the diameter of the circle are the same:\n\n text   equals   text   equals  2 square root of  \nNow, recall how to find the circumference of a circle:\n\n text   equals   text   times  pi \n\nSubstitute in the value for the diameter to find the circumference of the circle.\n\n text   equals  2 square root of   pi  2 square root of   pi   square root of   pi  2 square root of   pi  3 square root of   pi  Question image: a black and white image of a circle with a line in the middle an image of a circle with the center of the circle and the center of the circle at the center of the circle""]"
0,1464,0_dna_energy_cell_rna,"['dna', 'energy', 'cell', 'rna', 'proteins', 'chromosomes', 'replication', 'genetic', 'carbon', 'cycle']","['How are purines and pyrimidines distributed in the DNA structure and what role do they play? Firstly, Purines are double-ring, pyrimidines single-ring structures, which is their defining chemical classification (Adenine and Guanine are purines; Thymine and Cytosine are pyrimidines). Secondly, this structure enforces the rule that Purines (A, G) pair with pyrimidines (T, C)—specifically A pairs with T via two hydrogen bonds, and G pairs with C via three—a rule critical for maintaining the uniform width of the DNA helix. Finally, this highly specific pairing ensures that Base pairing specificity is critical for DNA stability, as the precise fit and correct number of hydrogen bonds are essential for holding the two DNA strands together. The other options are incorrect because base pairing is absolutely dependent on the identity of the bases, purines and pyrimidines have different ring structures, and pairing purine-purine or pyrimidine-pyrimidine would distort the helix. Purines are double-ring, pyrimidines single-ring structures. Base pairing specificity is critical for DNA stability. Purines (A, G) pair with pyrimidines (T, C). Base pairing does not depend on purine or pyrimidine identity. Purines and pyrimidines have identical ring structures in DNA. Purines pair with purines, pyrimidines with pyrimidines equally.', 'What is the consequence of complementary base pairing in DNA replication? During replication, each old DNA strand acts as a template, and the rule that Adenine (A) pairs only with Thymine (T) and Guanine (G) pairs only with Cytosine (C) dictates the sequence of new nucleotides. This strict pairing means the new strand is an exact complement of the template, resulting in two identical DNA double helices, which is crucial for passing the genetic code accurately to daughter cells during cell division. The other options are incorrect: complementary base pairing forms hydrogen bonds; it is essential for DNA being copied; the strands are not permanently separated (the process is semi-conservative); and while imperfections occur, its primary consequence is accuracy, not frequent mutation. It ensures precise duplication of genetic information for division. It hinders the formation of hydrogen bonds between strands. It prevents DNA from being copied during mitosis. It forces DNA strands to separate permanently after replication. It causes frequent mutations leading to genetic diversity.', ""Which is a reason that explains why DNA replication occurs in a continuous fashion on one parental DNA strand and in a discontinuous fashion on the other parental DNA strand? This enzyme can only synthesize DNA in the 5' to 3' direction, requiring a free hydroxyl group to attach new nucleotides. Since the two parental strands are antiparallel, one strand (the leading strand) can be synthesized continuously toward the replication fork, while the other (the lagging strand) must be built in short, discontinuous Okazaki fragments to maintain that specific chemical orientation. While it is true that initiation requires a primer, base pairing is essential, and enzymes ensure speed, these facts apply equally to both strands and do not explain the directional discrepancy. Therefore, the physical constraint of the polymerase's active site is the sole reason for the asymmetric nature of the replication process. DNA polymerase only works to add nucleotides in one direction of the growing chain. Initiation of a new DNA strand requires the use of a primer. DNA replication depends on base pairing between a template and nucleotides being added to the new strand. A polymerase enzyme is needed to ensure that the replication process is both accurate and rapid""]"
1,981,1_cell_dna_cycle_division,"['cell', 'dna', 'cycle', 'division', 'replication', 'mitosis', 'meiosis', 'cell division', 'chromatids', 'dna replication']","['How does cytokinesis differ in animal versus plant cells? The first correct point is that Plant cells have rigid cell walls affecting division, which prevents them from pinching in like an animal cell. This leads to the second correct point: Animal cells form a contractile ring to pinch cytoplasm, using a ring of actin and myosin filaments to form a cleavage furrow and divide the cell. Because plant cells cannot form a furrow, they rely on the third correct point: Plant cells form a cell plate from vesicles, where Golgi-derived vesicles merge in the center of the cell to build a new cell wall and membrane to separate the two daughter cells. The other options are incorrect because animal cells do not build a cell plate, cytokinesis occurs in both types of cells, and plant cells do not use a contractile ring. Plant cells have rigid cell walls affecting division Animal cells form a contractile ring to pinch cytoplasm Plant cells form a cell plate from vesicles Animal cells build a cell plate Cytokinesis only occurs in animal cells Plant cells use a contractile ring', 'Which statements accurately describe the reproduction process in living organisms? These options are correct because they accurately describe sexual reproduction, where multicellular organisms utilize meiosis to produce haploid gametes, and asexual reproduction, where single-celled organisms undergo binary fission or mitosis to create genetically identical clones . Other options are incorrect because reproduction does not always guarantee genetic uniqueness; for example, asexual offspring are clones, and many organisms like plants and fungi reproduce sexually just as animals do. Furthermore, the claim that reproduction does not involve DNA duplication is biologically false, as all forms of cellular and organismal reproduction require the faithful replication of the genome to ensure that the resulting offspring receive the necessary genetic instructions for survival and function. Multicellular organisms produce gametes for fertilization Single cells divide to produce identical offspring Reproduction guarantees offspring will be genetically unique Only animals reproduce sexually, plants do not Reproduction does not involve DNA duplication', ""What is the primary reason meiosis produces genetically unique gametes compared to mitosis? This is the primary reason meiosis produces genetically unique gametes, whereas mitosis produces genetically identical cells. Crossing over occurs in Prophase I when homologous chromosomes exchange genetic material, creating recombinant chromosomes. Independent assortment occurs in Metaphase I as homologous pairs line up randomly, leading to various combinations of maternal and paternal chromosomes in the resulting gametes. These two mechanisms introduce extensive genetic variability. The other options are incorrect because mitosis involves the identical replication of chromosomes, and while sister chromatids remain together during Anaphase I of meiosis, that doesn't explain the uniqueness of the final gametes; lastly, chromosome pairing does occur in meiosis (Prophase I), a step that is absent in mitosis. Crossing over and independent assortment create genetic variety Identical replication of chromosomes Sister chromatids remain together No chromosome pairing occurs""]"
2,967,2_data_regression_trees_algorithm,"['data', 'regression', 'trees', 'algorithm', 'variable', 'test', 'classification', 'overfitting', 'decision', 'bias']","['Which of the following statements is false about Ensemble learning? Ensemble learning is not an unsupervised learning algorithm. It is a supervised learning algorithm that combines several machine learning techniques into one predictive model to decrease variance and bias. It can be trained and then used to make predictions. And this ensemble can be shown to have more flexibility in the functions they can represent. It is a supervised learning algorithm It is an unsupervised learning algorithm More random algorithms can be used to produce a stronger ensemble Ensembles can be shown to have more flexibility in the functions they can represent', 'Which of the following statements is false about Ensemble learning? Ensemble learning is not an unsupervised learning algorithm. It is a supervised learning algorithm that combines several machine learning techniques into one predictive model to decrease variance and bias. It can be trained and then used to make predictions. And this ensemble can be shown to have more flexibility in the functions they can represent. It is a supervised learning algorithm More random algorithms can be used to produce a stronger ensemble It is an unsupervised learning algorithm Ensembles can be shown to have more flexibility in the functions they can represent', 'Which of the following statements is false about Ensemble learning? Ensemble learning is not an unsupervised learning algorithm. It is a supervised learning algorithm that combines several machine learning techniques into one predictive model to decrease variance and bias. It can be trained and then used to make predictions. And this ensemble can be shown to have more flexibility in the functions they can represent. It is a supervised learning algorithm More random algorithms can be used to produce a stronger ensemble It is an unsupervised learning algorithm Ensembles can be shown to have more flexibility in the functions they can represent']"
3,577,3_square root_parenthesis close parenthesis_open parenthesis close parenthesis_open parenthesis close,"['square root', 'parenthesis close parenthesis', 'open parenthesis close parenthesis', 'open parenthesis close', 'power minus', 'close parenthesis equals', 'parenthesis equals', 'parenthesis power', 'sud sud sud', 'parenthesis minus']","['Factor the expression below.\r\n\r\nx to the power of 3−3x to the power of 2−18x x to the power of 3  minus  3x to the power of 2  minus  18x\r\nFirst, factor out an x, since it is present in all terms.\r\nx open parenthesis x to the power of 2  minus  3x  minus  18 close parenthesis \r\nWe need two factors that multiply to  minus 18 and add to  minus 3.\r\n minus 6  times  3  equals   minus 18 and  minus 6  plus  3  equals   minus 3\r\nOur factors are  minus 6 and  plus 3.\r\nx open parenthesis x  minus  6 close parenthesis  open parenthesis x  plus  3 close parenthesis \r\nWe can check our answer using FOIL to get back to the original expression.\r\nFirst:  open parenthesis x close parenthesis  open parenthesis x close parenthesis   equals  x to the power of 2\r\nOutside:  open parenthesis x close parenthesis  open parenthesis 3 close parenthesis   equals  3x\r\nInside:  open parenthesis x close parenthesis  open parenthesis  minus 6 close parenthesis   equals   minus 6x\r\nLast:  open parenthesis  minus 6 close parenthesis  open parenthesis 3 close parenthesis   equals   minus 18\r\nAdd together and combine like terms.\r\nx to the power of 2  plus  3x  minus  6x  minus  18  equals  x to the power of 2  minus  3x  minus  18\r\nDistribute the x that was factored out first.\r\nx open parenthesis x to the power of 2  minus  3x  minus  18 close parenthesis   equals  x to the power of 3  minus  3x to the power of 2  minus  18x x open parenthesis x  minus  6 close parenthesis  open parenthesis x  plus  3 close parenthesis   open parenthesis x to the power of 2  minus  6x close parenthesis  open parenthesis x  plus  3 close parenthesis   open parenthesis x to the power of 3  minus  6 close parenthesis  open parenthesis 2x  plus  3 close parenthesis  x open parenthesis x  minus  4 close parenthesis  open parenthesis x  plus  2 close parenthesis ', 'Suppose we have an equality optimization problem as follows: Minimize f open parenthesis x, y close parenthesis   equals  x  plus  2y subject to x to the power of 2  plus  y to the power of 2  minus  4  equals  0. While solving the above equation we get x  equals   plus or minus  2 divided by  square root of   , y  equals   plus or minus  4 divided by  square root of   ,  lambda  equals   plus or minus   square root of   divided by 4 . At what value of x and y does the function f open parenthesis x, y close parenthesis  has its minimum value? When x  equals   minus  2 divided by  square root of   , y  equals   minus  4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 ,\n\nf open parenthesis x, y,  lambda  close parenthesis   equals  x  plus  2y  plus   lambda  open parenthesis x to the power of 2  plus  y to the power of 2  minus  4 close parenthesis \n\n equals   minus  2 divided by  square root of     plus  2 open parenthesis  minus  4 divided by  square root of    close parenthesis   plus or minus   square root of   divided by 4  open parenthesis  4 divided by 5   plus   16 divided by 5   minus  4 close parenthesis \n\n equals   minus  2 divided by  square root of     minus   8 divided by  square root of     plus or minus   square root of   divided by 4  open parenthesis  20 divided by 5   minus  4 close parenthesis \n\n equals   minus  10 divided by  square root of     plus or minus   square root of   divided by 4  open parenthesis 4  minus  4 close parenthesis \n\n equals   minus  10 divided by  square root of     plus or minus   square root of   divided by 4   times 0\n\n equals   minus  10 divided by  square root of   \n\nSimilarly when x  equals   2 divided by  square root of   , y  equals   4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 ,\n\nf open parenthesis x, y,  lambda  close parenthesis   equals   10 divided by  square root of   \n\nWhen x  equals   minus  2 divided by  square root of   , y  equals   4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 \n\nf open parenthesis x, y,  lambda  close parenthesis   equals   6 divided by  square root of   \n\nWhen x  equals   2 divided by  square root of   , y  equals   minus  4 divided by  square root of    and  lambda  equals   plus or minus   square root of   divided by 4 \n\nf open parenthesis x, y,  lambda  close parenthesis   equals   minus  6 divided by  square root of   \n\nSo the function f open parenthesis x, y close parenthesis  has its minimum value  open parenthesis  minus  10 divided by  square root of    close parenthesis  at x  equals   minus  2 divided by  square root of    and y  equals   minus  4 divided by  square root of   .  minus  2 divided by  square root of   ,  minus  4 divided by  square root of     2 divided by  square root of   ,  minus  4 divided by  square root of     minus  2 divided by  square root of   ,  4 divided by  square root of     2 divided by  square root of   ,  4 divided by  square root of   ', ""Find the eigenvalues and set of mutually orthogonal eigenvectors for the following matrix.\nA = \\begin{bmatrix} 3 & 2 & 4 \\\\ 2 & 0 & 2 \\\\ 4 & 2 & 3 \\end{bmatrix} In this problem, we will get three eigen values and eigen vectors since it's a symmetric matrix.\nTo find the eigenvalues, we need to minus lambda along the main diagonal and then take the determinant, then solve for lambda.\n|A - I\\lambda| = \\begin{vmatrix} 3-\\lambda & 2 & 4 \\\\ 2 & 0-\\lambda & 2 \\\\ 4 & 2 & 3-\\lambda \\end{vmatrix}\n equals   open parenthesis 3 minus  lambda  close parenthesis    minus  2   plus  4 \n equals   open parenthesis  open parenthesis 3 minus  lambda  close parenthesis  open parenthesis  open parenthesis  minus  lambda  close parenthesis  open parenthesis 3 minus  lambda  close parenthesis   minus   open parenthesis 2 close parenthesis  open parenthesis 2 close parenthesis  close parenthesis  close parenthesis   minus  2 open parenthesis  open parenthesis 2 close parenthesis  open parenthesis 3 minus  lambda  close parenthesis   minus   open parenthesis 2 close parenthesis  open parenthesis 4 close parenthesis  close parenthesis   plus  4 open parenthesis  open parenthesis 2 close parenthesis  open parenthesis 2 close parenthesis   minus   open parenthesis  minus  lambda  open parenthesis 4 close parenthesis  close parenthesis  close parenthesis \n equals   open parenthesis 3 minus  lambda  close parenthesis  open parenthesis  lambda  to the power of 2  minus  3 lambda  minus  4 close parenthesis   minus  2 open parenthesis  minus 2  minus  2 lambda  close parenthesis   plus  4 open parenthesis 4  plus  4 lambda  close parenthesis \n equals   minus  lambda  to the power of 3  plus  3 lambda  to the power of 2  plus  4 lambda  plus  3 lambda  to the power of 2  minus  9 lambda  minus  12  plus  4  plus  4 lambda  plus  16  plus  16 lambda \n equals   minus  lambda  to the power of 3  plus  6 lambda  to the power of 2  plus  15 lambda  plus  8\nThis can be factored to\n minus  open parenthesis  lambda  plus  1 close parenthesis  to the power of 2 open parenthesis  lambda  minus  8 close parenthesis   equals  0\nThus our eigenvalues are at  lambda  equals   minus 1, 8\nNow we need to substitute  lambda  into or matrix in order to find the eigenvectors.\nFor  lambda  equals   minus 1.\n(A + I)v = \\begin{bmatrix} 3-(-1) & 2 & 4 & | & 0 \\\\ 2 & 0-(-1) & 2 & | & 0 \\\\ 4 & 2 & 3-(-1) & | & 0 \\end{bmatrix}\n(A + I)v = \\begin{bmatrix} 4 & 2 & 4 & | & 0 \\\\ 2 & 1 & 2 & | & 0 \\\\ 4 & 2 & 4 & | & 0 \\end{bmatrix}\nNow we need to get the matrix into reduced echelon form.\nR subscript 1  minus  2R subscript 2  approaches R subscript 2\nR subscript 1  minus  R subscript 3  approaches R subscript 3\n\nThis can be reduced to\n\nThis is in equation form is 2x  plus  y  plus  2z  equals  0, which can be rewritten as y  equals   minus 2x  minus  2z. In vector form it looks like,  less than x,  minus 2x  minus  2z, z greater than .\nWe need to take the dot product and set it equal to zero, and pick a value for x, and z.\nLet x  equals  1, and z  equals  0.\n less than 1,  minus 2, 0 greater than   times  less than x,  minus 2x  minus  2z, z greater than   equals  0\nx  plus  4x  plus  4z  plus  0  equals  0\n5x  plus  4z  equals  0\nNow we pick another value for x, and z so that the result is zero. The easiest ones to pick are x  equals  4, and z  equals   minus 5.\nSo the orthogonal vectors for  lambda  equals   minus 1 are  less than 1,  minus 2, 0 greater than , and  less than 4, 2,  minus 5 greater than .\nNow we need to get the last eigenvector for  lambda  equals  8.\n(A + I)v = \\begin{bmatrix} 3-(8) & 2 & 4 & | & 0 \\\\ 2 & 0-(8) & 2 & | & 0 \\\\ 4 & 2 & 3-(8) & | & 0 \\end{bmatrix}\n(A + I)v = \\begin{bmatrix} -5 & 2 & 4 & | & 0 \\\\ 2 & -8 & 2 & | & 0 \\\\ 4 & 2 & -5 & | & 0 \\end{bmatrix}\nAfter row reducing, the matrix looks like\n\nSo our equations are then\nx  minus  z  equals  0, and 2y  minus  z  equals  0, which can be rewritten as x  equals  z, z  equals  2y.\nThen eigenvectors take this form,  less than 2y, y, 2y greater than . This will be orthogonal to our other vectors, no matter what value of y, we pick. For convenience, let's pick y  equals  1, then our eigenvector is  less than 2, 1, 2 greater than .  lambda  equals   minus 1,  minus 1, 8\n less than 1, 2, 0 greater than ,  less than 4, 2,  minus 5 greater than ,  less than 2, 1, 2 greater than   lambda  equals  8\n less than 1, 2, 0 greater than ,  less than 2, 1, 2 greater than  No eigenvalues or eigenvectors exist  lambda  equals  1, 8\n less than 4, 2,  minus 5 greater than ,  less than 2, 1, 2 greater than   lambda  equals   minus 1, 8\n less than 1, 2, 0 greater than ,  less than 4, 2,  minus 5 greater than ""]"
4,410,4_times equals_times equals times equals_18 times_equals times equals,"['times equals', 'times equals times equals', '18 times', 'equals times equals', 'times equals times', 'times 13 equals', 'times 12 equals', 'times 13', '13 equals', 'equals times equals times']","['2 times 4  equals  8 2 times 4  equals 8', '1 times 8 equals  8 1 times 8 equals  8', '7 times 1  equals  7 7 times 1  equals  7']"
5,262,5_ridge_regression_ridge regression_lasso,"['ridge', 'regression', 'ridge regression', 'lasso', 'selection', 'squares', 'cross validation', 'high dimensional', 'joint', 'variables']","['In statistical learning, smoothing splines are function estimates,  hat  open parenthesis x close parenthesis , obtained from a set of noisy observations y subscript i of the target f(x_i), in order to balance a measure of goodness of fit of  hat  open parenthesis x subscript i close parenthesis , with a derivative based measure of the smoothness of  hat  open parenthesis x close parenthesis  Smoothing splines are a method in statistical learning used to estimate an unknown function based on noisy data. The goal is to find a function, often represented as  hat  open parenthesis x subscript i close parenthesis , that provides a good balance between two competing objectives: fitting the data closely and maintaining a certain level of smoothness. This balance is achieved by minimizing a cost function that includes a term for goodness of fit, typically the sum of squared errors, and a penalty term that measures the ""roughness"" or smoothness of the function, which is usually based on the integral of its squared second derivative. The smoothing parameter, λ, controls the trade-off between these two terms.', 'In high-dimensional settings where p > n, least squares regression provides a useful predictive model with low test error. In high-dimensional settings where the number of predictors p is greater than the number of observations n (p  greater than  n), the standard least squares regression (also known as Ordinary Least Squares or OLS) is ill-posed. Specifically, there are infinitely many solutions for the regression coefficients  beta  that yield a residual sum of squares (RSS) of zero, meaning the model perfectly fits the training data but suffers from severe overfitting.  This extreme overfitting leads to an unstable model with high variance and typically results in a high test error, contrary to the claim. To address this, techniques like Ridge or Lasso regression, which introduce regularization, are necessary to yield a useful predictive model with lower test error in the p  greater than  n regime.', ""In the context of Partial Least Squares (PLS), what determines the number M of PLS components used in prediction? The number of Partial Least Squares (PLS) components, M, is a hyperparameter that controls the complexity of the model. Choosing the optimal M is critical to achieving a good balance between model bias and variance, avoiding both underfitting and overfitting. This optimal value is not arbitrary and is typically determined using an external validation technique like cross-validation (e.g., K-fold cross-validation or Leave-One-Out Cross-Validation). Cross-validation estimates the prediction error for different values of M, and the value that minimizes this error is selected. The number M is not always fixed to the number of observations or predictors, and while PLS is related to Principal Component Regression (PCR), M is not determined by Principal Component Analysis (PCA) alone, but by optimizing the model's predictive performance. M is chosen by cross-validation as a tuning parameter. The number M is chosen arbitrarily by the researcher. The number M is always equal to the number of observations. The number M is fixed based on the number of predictors. The number M is determined by the principal components analysis.""]"
6,239,6_century_russia_russian_government,"['century', 'russia', 'russian', 'government', 'britain', 'tokugawa', 'british', 'world war', 'political', 'fur trade']","['As the American War for Independence began, Britain had the advantage of: Britain had the advantage of overwhelming national wealth and naval power during the War for Independence. This meant that they had the financial resources to fund their military operations and maintain a strong navy, which was crucial for controlling trade routes and supplying their forces. Their naval power also allowed them to project their military strength across the Atlantic and provide support to their forces in North America. Additionally, their wealth gave them the ability to hire and maintain a well-trained and well-equipped army, which included first-rate generals. This combination of wealth and naval power gave Britain a significant advantage in the war. Overwhelming national wealth and naval power A well-organized and united home government and population Political and diplomatic unity throughout Europe An alliance with Spain and Holland First-rate generals and a well-supplied professional army in North America', '""We [are] determined to save succeeding generations from the scourge of war, which twice in our lifetime has brought untold sorrow to man kind . . ."" this statement comes from which of the following texts? The United Nations was formed at the end of the Second World War and was designed to accomplish what the earlier League of Nations had failed to accomplish: to prevent another world war. the ""Declaration of the Rights of Man and Citizen"" was the founding document of the French Revolution, and Burke\'s work was a critique of that revolution. Bossuet\'s book promoted the theory of ""divine right"". the U.S. Constitution is not explicitly interested in preventing future wars. Charter of the United Nations National Assembly\'s ""Declaration of the Rights of Man and Citizen"" U.S. Constitution Edmund Burke\'s Reflections on the Revolution in France Bishop bossuet\'s Politics Drawn from the Very Words of Holy Scripture', 'During the American Revolution, unanimous approval of the Articles of Confederation was achieved when: The Articles of Confederation, America’s first governing document post-independence, required unanimous consent for amendments and significant decisions. Initially, states held substantial power, especially regarding western lands.\nSignificance of Land Surrender:\n\n1. Centralization of Authority: By surrendering western lands, states acknowledged the need for a centralized government to manage these territories.\n2. Facilitating Expansion: This act helped the U.S. govern newly acquired lands effectively and allowed for future state admissions.\n3. Promoting Unity: Surrendering land interests fostered cooperation among states, enhancing national cohesion and addressing regional rivalries. All states claiming western lands surrendered them to the national government A compromise on slavery was reached States gave up their right to establish tariffs The states gave up their power to print money Three co-equal branches of government were established']"
7,221,7_atomic number highest_sort following elements atomic_following elements atomic number_following elements atomic,"['atomic number highest', 'sort following elements atomic', 'following elements atomic number', 'following elements atomic', 'atomic number lowest highest', 'correct order atomic number', 'order atomic number', 'order atomic', 'elements atomic', 'elements atomic number']","['Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nDs - Darmstadtium (Atomic #: 110)\nCf - Californium (Atomic #: 98)\nBk - Berkelium (Atomic #: 97)\nBi - Bismuth (Atomic #: 83)\nTl - Thallium (Atomic #: 81) Ds - Darmstadtium Cf - Californium Bk - Berkelium Bi - Bismuth Tl - Thallium', 'Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nDs - Darmstadtium (Atomic #: 110)\nRf - Rutherfordium (Atomic #: 104)\nLr - Lawrencium (Atomic #: 103)\nGd - Gadolinium (Atomic #: 64)\nPm - Promethium (Atomic #: 61) Ds - Darmstadtium Rf - Rutherfordium Lr - Lawrencium Gd - Gadolinium Pm - Promethium', 'Sort the following elements by atomic number (highest to lowest): The correct order by atomic number (highest to lowest) is:\nHs - Hassium (Atomic #: 108)\nBi - Bismuth (Atomic #: 83)\nPt - Platinum (Atomic #: 78)\nGd - Gadolinium (Atomic #: 64)\nPm - Promethium (Atomic #: 61) Hs - Hassium Bi - Bismuth Pt - Platinum Gd - Gadolinium Pm - Promethium']"
8,215,8_memory_recollection_strength_familiarity,"['memory', 'recollection', 'strength', 'familiarity', 'item', 'forgetting', 'remember', 'associations', 'recollection model', 'familiarity recollection']","['According to the Yonelinas model, how do recollection and familiarity influence recognition responses? The Dual-Process Model of Recognition Memory, proposed by Yonelinas, posits that recognition is based on two independent processes: recollection (slow, effortful retrieval of contextual details) and familiarity (fast, effortless feeling of knowing). A recognition response (""yes, I saw that before"") is triggered if either the recollection process is successful or the familiarity signal exceeds a threshold. Crucially, if recollection occurs, it provides a high-confidence, definitive response that overrides or bypasses the need for the slower familiarity judgment, leading immediately to a ""yes"" response. This is often referred to as a high-threshold process for recollection, which takes precedence over the continuous familiarity signal. Recollection responses override familiarity responses to produce a \'yes.\' Recollection and familiarity signals are averaged before deciding. Recollection is used only if familiarity is below a certain threshold. Only familiarity determines responses; recollection is ignored. Familiarity always overrides recollection in recognition judgments.', 'How do confidence judgments differ from R-K judgments regarding sources of evidence? The Dual-Process Theory of Recognition Memory, posits that recognition is based on two processes: Familiarity (a fast, context-free feeling of knowing) and Recollection (a slower retrieval of specific contextual details). R-K (Remember-Know) judgments are specifically designed to separate these two, with ""Remember"" mapping onto Recollection and ""Know"" mapping onto Familiarity. Confidence judgments, in contrast, are typically holistic ratings of certainty that are influenced by both familiarity and recollection, often weighting familiarity heavily, especially for rapid or low-signal responses. The other options are incorrect: R-K judgments are not random guesses, they are highly related to memory; both judgments do not rely exclusively on familiarity; they are not identical; and confidence judgments reflect more than just recollection signal strength. Confidence judgments may weight familiarity more Confidence judgments only reflect recollection signal strength. R-K judgments are random guesses unrelated to memory. Both judgments rely exclusively on familiarity evidence. Confidence and R-K judgments are identical in evidence weighting.', 'What does the Yonelinas familiarity-recollection model assume about the relationship between recollection and familiarity during recognition? The Yonelinas Familiarity-Recollection Model (often called the Dual-Process Model of Recognition Memory) proposes that recognition is based on two distinct processes: a fast, automatic process called familiarity (a feeling of knowing), and a slower, effortful process called recollection (retrieval of specific details). The model assumes a threshold, all-or-none role for recollection; if recollection succeeds, it is sufficient for the ""yes"" response and overrides any familiarity signal. If recollection fails, the decision falls back to a familiarity-based signal-detection process. Thus, the three selected options correctly capture the core tenets of the model: recollection is all-or-none and overrides familiarity, when recollection fails, the decision uses familiarity, and consequently, if recollection occurs, the participant answers ""yes"". The other options, like familiarity always overriding recollection or the processes combining into a single scalar value, are contrary to the distinct, hierarchical nature of this model. If recollection occurs, participant answers ""yes"" When recollection fails, decision uses familiarity Recollection is all-or-none and overrides familiarity Familiarity always overrides recollection in responses Participants respond ""no"" when recollection and familiarity conflict Recollection and familiarity are combined into a single scalar value']"
9,214,9_deep_local_neural_deep learning,"['deep', 'local', 'neural', 'deep learning', 'neural networks', 'local learning', 'backpropagation', 'hidden units', 'local deep learning', 'local deep']","['The purely local learning rule \\Delta w = \\eta(T - O)I cannot find optimal solutions in deep networks without target information reaching deep synapses. The statement is True and describes the limitations of purely local learning rules like the Perceptron Learning Rule or the Hebb rule (where \\Delta w = \\eta(T-O)I is a form of the Perceptron rule). In deep neural networks, the required weight update ( Delta w) for a deep synapse depends on the error or target information (T) computed at the final output layer. Local rules cannot efficiently propagate this target information backward through multiple layers to adjust the deep synapses (weights in hidden layers). This fundamental difficulty of attributing output error to deep, hidden units is why the backpropagation algorithm, which uses the chain rule to recursively compute and propagate the error signal, became essential for training deep networks', ""What does the fundamental limitation of deep local learning imply about its practical usage? Deep local learning (like purely Hebbian learning) relies only on information available locally at a synapse, typically the activity of the pre- and post-synaptic neurons, to update weights. The fundamental limitation is that it cannot effectively utilize the global error signal (the network's final output error) to adjust weights throughout the network, particularly in deeper layers. In contrast, backpropagation propagates this global error backward. Therefore, deep local learning methods may not find the globally or even locally optimal set of weights to minimize the overall error, making them less effective than backpropagation for training complex, deep neural networks in real-world applications where optimal error minimization is essential. It may not find weights to minimize error optimally in deep networks. It does not depend on local information at synapses. It is the perfect method for all deep networks. It can always replace backpropagation without issues."", 'What main advantage does a multilayer network have compared to a single hidden layer network? The main advantage of a multilayer (deep) neural network lies in its hierarchical structure, allowing it to compose complex functions from simpler, intermediate representations . While the Universal Approximation Theorem states that a single hidden layer can theoretically model any function, it often requires an exponentially large number of neurons for complex problems, making training computationally intractable and leading to poor generalization. Multilayer networks, by contrast, can achieve the same complexity with fewer overall parameters spread across several modest-sized layers, which simplifies the learning process, improves feature extraction, and makes the model easier to train efficiently. Multiple layers allow building complex functions from modest-sized layers, simplifying learning compared to one very large layer. Multilayers improve interpretability by reducing the number of parameters required. A single hidden layer can model any function exactly but requires exponential computation time. Single-layer networks can only perform linear regression, unable to fit nonlinear data. Multilayers always guarantee perfect fitting whereas single layers do not approximate functions well.']"
10,195,10_operator_println_method_python,"['operator', 'println', 'method', 'python', 'my_list', 'static void main', 'public static void', 'public static void main', 'main string', 'java code']","['What is the output of the following Java program? class variable_scope \n        {\n            public static void main(String args[]) \n            {\n                int x;\n                x = 5;\n                {\n    \t               int y = 6;\n    \t               System.out.print(x + "" "" + y);\n                }\n                System.out.println(x + "" "" + y);\n            } \n        }  Compilation Error exception in thread ""main"" java.lang.error: unresolved compilation problem: y cannot be resolved to a variable The second print statement doesn\'t have access to y, scope y was limited to the block defined after initialization of x. Output:\nException in thread ""main"" java.lang.Error: Unresolved compilation problem: y cannot be resolved to a variable', 'What will be the output of the following Java code? class output\n        {\n            public static void main(String args[])\n            { \n               String c = ""Hello i love java"";\n               boolean var;\n               var = c.startsWith(""hello"");\n               System.out.println(var);\n            }\n        } The startsWith() method is case-sensitive; for example, ""hello"" and ""Hello"" are treated as different strings. Therefore, the result of the method is false, which is stored in the variable since the return type of startsWith() is a boolean.\n\nNote: Although var is a keyword used for type inference in Java, It can still be used as an Identifier in earlier versions where var is not a reserved keyword 0 true false 1', 'What will be the output of the following Java code? class newthread extends Thread\n       {\n    \tThread t1, t2;\n    \tnewthread()\n           {\n    \t    t1 = new Thread(this,""Thread_1"");\n    \t    t2 = new Thread(this,""Thread_2"");\n    \t    t1.start();\n    \t    t2.start();\n    \t}\n    \tpublic void run()\n           {\n    \t           t2.setPriority(Thread.MAX_PRIORITY);\t\n    \t           System.out.print(t1.equals(t2));\n           }   \n       }\n       class multithreaded_programing\n       {\n           public static void main(String args[])\n           {\n               new newthread();       \n           }\n       } Threads t1 and t2 are created by the NewThread class, which implements the Runnable interface. therefore, both threads have their own run() methods defining the actions to be performed. When the constructor of the NewThread class is invoked, the run() method of t1 executes first, followed by the run() method of t2. Each execution prints false because the two threads are not equal - one has a different priority than the other. As a result, the output is falsefalse truetrue falsefalse true false']"
11,181,11_nerve_lesion_year old_medulla,"['nerve', 'lesion', 'year old', 'medulla', 'cord', 'aphasia', 'artery', 'trigeminal', 'cerebellum', 'cranial nerve']","['A 30-year-old man presents with loss of pain and temperature in the right face and left body. The lesion is most likely in the: The most likely location of the lesion is the right lateral medulla. This is a classic presentation of Wallenberg syndrome, or lateral medullary syndrome. The loss of pain and temperature on the right side of the face and left side of the body is caused by damage to the spinothalamic tract and the spinal trigeminal tract in the lateral medulla. These two tracts are responsible for pain and temperature sensation. This specific pattern of crossed sensory loss is a hallmark of a lesion in the right lateral medulla. Right lateral medulla  Left lateral medulla Right medial medulla  Left medial medulla', ""A 45-year-old woman presents with right-sided hemianesthesia and left-sided tongue deviation. The lesion is most likely in the: The left medial medulla is the correct answer. This is because the patient's symptoms point to a lesion in this specific area. The right-sided hemianesthesia (loss of sensation) indicates damage to the medial lemniscus, a sensory pathway that crosses over in the brainstem. The left-sided tongue deviation is a classic sign of damage to the hypoglossal nerve (CN XII), which originates in the left medial medulla. The combination of these two findings is characteristic of medial medullary syndrome, also known as Dejerine syndrome. This syndrome is caused by an infarction (stroke) in the territory of the anterior spinal artery or a penetrating branch of the vertebral artery, which supplies the medial medulla. Left medial medulla  Right medial medulla Left lateral medulla Right lateral medulla"", ""Which structure is essential for consolidation of procedural (implicit) memory? The basal ganglia handles procedural learning and habit formation - skills acquired through practice without conscious awareness. The hippocampus manages declarative memory - facts and events we can consciously recall. These systems work independently but can interact during learning.\n Further Reading:\nThe basal ganglia primarily supports non-declarative memory processes, particularly procedural learning and habit formation. This system enables the gradual acquisition of skills and behaviors through repeated practice, often without conscious awareness of the learning process itself. The basal ganglia is heavily dependent on dopaminergic signaling and specializes in reward-based learning, helping organisms learn to predict outcomes and develop automatic behavioral patterns. When you learn to ride a bicycle, play a musical instrument, or develop diagnostic skills as a radiologist, the basal ganglia is primarily responsible for encoding these procedural memories. In contrast, the hippocampus is essential for declarative memory, enabling the formation of explicit memories for facts and events that can be consciously recalled and verbally expressed. The hippocampus creates rich, contextual representations that allow for flexible memory retrieval and the ability to generalize across different situations. When you remember what you had for breakfast, recall historical facts, or remember the details of a conversation, the hippocampus is primarily responsible for encoding and retrieving these declarative memories. These systems operate largely independently but can interact during certain learning tasks. The functional separation is evident in patients with selective damage to either system - those with hippocampal damage can still learn new procedures despite severe amnesia for facts and events, while those with basal ganglia dysfunction (such as in Parkinson's disease) show deficits in habit learning while maintaining declarative memory abilities. This dissociation demonstrates that the brain has evolved specialized neural circuits optimized for different types of learning and memory demands. Hippocampus Amygdala Basal ganglia  Prefrontal cortex""]"
12,157,12_image_question image_question_figure,"['image', 'question image', 'question', 'figure', 'text equals', 'question image diagram', 'image diagram', 'given', 'question image image', 'equals square']","['Given the below neural network with w subscript 5  equals  0.4, Output O subscript 1  equals  0.5 and Out h subscript 1  equals  0.59. How much does the total error changes with respect to w subscript 5? Given w subscript 5  equals  0.4, Output O subscript 1  equals  0.5, Out h subscript 1  equals  0.59 and target O subscript 1  equals  0.2. As we have to calculate the total error changes with respect to w subscript 5:   partial derivative E subscript  text  divided by  partial derivative w subscript 5   equals    partial derivative E subscript  text  divided by  partial derivative  text  O subscript 1   times    partial derivative  text  O subscript 1 divided by  partial derivative  text  O subscript 1   times    partial derivative  text  O subscript 1 divided by  partial derivative w subscript 5 \n  partial derivative E subscript  text  divided by  partial derivative w subscript 5   equals   minus  open parenthesis  text  O subscript 1  minus  text  O subscript 1 close parenthesis   times   open parenthesis  text  O subscript 1  open parenthesis 1  minus   text  O subscript 1 close parenthesis  close parenthesis   times   open parenthesis  text  h subscript 1 close parenthesis \n equals   minus  open parenthesis 0.2  minus  0.5 close parenthesis   times  0.5  times   open parenthesis 1  minus  0.5 close parenthesis   times  0.59\n equals   minus  open parenthesis  minus 0.3 close parenthesis   times  0.5  times  0.5  times  0.59\n equals  0.3  times  0.25  times  0.59\n equals  0.044 0.044 0.25 0.3 0.59 Question image: a diagram of a network with two nodes and one node', 'Find the radius of a circle inscribed in a square with a diagonal of 6 square root of  . Notice that the diagonal of the square is also the hypotenuse of a right isosceles triangle whose legs are also the sides of the square. You should also notice that the diameter of the circle has the same length as that of a side of the square.\n\nIn order to find the radius of the circle, we need to first use the Pythagorean theorem to find the length of the side of the square.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \n\n text   equals   square root of    equals    text  square root of   divided by 2 \n\nNow, substitute in the value of the diagonal to find the length of a side of the square.\n\n text   equals   6 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \n\nSimplify.\n\n text   equals  6\n\nNow keep in mind the following relationship between the diameter and the side of the square:\n\n text   equals   text   equals  6\n\nRecall the relationship between the diameter and the radius.\n\n text   equals   1 divided by 2  open parenthesis  text  close parenthesis \n\nSubstitute in the value of the radius by plugging in the value of the diameter.\n\n text   equals   1 divided by 2  open parenthesis 6 close parenthesis \n\nSolve.\n text   equals  3 6 square root of  . 3 3 square root of  . 6 an image of a circle with a line going through it', 'Find the length of the radius of a circle inscribed in a square that has a diagonal of 10 square root of  . Notice that the diagonal of the square is also the hypotenuse of a right isosceles triangle whose legs are also the sides of the square. You should also notice that the diameter of the circle has the same length as that of a side of the square.\n\nIn order to find the radius of the circle, we need to first use the Pythagorean theorem to find the length of the side of the square.\n\n text  to the power of 2  equals   text  to the power of 2  plus   text  to the power of 2\n\n2 open parenthesis  text  close parenthesis  to the power of 2  equals   text  to the power of 2\n\n text  to the power of 2  equals    text  to the power of 2 divided by 2 \n\n text   equals   square root of    equals    text  square root of   divided by 2 \n\nNow, substitute in the value of the diagonal to find the length of a side of the square.\n\n text   equals   10 square root of   open parenthesis  square root of   close parenthesis  divided by 2 \n\nSimplify.\n\n text   equals  10\n\nNow keep in mind the following relationship between the diameter and the side of the square:\n\n text   equals   text   equals  10\n\nRecall the relationship between the diameter and the radius.\n\n text   equals   1 divided by 2  open parenthesis  text  close parenthesis \n\nSubstitute in the value of the radius by plugging in the value of the diameter.\n\n text   equals   1 divided by 2  open parenthesis 10 close parenthesis \n\nSolve.\n\n text   equals  5 10 square root of   5 square root of   5 2 an image of a circle with a yellow line in the middle']"
13,146,13_rna_mrna_rna strand_amino acid,"['rna', 'mrna', 'rna strand', 'amino acid', 'rna polymerase', 'protein', 'stop codon', 'nucleus', 'dna template', 'translation']","['Thorndike concluded that mere temporal contiguity was not a sufficient condition for learning, emphasizing that participants had to do something to forge a connection. Edward Thorndike’s research on instrumental conditioning led him to conclude that simply having two events occur close together in time—temporal contiguity—was insufficient to produce lasting learning. He argued that for a connection to be forged, the behavior must be followed by a satisfying consequence, a principle known as the Law of Effect, rather than relying on mere association through timing. Other options would be incorrect because terms like ""reinforcement"" or ""punishment"" describe the consequences themselves rather than the temporal relationship between events. Furthermore, suggesting that ""frequency"" or ""intensity"" alone creates learning ignores Thorndike\'s specific emphasis on the active role of the participant\'s response and its outcome. Consequently, contiguity is the only term that accurately identifies the purely passive temporal link that Thorndike deemed inadequate for meaningful behavioral change.', 'The substitution of a single amino acid at position 6 in β-globin causes red blood cells to have defective shape , leading to sickling. structure Sickle Cell disease is caused by a point mutation in the gene encoding the  beta -globin chain of hemoglobin, specifically resulting in the substitution of the amino acid valine for glutamic acid at position 6. This single change makes the hemoglobin molecules prone to clumping when oxygen levels are low, which in turn deforms the red blood cells, giving them a rigid, crescent, or sickle shape instead of the normal biconcave disc shape. This defective shape is what causes the cells to become stuck in capillaries, leading to the symptoms of the disease. While the hemoglobin itself has a defective structure (due to the amino acid change), the direct, visible effect on the red blood cell that leads to ""sickling"" is the change in its overall shape.', 'In mammals, the pyruvate dehydrogenase complex is regulated by phosphorylation; the kinase responsible for phosphorylation is activated by acetyl-CoA and NADH , and inhibited by Ca2+, ADP, and pyruvate. The pyruvate dehydrogenase (PDH) complex is regulated by a kinase that inactivates it when high-energy signals are abundant. In mammals, PDH kinase is allosterically activated by high ratios of acetyl-CoA and NADH, which are the immediate products of the PDH reaction and signals of a high energy state . This feedback mechanism ensures that the cell stops producing more acetyl-CoA when it already has sufficient substrates for the Krebs cycle. Other options like NAD+ or CoA are incorrect because they are reactants that signal a low energy state and would typically inhibit the kinase to keep the complex active. Additionally, ATP is a more general energy signal, whereas NADH specifically reflects the redox balance tied directly to this enzymatic step. Consequently, NADH serves as a precise metabolic sensor to trigger the inhibitory phosphorylation of the PDH complex.']"
14,131,14_voting_hyperplane_ensemble_votes,"['voting', 'hyperplane', 'ensemble', 'votes', 'decision', 'kernel', 'classifiers', 'multiclass classification', 'majority', 'hard svm']","['Different algorithms make different assumptions about the data and lead to different classifiers in generating diverse learners. Different algorithms make different assumptions about the data and lead to different classifiers. For example one base – learner may be parametric and another may be nonparametric. When we decide on a single algorithm, we give importance to a single method and ignore all others.', 'Different algorithms make different assumptions about the data and lead to different classifiers in generating diverse learners. Different algorithms make different assumptions about the data and lead to different classifiers. For example one base – learner may be parametric and another may be nonparametric. When we decide on a single algorithm, we give importance to a single method and ignore all others.', 'Different algorithms make different assumptions about the data and lead to different classifiers in generating diverse learners. Different algorithms make different assumptions about the data and lead to different classifiers. For example one base – learner may be parametric and another may be nonparametric. When we decide on a single algorithm, we give importance to a single method and ignore all others.']"
15,101,15_darwin_dna_evolution_watson crick,"['darwin', 'dna', 'evolution', 'watson crick', 'natural selection', 'mendel', 'ray diffraction', 'genetics', 'inheritance', 'chase']","[""Why were dyes developed in the early 20th century important to DNA studies? Early \\text{DNA}-specific dyes, like the Feulgen stain, were crucial because they directly bind to \\text{DNA}, enabling scientists to visualize it under a microscope within the cell nucleus. . This staining clearly showed that chromosomes, the carriers of hereditary information, were composed largely of this material. Furthermore, the intensity of the dye uptake was observed to correlate directly with the quantity of \\text{DNA} present in a cell's nucleus, which was essential evidence supporting \\text{DNA} as the genetic material. The other options are incorrect as \\text{DNA} staining was relevant, it focused on the nucleus, and the dyes bound to \\text{DNA}, not exclusively to proteins Staining revealed chromosomes that contain most DNA. Dyes bind DNA, allowing visualization under microscope. Dye intensity correlated with DNA quantity in cells. Staining technology was irrelevant to genetic material identification. Early dyes stained cytoplasm more than nuclear material. Dyes bind exclusively to proteins, not DNA, in the nucleus."", ""What role did geology play in Darwin's evolutionary thinking? This is based on the geological theory of Uniformitarianism, championed by Charles Lyell (whose book Darwin read), which proposed that the Earth was shaped by the same slow, continuous processes operating today over vast spans of time. This idea of deep time and gradual change provided the necessary framework for Darwin's theory of gradual evolution through natural selection, suggesting that the small changes observed in individuals could accumulate over immense timescales to produce new species. The other options are incorrect because geology provided the foundation for an old Earth, supported a relationship between changes in the environment and organisms, and Darwin integrated geological principles into his theory. Geology showed Earth's slow changes, supporting Darwin's view of gradual evolution. Earth's geological changes implied no relation to living organisms. Geology disproved slow Earth changes, favoring sudden events. Geology showed Earth was too young for evolution to occur. Darwin rejected geology as irrelevant to biological processes."", ""What experiment provided clear evidence that DNA replication follows a semiconservative mechanism? The Meselson-Stahl experiment provided the definitive evidence for the semiconservative model of DNA replication. They grew E. coli in a heavy nitrogen isotope (\\text{}^{15}\\text{N}) and then transferred them to a light nitrogen isotope (\\text{}^{14}\\text{N}). Using density gradient centrifugation , they showed that after one generation, DNA had an intermediate density, and after two generations, it had two bands (intermediate and light), exactly as predicted by the semiconservative model, where each new DNA molecule consists of one old strand and one new strand. The other experiments, while significant, established different facts: Hershey-Chase identified DNA as the genetic material; Watson-Crick determined the structure; and Avery and Griffith studied bacterial transformation. The Meselson-Stahl experiment used nitrogen isotopes to prove semiconservative replication. Hershey-Chase used bacteriophage labeling to confirm DNA as genetic material. The Watson-Crick model demonstrated DNA forms a double helix structure. Avery's work identified DNA as the molecule responsible for transformation. Griffith's experiment showed that genetic material could transform bacteria types.""]"
16,100,16_game_dynasty_professional_greek,"['game', 'dynasty', 'professional', 'greek', 'players', 'century', 'greek mythology', 'edo period', 'hon inbō', 'wimmer']","[""John Locke died in High Laver , England , on October 28th, 1704 After a lengthy period of poor health, Locke died on 28 October 1704, and is buried in the churchyard of All Saints' Church in High Laver, near Harlow in Essex, where he had lived in the household of Sir Francis Masham since 1691. Locke never married nor had children. Events that happened during Locke's lifetime include the English Restoration, the Great Plague of London, the Great Fire of London, the Glorious Revolution and war against France including the Battle of Blenheim just before his death. He did not live long enough to see the Act of Union of 1707, but the thrones of England and Scotland were held in personal union throughout his lifetime. Constitutional monarchy and parliamentary democracy were in their infancy during Locke's time. Locke has an engraved floor memorial plaque at Christ Church Cathedral, Oxford."", ""How did the opening strategies in Japanese Go evolve during the Edo period? The Edo period (1603-1868) was the Golden Age of Go in Japan, characterized by intense study under the patronage of the Shogunate. This environment fostered great theoretical advancements, including the development of numerous fuseki (whole-board opening patterns) beyond simple corner-first play. Go strategy evolved from older practices, like handicap games with preset stone placements, toward a modern game played on an empty board, requiring more strategic depth. Furthermore, new josekis (corner sequences) and advanced concepts like sacrifice tactics emerged as masters innovated and refined the game's opening theory. Various opening patterns (fuseki) then developed Shifted from preset to empty board starts New josekis and sacrifice tactics emerged Opening strategies remained unchanged from Chinese traditions Players started with all stones placed for gameplay The Edo period discouraged opening strategy experimentation"", 'The Tokugawa shogunate in Japan established four official Go schools and patronized formal competitive play starting from 1700 onwards. The Tokugawa shogunate provided essential official patronage to Go, institutionalizing it through the four official schools—Hon’inbo, Inoue, Yasui, and Hayashi—each led by a hereditary master or iemoto. This system emerged after Tokugawa Ieyasu, an avid Go player, granted salaries to top experts, eventually formalizing their status as government-supported professionals. While the schools were active earlier, the system matured into its most prestigious form by the early 18th century (1700 onwards), characterized by the famous Oshirogo (Castle Go) matches played annually in the presence of the shogun at Edo Castle. Other options claiming Go was only a casual pastime or lacked official structure are incorrect because the shogunate specifically integrated these schools into the national administrative framework, governed under the Magistrate of Temples and Shrines. This high-level patronage transformed Go from a simple game into a formalized, competitive art with deep political and social significance in Edo Japan.']"
17,100,17_odd numbers_select numbers numbers divisible_odd numbers divisible_select odd numbers odd,"['odd numbers', 'select numbers numbers divisible', 'odd numbers divisible', 'select odd numbers odd', 'select odd numbers', 'numbers odd numbers divisible', 'numbers odd numbers', 'select odd', 'odd numbers odd', 'odd numbers odd numbers']","['Select all the ODD numbers. Odd numbers are not divisible by 2. 2 6 42 72 55 45', 'Select all the ODD numbers. Odd numbers are not divisible by 2. 82 39 61 7 43 5', 'Select all the ODD numbers. Odd numbers are not divisible by 2. 52 5 62 6 7 91']"
18,96,18_predictors_weights_linear_outputs,"['predictors', 'weights', 'linear', 'outputs', 'learning', 'regression', 'regularization', 'classification', 'synaptic', 'dynamic programming']","[""What kinds of network architectures are included in the feed-forward networks class? A feed-forward neural network is fundamentally characterized by the unidirectional flow of information . This means that data moves strictly forward from the input layer through hidden layers to the output layer, with no cycles or closed directed paths (i.e., the output of a layer never feeds back into a previous layer). Even architectures with skip-layer connections (e.g., in ResNets), where a layer's output bypasses immediate subsequent layers, are still considered feed-forward as long as the overall data flow remains acyclic (forward-only). Networks with cycles or recurrent connections are called Recurrent Neural Networks (RNNs), and networks where output depends on future inputs involve non-causal processing, thus excluding them from the standard feed-forward class. Networks with layers where information flows forward. Networks with skip-layer connections can be included if acyclic. Networks without cycles (no closed directed paths). Networks with cycles. Networks where output depends on future inputs. Networks with recurrent connections."", 'What is the primary cause of performance slowdown for ThreeSum with very large arrays, as observed in empirical tests? In empirical tests of the ThreeSum algorithm, performance often slows down significantly more than predicted by its theoretical time complexity once arrays exceed a certain size due to hardware limitations. Specifically, as the dataset grows beyond the capacity of the CPU cache, the system must frequently fetch data from the much slower main memory (RAM), a phenomenon known as a cache miss . This memory hierarchy bottleneck makes the first option correct, while other choices like network latency or compiler bugs are incorrect as they do not scale fundamentally with array size in a local execution environment. Furthermore, ""application-layer design"" is too vague to be the primary hardware-level cause, and faster processors would decrease execution time rather than causing a performance slowdown relative to the input size. Caching effects increase time to access memory with large arrays. Inherent limitations in application-layer design. Network latency during execution. Compiler bugs that intensify with input size. Faster processors that reduce the time too much.', 'What does the equation \\Delta w h_{ij} = F(I h_i, O h_j, O h_{-1j}, w h_{ij}) describe? The weight change  Delta wh subscript ij depends on local information the input to the current neuron i (Ih subscript i), the current output of neuron j (Oh subscript j), the previous output of neuron j (Oh subscript  minus 1j), and the current weight itself (wh subscript ij). This dependency on the local pre- and post-synaptic activations and the current weight is characteristic of local learning rules like Hebbian learning, backpropagation, or their variations. The options suggesting weight decay, independence from activations, or use of only global outputs are incorrect as the formula explicitly uses local neuron activations (Ih subscript i, Oh subscript j) and the synaptic variable (wh subscript ij). A local learning rule using target and synaptic variables for updates. A formula for weight decay during unsupervised training. A fixed update rule independent of neuron activations. A rule that updates weights only using global network outputs.']"
19,92,19_perpendicular_parallelogram_circle_angles equal,"['perpendicular', 'parallelogram', 'circle', 'angles equal', 'quadrilateral', 'triangle', 'straight line', 'right angles', 'parallel sides', 'equal diagonals']","[""Which construction is used to find the midpoint of a line segment? A perpendicular bisector is a line that intersects a line segment at its exact center, or midpoint, and forms a 90-degree angle with it. To construct this, you use a compass to draw intersecting arcs from each endpoint of the line segment. The line drawn through the two points where the arcs intersect is the perpendicular bisector, and where it crosses the original line segment is the midpoint. The other options are incorrect: a tangent touches a circle at one point, a simple circle doesn't find a midpoint, and a parallel line never intersects the original line. Drawing a perpendicular bisector  Drawing a tangent Drawing a circle Drawing a parallel line"", 'Which of the following is a property of a parallelogram? A parallelogram is a quadrilateral with two pairs of parallel sides. A fundamental property of parallelograms is that their opposite sides are equal in length. In addition to this, their opposite angles are also equal. The other options are properties of specific types of parallelograms, but not all of them. For example, a square or a rectangle has equal diagonals and all right angles, while a rhombus has all equal sides. A parallelogram in general only guarantees that its opposite sides are equal. Diagonals are equal Opposite sides are equal  All angles are right angles All sides are equal', 'What is the first step in bisecting a straight line segment? The first step in bisecting a line segment with a compass and straightedge is to set your compass to a width that is greater than half the length of the segment. This is essential because it ensures that when you draw arcs from both endpoints of the line, they will intersect at two distinct points. These intersection points are used to draw the perpendicular bisector, which cuts the original line segment into two equal halves. If the compass opening were less than half, the arcs would not cross, making it impossible to find the bisection point. Draw a perpendicular line Use a protractor to measure half the length Open the compass to more than half the length Draw a parallel line']"
20,87,20_scientific_applied science_inductive reasoning_deductive,"['scientific', 'applied science', 'inductive reasoning', 'deductive', 'research', 'scientific method', 'deductive reasoning', 'natural sciences', 'hypothesis based science', 'based science']","['Which statements reflect valid reasons for defining science beyond just the scientific method? they acknowledge that science is a diverse endeavor not always confined to the rigid, laboratory-based ""scientific method."" Many legitimate fields, such as archaeology or astronomy, rely on observational data and inference where exact experimental replication is physically impossible, yet they still provide vital understanding of the natural world. In contrast, stating that science excludes study based on hypotheses or always requires exact repetition is incorrect as it ignores these observational disciplines and the evolving nature of scientific inquiry. Furthermore, non-testable supernatural explanations are universally excluded from science because they cannot be empirically observed or falsified. Hypotheses are sometimes supported without repetition. Fields like archaeology have trouble repeating experiments. Science seeks to broadly comprehend nature’s universe. Science excludes any study based on hypotheses or inference. Science always requires exact experimental repetition. Non-testable supernatural explanations are considered scientific.', 'What are characteristics of inductive reasoning? Inductive reasoning is the hallmark of descriptive science, as it involves analyzing large volumes of qualitative or quantitative data to identify patterns that lead to broader generalizations. Unlike deductive reasoning, which moves from a broad theory to a single observable event or uses general premises to deduce specific predictions, inductive reasoning works ""bottom-up"" by using specific instances to build a general theory. Consequently, it is not primarily employed in hypothesis-driven experimental science, which typically starts with a hypothesis (a general prediction) and tests it through specific experiments. Thus, the selected options accurately define the directionality and scientific application of the inductive process. It supports descriptive or discovery science methods. It infers general conclusions using many observations. It moves from specific cases to general principles. It is mainly employed in hypothesis-driven experimental science. It moves from a broad theory to a single observable event. It uses general premises to deduce specific predictions.', 'What factors differentiate descriptive science from hypothesis-based science? Descriptive science is primarily concerned with observing and recording phenomena, often using inductive reasoning to form generalizations from specific observations. In contrast, hypothesis-based science starts with a proposed explanation (hypothesis) and uses deductive reasoning to make and test specific predictions. While distinct in their core methodology, the line between the two is often blurred, as descriptive discoveries frequently lead to testable hypotheses, and hypothesis testing relies on careful observation and data collection. Therefore, the differences lie in their primary reasoning and goal (discovery versus explanation), not in being entirely independent or mutually exclusive. Hypothesis-based science tests predictions deductively. Descriptive science uses inductive reasoning. The line between them is sometimes blurred. Hypothesis-based science never involves observation data collection. Both forms are entirely independent and never combined in research. Descriptive science always tests hypotheses with experiments.']"
21,81,21_11_10 11_11 12_11 13,"['11', '10 11', '11 12', '11 13', '15 10', '17 10', '15 16', '16 19', '12 10', '11 18']","['4 + 6 = ? 4+6 = 10 9 46 15 10 11 12', '9 + 3 = ? 9+3 =12 12 13 93 18 14 11', '3 + 8 = ? 3+8 = 11 11 38 21 13 12 10']"
22,65,22_error_irreducible error_learning_low bias,"['error', 'irreducible error', 'learning', 'low bias', 'high variance', 'variance error', 'deep neural network', 'variance statistical learning', 'overfitting', 'neural']","[""Linear and Generalized linear models can be regularized to decrease variance at the cost of increasing bias. Regularization techniques, such as Ridge or Lasso regression, add a penalty term to the model's loss function to constrain the magnitude of the coefficients. This constraint effectively reduces model complexity and variance, preventing the model from overreacting to noise in the training data, though it intentionally introduces a small amount of bias by pulling estimates away from their true values. Other potential methods, like standard OLS estimation, are not appropriate in this specific context because they seek to minimize bias without regard for variance, often leading to overfitting in high-dimensional datasets. Furthermore, simply increasing the number of predictors or removing all constraints would typically increase variance rather than decrease it."", ""The bias–variance decomposition analyzes a learning algorithm's expected generalization error as a sum of three terms: bias, variance, and irreducible error. The bias-variance decomposition states that the total expected prediction error (generalization error) of a supervised learning model can be broken down into three components : Bias (error from overly simplistic assumptions), Variance (error from sensitivity to training data fluctuations), and the Irreducible Error (or noise). This third component is an inherent error that cannot be reduced by any learning algorithm, as it arises from fundamental randomness or unmeasured variables in the data generation process. Therefore, it sets a lower bound on the generalization error, making the term irreducible the correct fit for the decomposition model."", ""R-squared alone is not sufficient to detect overfitting in a model, and other methods and validation techniques should be used. R-squared (or the coefficient of determination) measures the proportion of the variance in the dependent variable that's predictable from the independent variables. In the context of a model, R-squared will always increase or stay the same as you add more predictors, even if those predictors are not meaningful, meaning a high R-squared can be achieved by a model that is heavily overfit to the training data. Overfitting occurs when a model learns the noise and specific details of the training data too well, leading to high performance on training data but poor generalization to new, unseen data. Because R-squared only reflects the fit on the training data, it fails to capture this drop in generalization performance, making it an insufficient metric to detect overfitting. Other options are incorrect as they do not relate to the fundamental limitation of R-squared in model evaluation; for instance, R-squared is designed to measure the goodness-of-fit, not model complexity itself, and therefore cannot alone identify the problem of overfitting.""]"
23,59,23_question image_question_ant ant_ant ant ant,"['question image', 'question', 'ant ant', 'ant ant ant', 'ant ant ant ant', 'white background', 'shown middle middle', 'question image group', 'clipart', 'clipart corn clipart']","['solve the addition below one hand has 1 finger raise and other has 2 fingers, so it adds upto 3. 3 4 2 5 Question image: a hand is shown in the middle and middle of a hand is shown in the middle and middle of a hand is shown in the middle of', 'solve the addition below one hand has 2 fingers while other hand also has two fingers, so it adds upto 4 4 5 3 2 Question image: a hand is shown in the middle and middle of a hand is shown in the middle and middle of a hand is shown in the middle of', 'Solve the addition below one hand has 4 fingers and other hand has 3 fingers, so it adds up to 7 7 8 6 5 Question image: a hand is shown in the middle and middle of a hand is shown in the middle and middle of a hand is shown in the middle of']"
24,54,24_consonant vowel vowel vowel_argument_true true_argument structure,"['consonant vowel vowel vowel', 'argument', 'true true', 'argument structure', 'standard way notating argument', 'structure valid form argument', 'structure disjunctive', 'structure disjunctive syllogism', 'structure argument', 'structure valid form']","['Is A a consonant? A is not a consonant, it is a vowel.', 'Is U a consonant? U is not a consonant, it is a vowel.', 'Is E a consonant? E is not a consonant, it is a vowel.']"
25,50,25_rhymes_sound similar_rhyming_rhyming words,"['rhymes', 'sound similar', 'rhyming', 'rhyming words', 'similar sounds rhyming words', 'similar sounds rhyming', 'similar sounds rhyme', 'sounds rhyming words', 'sounds rhyming', 'sounds rhyme']","['What rhymes with “beep”?\n Sheep and beep both have similar sounds so they are rhyming words. Sheep Dog Wolf Cat', 'What rhymes with “fan”?\n Fan and Pan have similar sounds so they are rhyming words. Pan Light Cat Fat', 'Following the First World War, dress among Westerners became increasingly The late-twentieth-century phenomenon of people traveling, attending church, or even going shopping in casual clothing was unique. Into the 1950s it was unusual for a man to be seen in public in anything other than a coat and tie. The teen cultures of the 1920s and 1950s, which emphasized distinct dress for youth, promoted an increasingly casual approach to dress. casual formal monotone homemade dirty']"
26,49,26_naive bayes_naive_high dimensional_larger margin,"['naive bayes', 'naive', 'high dimensional', 'larger margin', 'images', 'training', 'skin lesion', 'performance', 'pcr ridge', 'regularization']","[""Why might naive Bayes outperform LDA or QDA when the number of predictors is large and data is limited? The Naive Bayes classifier often outperforms more complex models like  text  or  text  when the number of predictors (p) is large and the amount of data (N) is limited because it makes a strong, simplifying assumption: that all predictors are conditionally independent given the class. This assumption drastically reduces the number of parameters that need to be estimated for the class-conditional density functions, thereby reducing the model's variance. This regularization effect is particularly beneficial in high-dimensional, low-sample-size scenarios, preventing the model from overfitting where complex models like  text  would likely fail due to insufficient data to estimate their covariance matrices accurately. Because naive Bayes reduces variance by assuming independence, simplifying density estimation. Because naive Bayes ignores predictor values. Because naive Bayes always uses more data than other methods. Because naive Bayes uses more complex models."", 'Why is cross-validation essential when choosing the tuning parameter in models like the lasso for high-dimensional data? Cross-validation is necessary because the tuning parameter directly controls the bias-variance tradeoff; a value that is too small leads to overfitting (high variance), while one that is too large leads to underfitting (high bias). By evaluating model performance on held-out data, cross-validation identifies the optimal regularization amount that minimizes the expected test error rather than just the training error. Other options are incorrect because the goal of lasso is often to reduce, not maximize, the number of predictors, and cross-validation does not eliminate the need for standardization, which is a separate preprocessing step. Furthermore, it does not minimize training error—which is lowest when there is no regularization—nor does it inherently guarantee the selection of the fewest possible variables, as the ""optimal"" model might require several predictors to remain accurate. It helps select the regularization amount balancing bias and variance. Because it maximizes number of predictors included in the final model. Because it eliminates the need to standardize predictors. Because it always minimizes training error perfectly. Because it guarantees selection of the fewest variables possible.', 'Why is it often unnecessary to compute the constant factor c in the running time expression T(n) ~ c * f(n)? The primary goal of analyzing algorithm efficiency is to determine the asymptotic behavior—how the running time T(n) scales as the input size n approaches infinity, which is captured by the function f open parenthesis n close parenthesis  (the time complexity, e.g., O(n \\log n)). When comparing two algorithms, T subscript 1 open parenthesis n close parenthesis   sim c subscript 1  times f subscript 1 open parenthesis n close parenthesis  and T subscript 2 open parenthesis n close parenthesis   sim c subscript 2  times f subscript 2 open parenthesis n close parenthesis , the one with the slower growth rate (the smaller f open parenthesis n close parenthesis ) will eventually be superior, regardless of the constants c subscript 1 and c subscript 2. The constant factor c captures machine-specific details, such as hardware speed and implementation efficiency, which are ignored in Big O notation as it focuses on the dominant term f open parenthesis n close parenthesis . The other options are incorrect: constants do not dominate performance at large input sizes (the function f open parenthesis n close parenthesis  does); they do depend on algorithm design and implementation; constants are not always zero; and while constants vary with hardware, their irrelevance in Big O is due to the focus on the growth rate, not solely their dependence on hardware. Because constant factors cancel out in relative running time comparisons. Because constants dominate the performance at large input sizes. Because constants depend only on input data, not algorithm design. Because constants always equal zero in running time analyses. Because constants vary with hardware but not with input size.']"
27,40,27_java_modular programming_memory_aliasing,"['java', 'modular programming', 'memory', 'aliasing', 'memory usage', 'arrays objects', 'garbage collection', 'java class', 'code', 'class file']","[""After modular programming, object-oriented programming is the next step in modern Java programming models, extending capabilities further. The statement is generally considered true in the context of Java's evolution because while Object-Oriented Programming (OOP) is the fundamental paradigm of Java itself, the introduction of Modular Programming (the Java Platform Module System, or JPMS, in Java 9) represents a next step in organizing and scaling large applications. Modular programming extends the organizational benefits of OOP by structuring code into named, self-contained modules, improving security, maintainability, and reliability by strictly defining dependencies and encapsulated access, which is a modern enhancement to the core OOP model."", 'Garbage collection in Java immediately frees memory as soon as an object becomes unreachable. The statement that garbage collection in Java immediately frees memory as soon as an object becomes unreachable is incorrect because the process is non-deterministic and automatic. The Java Virtual Machine (JVM) runs the Garbage Collector (GC) in the background at its own discretion and schedule, which is determined by factors like available memory, heap utilization, and GC algorithm choice, not immediately upon an object losing its last reference. While an object is eligible for collection the moment it becomes unreachable, the actual freeing of memory happens only when the GC executes, which could be moments, minutes, or even longer afterward.', 'Java primitive types like int and double use fixed-size memory representations regardless of runtime environment. The statement is False because Java primitive types like  text  (32-bit signed integer) and  text  (64-bit double-precision float) are defined by the Java Language Specification to have a fixed, platform-independent size . This design choice ensures that Java code, when executed on any Java Virtual Machine (JVM), behaves consistently and predictably, adhering to the principle of ""Write once, run anywhere."" If their sizes were dependent on the runtime environment (like in languages such as C/C++), portability and predictability would be lost.']"
28,38,28_memory_recollection_participants_strength,"['memory', 'recollection', 'participants', 'strength', 'item', 'indirect', 'repetitions', 'incidental learning', 'nonverbal', 'learning']","['What does the usage of multiple sources of evidence imply for models of recognition memory? Since recognition is not based on a single source, the first point holds: Sources might combine to affect recognition strength (e.g., both a strong feeling of familiarity and some recollection of context). This confirms the second point: Recognition can use both familiarity and recollection. Furthermore, the third point is true because a recognition judgment requires people selectively respond based on retrieved evidence, making a high-confidence ""yes"" if recollection occurs, or a lower-confidence ""yes"" if only familiarity is present. The other options are incorrect as they reject the dual-process idea, wrongly assuming evidence type is ignored, or that only a single source is used. Sources might combine to affect recognition strength Recognition can use both familiarity and recollection People selectively respond based on retrieved evidence Decision processes ignore the type of evidence retrieved Recognition depends only on a single scalar memory strength Memory sources do not influence recognition responses', ""What problem did Müller's memory drum address in the presentation of memory items? The Müller memory drum was an early experimental device in psychology designed to address a critical methodological problem: controlling the exact timing and presentation of memory stimuli. Its primary purpose was to present one syllable (or item) at a time, for a precise, brief duration, which effectively prevented participants from engaging in uncontrolled covert rehearsal of previously seen items while the list was still being presented. By strictly regulating the item-by-item exposure and interval, Müller's apparatus ensured that all participants received the same controlled conditions for initial encoding, which was necessary for rigorous memory research. It prevented participants from rehearsing items during presentation It produced auditory cues to support encoding through phonetic repetition It allowed participants to see entire lists at once to improve rehearsal ability It extended exposure time for each syllable to enhance memorization"", ""What was a critical feature that distinguished Müller's laboratory experiments from Ebbinghaus's? While Ebbinghaus pioneered the experimental study of memory, he primarily served as his own participant, focusing on a single case study. Georg Elias Müller, by contrast, introduced greater experimental control and laboratory rigor to memory studies, including the use of controlled apparatus (like the memory drum for presenting stimuli) and, most critically, testing larger groups of participants. This shift from self-experimentation to studying groups with controlled methods allowed for the calculation of mean effects and enhanced the generalizability and statistical validity of memory research, moving the field towards modern psychological methods. Müller studied large groups with controlled apparatus, improving rigor Müller avoided timing controls and allowed self-paced learning sessions Müller only tested a single participant repeatedly under similar conditions Müller relied solely on introspection without experimental equipment""]"
29,32,29_fibonacci_fibonacci numbers_numbers_fibonacci number,"['fibonacci', 'fibonacci numbers', 'numbers', 'fibonacci number', 'fibonacci primes', 'fibonacci sequence', 'strings', 'prime indices', 'modulo', 'indices fibonacci']","[""What does Zeckendorf's theorem state about positive integers and Fibonacci numbers? Zeckendorf's theorem is a fundamental result in number theory stating that any positive integer can be written as the sum of one or more non-consecutive Fibonacci numbers, and this representation is unique . The requirement that the Fibonacci numbers are non-consecutive is crucial for uniqueness. The other options are incorrect statements about the theorem: it applies to all positive integers, not just primes or multiples, and it confirms a unique representation exists. Every positive integer can be uniquely represented as a sum of non-consecutive Fibonacci numbers. Fibonacci numbers can represent only prime integers uniquely. Every positive integer is a multiple of some Fibonacci number. There is no representation of integers using Fibonacci numbers."", 'What is the defining rule of the Fibonacci sequence recurrence relation for n > 1?  The Fibonacci sequence (F subscript n) is defined by the recurrence relation for n  greater than  1 as: F subscript n  equals  F subscript n minus 1  plus  F subscript n minus 2. This means that any number in the sequence (after the first two starting terms, typically F subscript 1 equals 1 and F subscript 2 equals 1 or F subscript 0 equals 0 and F subscript 1 equals 1) is generated by adding the two terms immediately before it. For example, the third term (F subscript 3) is F subscript 2  plus  F subscript 1  equals  1  plus  1  equals  2. The other options describe different types of sequences: a geometric sequence (product of terms), a simple arithmetic sequence (adding 1), or a multiplicative sequence (doubling the previous term), none of which define the Fibonacci sequence. Each term is the sum of the two preceding terms. Each term is the product of the two preceding terms, Fn = Fn−1 × Fn−2. Each term is one more than the previous term, Fn = Fn−1 + 1. Each term is double the previous term, Fn = 2 × Fn−1.', 'How does the Fibonacci sequence relate to the golden ratio as n increases? The Fibonacci sequence, defined by F subscript n  equals  F subscript n minus 1  plus  F subscript n minus 2 with starting values F subscript 1 equals 1 and F subscript 2 equals 1, exhibits a property where the ratio of any term to its immediately preceding term, represented by the limit  limit  subscript n  approaches  infinity   F subscript n divided by F subscript n minus 1 , converges to the Golden Ratio ( phi ) . The Golden Ratio is an irrational number approximately equal to 1.618. The other options are incorrect: the ratio tends towards  phi  (not zero or infinity); and the ratio of alternate terms converges to  phi  to the power of 2  approximately equal to 2.618, not the square root of two ( approximately equal to 1.414). The ratio of two consecutive Fibonacci numbers tends towards the golden ratio. The ratio of two consecutive Fibonacci numbers tends towards zero rapidly. The ratio of alternate Fibonacci numbers tends towards the square root of two. The ratio of two consecutive Fibonacci numbers tends towards infinity as n increases.']"
30,28,30_scientific_research_focusing_commercial,"['scientific', 'research', 'focusing', 'commercial', 'plagiarism', 'commercial gain', 'running', 'peer', 'review', 'test hypotheses']","['What is a significant reason for worst-case performance guarantees in software systems? The primary reason for worst-case performance guarantees is to ensure a software system, especially a critical or real-time one, will always finish its task within a guaranteed maximum time, even when facing the most challenging or pathological data inputs. This predictable maximum time is vital for safety and reliability in applications like aircraft control, medical devices, or financial trading, where delays or failure to complete a task could lead to catastrophic results. The other options focus on typical or average performance, energy, or simplicity, which are secondary concerns to the fundamental requirement of guaranteed timely operation in critical environments. To ensure safety and reliable completion in critical systems under any input. To optimize average performance for casual users. To reduce the overall energy consumption during execution. To make programs run faster on typical inputs only. To simplify programming by ignoring pathological cases.', ""What is a primary goal of cognitive neuroscience in studying memory? Cognitive neuroscience is an interdisciplinary field that specifically seeks to bridge the gap between cognitive processes (like memory, attention, and language) and their underlying brain function. Its primary goal is to determine the neural substrates—the biological mechanisms—that give rise to these mental phenomena. By using tools like fMRI, EEG, and lesion studies, cognitive neuroscientists aim to map memory formation, storage, and retrieval onto specific brain regions, cell circuits, and molecular pathways. The other options are incorrect because they propose to discard biological explanations, prove a lack of brain-behavior link, or focus solely on behavior, all of which contradict the core, integrative mission of cognitive neuroscience To uncover the brain's biological mechanisms underlying memory. To discard biological explanations in favor of abstract mental models. To prove that mental processes cannot be linked to biology in any way. To focus solely on observable behavior without reference to the brain."", ""What ethical questions arise from the Henrietta Lacks case? These selections address the core bioethical violations surrounding the HeLa cell line, which was established without her permission or her family's awareness. These options highlight the modern standards of informed consent, equitable compensation, and proper attribution that were ignored during her treatment and subsequent decades of commercial research. In contrast, accepting tissue use without review, prioritizing commercial gain over ethics, and ignoring patient identity are the very practices that created the controversy and are universally rejected by current ethical frameworks. Consequently, the case serves as a landmark lesson in the necessity of protecting patient rights and bodily autonomy in scientific exploration. Sharing financial benefits from cell research. Consent and knowledge before taking tissue samples. Recognizing Henrietta Lacks in publications, awards. Accepting tissue use without any ethical review. Prioritizing commercial gain over ethical considerations. Ignoring patient identity when conducting research.""]"
31,25,31_sum interior angles_interior angles_interior_exterior,"['sum interior angles', 'interior angles', 'interior', 'exterior', 'exterior angles', 'sum exterior angles', 'sum exterior', 'angles polygon', '360 sum', '90 180 270 360']","[""What is the sum of the exterior angles of any polygon (one at each vertex)? The sum of the exterior angles of any polygon is always 360 to the power of ∘ . This is a fundamental property of all convex polygons, regardless of the number of sides. To understand why, imagine walking around the perimeter of any polygon. As you turn each corner, you are turning through an exterior angle. By the time you get back to where you started, you've made one full 360  to the power of ∘ rotation.  180° 360° 540° 720°"", 'What is the sum of the exterior angles of any triangle (one at each vertex)? The sum of the exterior angles of any convex polygon, including a triangle, is always 360 to the power of ∘. An exterior angle is formed by extending one side of the polygon and the adjacent side. This principle holds true regardless of the number of sides the polygon has. For a triangle specifically, the three exterior angles, one at each vertex, will always add up to 360 to the power of ∘. 180°  270° 360° 540°', 'What is the sum of the measures of the four angles of a rectangle? A rectangle is a special type of quadrilateral, which is a polygon with four sides. It has two key properties:\r\n\r\n    All four of its angles are right angles, meaning each angle measures exactly 90 degrees.\r\n\r\n    Its opposite sides are parallel and equal in length.\r\n\r\nSince a rectangle has four right angles, the sum of their measures is:\r\n\r\n90 to the power of ∘ plus 90 to the power of ∘ plus 90 to the power of ∘ plus 90 to the power of ∘ equals 360 to the power of ∘\r\n\r\nThis principle also applies to all quadrilaterals, as the sum of their interior angles is always 360 degrees. 180°  270° 360° 540°']"
