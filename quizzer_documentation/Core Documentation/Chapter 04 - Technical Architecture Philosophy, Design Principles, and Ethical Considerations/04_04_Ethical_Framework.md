# Potential and Ethical Considerations of Learning Analytics

Learning analytics represents a transformative approach to understanding and optimizing educational processes, with Long and Siemens (2011) noting that "the most dramatic factor shaping the future of higher education is something that we can't actually touch or see: big data and analytics" (p. 31). Indeed today's technological landscape fundamentally changes what's possible in educational data collection. Where classroom interactions once left minimal traces, now every digital interaction creates a data point that can be analyzed (Long & Siemens, 2011). And while industries like medicine and business have embraced data-driven decision-making, educational institutions have historically underutilized their data despite collecting extensive information about students. This transition raises critical ethical questions about surveillance, privacy, and student autonomy.

Long and Siemens paper "Penetrating the Fog" makes a definitive statement that learning analytics does have tremendous benefits to higher education, learning analytics also has value well beyond just higher education and there is no need to limit the discussion to just a single sub-set of a much broader issue. The issue being that education from k-12 is poorly managed and there is an epidemic of teachers quitting because of the conditions, adjunct professors who have somehow become a staple of university staff when in fact they are classified as contractors and therefore don't provide long term help to students, higher education being off limits to most Americans and education loses its status as a priority once we graduate from high school, IF a person graduates from high school. Learning analytics can help address the whole suite of issues in education as an industry, as well as providing feedback on how economic and power structures fundamentally effect the way education operates.

Traditional approaches to learning analytics ethics have often relied on utilitarian frameworks that prioritize maximum benefit. However, Willis (2014) argues that determining "the most good for the most people" becomes problematic when institutions define "good" in terms of efficiency and profit rather than student welfare. These utilitarian frameworks typically fail to question the underlying capitalist assumptions that treat education as a commodity and students as consumers. Slade and Prinsloo (2013) advance this conversation by proposing a sociocritical perspective that examines power dynamics and surveillance in learning analytics. Their later work develops an ethics of care framework that emphasizes relationships and responsibilities toward vulnerable students rather than just abstract principles of justice (Prinsloo & Slade, 2017).

Capitalist frameworks are fundamentally antithetical to education's true mission. When education becomes commodified, data collection serves primarily to maximize efficiency and revenue—not to enhance learning. Kyle (2019) notes that learning analytics systems often prioritize institutional finances over student privacy, with commercial interests driving development rather than pedagogical needs.

The key ethical question is not simply how data can improve educational outcomes, but how learning analytics can be implemented while resisting market-driven forces that undermine education's emancipatory potential, and to prevent the abuse of power that can result from careless practices. Current ethical frameworks are inadequate because they often uncritically accept capitalist assumptions about education's purpose. By integrating perspectives from critical data studies, social justice theory, and educational philosophy, we can develop a more nuanced ethical approach to learning analytics that recognizes education as a social good rather than a market transaction, and students as active participants in knowledge creation rather than data points to be optimized for institutional gain.

# Privacy and Power Across the Learning Analytics Lifecycle
Qinyi Liu and Mohammad Khalil made an important contribution to addressing the downsides of learning analytics by bringing up issues surrounding privacy and data protection. The definition they provide for privacy is "students having control over their own data, and personal information not being disclosed throughout the process of data collection, analysis, or reporting." Their paper further adopts the EU's data protection laws for a definition, "specifically data protection is covered in both acts by the following three points: (1) all activities related to personal data, such as collection, processing, storage, transmission and deletion, must be used for specific and explicit purposes in accordance with the principles of fairness and transparency, (2) measures must be taken during data processing to ensure that personal data are handled in a secure manner to prevent unauthorized access and possible damage and loss and (3) data must be kept up to date during this process and saved only for as long as necessary (Data Protection Act, 2018; Kuner et al., 2020)."

Liu and Khalil categorize existing solutions as legal/framework-based (65% of proposals), technical (31%), or combined approaches, noting a troubling implementation gap where only 18% of legal/framework solutions have been applied compared to 63% of technical solutions. The authors conclude that privacy and data protection concerns cannot be relaxed at any stage of learning analytics implementation, as these issues permeate the entire process from data collection to reporting. They emphasize the need for targeted stakeholder research before implementation and advocate for more evidence-based approaches to privacy solutions that would increase the trustworthiness and utility of learning analytics. By highlighting the tension between the promise of data-driven educational improvement and the fundamental right to privacy, this research provides a comprehensive counterpoint to more enthusiastic promotions of learning analytics, suggesting that the field cannot advance without directly addressing these persistent privacy and ethical concerns.

However judging these concerns based purely on whether it is legal or not, is a poor standard of ethics. Law itself emerges as a formalized expression of collectively recognized values, not vice versa (Stanford Encyclopedia of Philosophy, Natural Law Theories). When deliberating about ethical frameworks, referencing existing law creates a problematic circular argument that impedes moral progress, as ethical considerations extend "far beyond the question of what's legal" (McKinsey, 2022). This circular dependency is particularly dangerous given the temporal lag between ethical development and legal codification, with "legal solutions usually at least one step behind technological developments" (ISACA, 2016). The history of Jim Crow laws—legally enacted yet profoundly unethical—demonstrates how established law can codify injustice rather than serve as an ethical benchmark (Ferris State University). Therefore, authentic ethical deliberation, particularly regarding novel challenges like data privacy, must proceed independently from existing legal frameworks, anchored instead in fundamental principles of human flourishing and dignity.

This is where Liu and Khalil’s paper becomes especially revealing—not because of what they get wrong, but because of what they unintentionally illustrate: how often ethical and moral issues are filtered through a legalistic framework rather than through foundational questions of metaphysics or epistemology. As a result, many discussions get stuck on the _structure_ of the problem—like regulations, procedures, or institutional policies—rather than its _substance_. Still, the authors do point to several meaningful concerns that go beyond the legal framing, including issues of data collection, anonymization, misuse, transparency, power relationships, and stakeholder values.

Liu and Khalil highlight data collection issues such as “collecting too much sensitive data, and concerns about excessive data.” But is the collection of sensitive or large volumes of data inherently unethical? Likely not. The deeper issue is consent—specifically whether it’s given freely and with full understanding. Consent is a long-standing ethical value across many traditions, echoed in the Wiccan principle “do as you will,” which, though vague, reinforces the idea that autonomy is morally acceptable so long as it doesn’t harm others. Ethical violations arise not from the data itself, but from manipulating consent or bypassing it entirely. Still, even this framework has limits. Should everyone have a seat at the table in ethical deliberations—even those whose interests conflict with the general good, where “good” is defined as collective physical and mental well-being? It’s a difficult question. At the very least, we should demand transparency about intentions and clearly identify conflicts of interest when ethical arguments are made. Anything less risks undermining the ethical legitimacy of those discussions entirely.

The common ethical guideline around data collection—that data should be kept only as long as needed and limited to what's “necessary”—sounds sensible. But again: who decides what’s “necessary”? Who decides how long data should be retained? And who defines the outcome we’re working toward in the first place? These phrases might sound principled, but in practice, they’re often little more than fluff: vague enough to appear wise and cultured, yet too ambiguous to resolve anything meaningful. They look like solutions, but they function more like a smokescreen—masking the fact that no one has really answered the harder questions.
## Against the Profit Motive: Reclaiming Education as a Social Good
Stakeholders as defined in Liu and Khalil's paper is everyone involved or affected by learning analytics practices. By this definition we should indeed include the views and stances of data privacy and security of the stakeholders, however the connotation of stakeholder is very similar to shareholder. This naturally leads the conversation into capitalistic power structures and the underlying profit motive. As Hall and Stahl (2012) argue, there is a clear trend of the commodification and fetishisation of technologies in universities which allow capital to proliferate (p. 184) of technologies in educational settings. Data analytics has been applied across industries to maximize profits and shareholder value. The same techniques are now being applied as "learning analytics" using much of the same methodology. Williamson (2019) observes that learning analytics represents "algorithmic-driven models" that transform "learners' digital traces" into data for institutional purposes. For these reasons, it is not viable to consider the arguments of those administrators who profit off of the education system. These systems maximize value for the universities and the administrators, not the students, creating what Jones and VanScoy (2019) describe as a "highly asymmetric power relationship" (p. 25) between institutions and students.

Now let us review a couple of cases of actual abuse and misaligned values at play within the realm of educational technology. At Georgia State University, predictive analytics systems are used to flag students deemed “at risk” based on factors like dropping a class, earning a B- in a core course, or switching majors. While lauded for improving retention, these systems have a darker side: they risk pathologizing normal student behavior and nudging individuals toward "easier" academic tracks—not based on their interests or potential, but on institutional data trends designed to safeguard graduation metrics. This amounts to algorithmic gatekeeping, where predictive tools quietly foreclose the very intellectual freedom that education claims to champion (Scholes, 2016; Willis et al., 2016). A more egregious example is the short-lived InBloom initiative (2011–2014), which sought to centralize K–12 student data nationwide under the guise of improving educational outcomes. In practice, it handed sensitive student information—including disciplinary records and learning disabilities—to third-party vendors without adequate oversight or consent. The backlash from parents, educators, and privacy advocates was swift, and the platform was dismantled in 2014. The InBloom case remains a cautionary tale of how "student success" rhetoric can mask the commodification of learners under the pretense of personalization (Singer, 2014; Regan et al., 2016). These cases illustrate how learning systems, when driven by institutional convenience or commercial gain, risk not only reinforcing social inequalities but also undermining the very autonomy and dignity that education ought to foster.

Capitalism is largely an anti-social mechanism, highlighting the fact that it turns everything into purely transactional elements rather than ongoing elements, and that attempts to resolve this are artificial at best. As Hartmann (2016) explains, para-social relationships is the term given to describe this phenomenon where inherently anti-social relationships are "painted as true social relationships" through a process where "users intuitively experience physical stimuli... as a social being to which they, in turn, respond socially" (p. 131). On the other hand, education, government, community groups are not compatible with a purely transactional economic system, because these elements of society are inherently sociable communal efforts and defy artificial economic constraints. According to Caffentzis and Federici (2014), there is a fundamental "contrast [between] the logic underlining the production of 'commons' with the logic of capitalist relations" (p. i92), showing how communal efforts function differently from market-based systems.

Many technologies are the result of community efforts, examples include open source producing many great products, the internet was a government program, the same with GPS, with rocketry, NASA, None of these were produced with a capitalistic profit motive in mind. As Mazzucato explains, many revolutionary innovations came from government investments "irrespective of the business cycle" because they were "driven by big missions" rather than profit motives (Mazzucato, 2019). For GPS specifically, it emerged from NASA's work in the 1960s with "radio telescopes and a technique called very large baseline interferometry (VLBI)" which was later adapted for global positioning systems (NASA Spinoff, 2019). The open source software movement demonstrates how non-profit motivations can drive technological innovation, with developers contributing to projects like Linux primarily due to "identification as a Linux developer, by pragmatic motives to improve own software, and by their tolerance of time investments" rather than financial gain (Hertel et al., 2003, p. 1159). Efforts like public libraries, while established and initially funded by the Robber Baron Andrew Carnegie are socially funded efforts. Carnegie donated "more than $40 million for the construction of 1,679 public libraries across the United States" with the condition that "libraries be maintained by the communities they served" (Bobinski, 2021). On his part it was a move to mask the fact that he caused much suffering and exploitation and paint himself as a good philanthropist. Even social good when funded by capitalists is rarely done with a pure profit motive in mind.

Efforts meant for social good, should remain divorced from the profit motive. Education as a whole is a social good effort. Data analytics though originally adopted and pushed by capitalists to maximize profits, Long and Siemens briefly mentions how insurance companies use analytics to "find high risk people" (Long & Siemens, 2011, p. 33). Actuarial science steps in to ensure that insurance becomes only a good deal for the insurance companies, psychoanalyzing people to extract wealth from their hands and avoid providing a promised service involves deceit and dishonesty, which is definitively anti-social and harmful to society at large. As Barabas et al. (2018) note, actuarial risk assessments "might reproduce existing patterns of discrimination and historical biases that are reflected in the data," showing how these methods can perpetuate inequalities rather than resolve them. But we can flip this on its head in communal efforts like education, learning analytics can be applied here to ensure that education is highly effective from the first class someone takes at 5 years old to when they graduate at 18, and well beyond into their adult years. Wong and Li (2020) demonstrate how learning analytics provides numerous benefits, including helping institutions "monitor closely students' learning and persistence, predict students' performance, detect undesirable learning behaviours... and identify students at risk, for taking prompt follow-up action and providing proper assistance."

Addressing privacy and data protection is also largely something for social good, for those in power it is imperative to establish an all-seeing-eye, to know everything about the people they lord over. The most recent example of this in practice was the US surveillance state that Edward Snowden brought to public attention. By knowing everything absolute control and manipulation can be exercised upon them. As Mühlhoff (2021) argues, predictive analytics raises severe ethical concerns because it can be used to "stabilize or even increase social and economic inequalities and power differentials within societies," demonstrating how data collection can become a tool of control rather than empowerment. By developing privacy and data protection and ensuring these systems adhere to ethical frameworks we maintain a balance of power and diminish the possibility of misuse and exploitation. This is especially important since "the possibilities for implementing feedback in the data processing life cycle is highly domain specific" and without proper safeguards, power imbalances can be reinforced (Mühlhoff, 2021). Analytics let's us see what's going right with a system and what's going wrong with a system, we can see what's going on at the margins of society and provide more effective action. However, as Liu and colleagues (2023) point out, "students report that their ideal expectations are that they are the beneficiaries of LA and are able to control the data, but the power imbalance in the LA process often makes a difference between their actual feelings and their ideal expectations". The security aspect involves preventing abuse of power, and therefore should not be a discussion had with those with absolute power and who would have an interest in maintaining said power.

In conclusion, while learning analytics holds remarkable promise for addressing the deep structural failures of education, its implementation must be guided by ethical frameworks that go far beyond legal compliance or utilitarian efficiency. The prevailing capitalist logic that prioritizes profit and performance metrics must be critically examined, not accepted as a given. Instead, an ethics rooted in care, autonomy, and justice—one that recognizes students as human beings rather than data sources—must shape the development and deployment of these systems. Only by foregrounding issues of consent, power, and purpose at every stage of the learning analytics cycle can we ensure that this technology enhances education’s emancipatory potential rather than further entrenching its commodification.
## Works Cited

Siemens, George, and Phillip Long. "Penetrating the Fog: Analytics in Learning and Education." EDUCAUSE Review, vol. 46, no. 5, 2011. https://er.educause.edu/articles/2011/9/penetrating-the-fog-analytics-in-learning-and-education

Liu, Qinyi, and Mohammad Khalil. "Understanding privacy and data protection issues in learning analytics using a systematic review." British Journal of Educational Technology, vol. 54, no. 6, 2023. https://doi.org/10.1111/bjet.13388.

Finnis, J. (2020). Natural law theories. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy. [https://plato.stanford.edu/entries/natural-law-theories/](https://plato.stanford.edu/entries/natural-law-theories/)

Ferris State University. (n.d.). Examples of Jim Crow laws. Jim Crow Museum. [https://jimcrowmuseum.ferris.edu/links/misclink/examples.htm](https://jimcrowmuseum.ferris.edu/links/misclink/examples.htm)

ISACA. (2016). An ethical approach to data privacy protection. ISACA Journal, 6. [https://www.isaca.org/resources/isaca-journal/issues/2016/volume-6/an-ethical-approach-to-data-privacy-protection](https://www.isaca.org/resources/isaca-journal/issues/2016/volume-6/an-ethical-approach-to-data-privacy-protection)

McKinsey & Company. (2022, September 23). Data ethics: What it means and what it takes. [https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/data-ethics-what-it-means-and-what-it-takes](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/data-ethics-what-it-means-and-what-it-takes)

Kyle, M. L. J. (2019). Learning analytics and higher education: A proposed model for establishing informed consent mechanisms to promote student privacy and autonomy. International Journal of Educational Technology in Higher Education, 16, 1-22.

Long, P., & Siemens, G. (2011). Penetrating the fog: Analytics in learning and education. EDUCAUSE Review, 46(5), 31-40.

Prinsloo, P., & Slade, S. (2017). Big data, higher education and learning analytics: Beyond justice, toward an ethics of care. In B. K. Daniel (Ed.), Big Data and Learning Analytics in Higher Education (pp. 109-124). Springer.

Slade, S., & Prinsloo, P. (2013). Learning analytics: Ethical issues and dilemmas. American Behavioral Scientist, 57(10), 1510-1529.

Willis, J. E. (2014). Learning analytics and ethics: A framework beyond utilitarianism. EDUCAUSE Review Online.

Hall, R., & Stahl, B. (2012). Against commodification: The university, cognitive capitalism and emerging technologies. TripleC, 10(2), 184-202.

Jones, K. M., & VanScoy, A. (2019). The syllabus as a student privacy document in an age of learning analytics. Journal of Intellectual Freedom & Privacy, 2(2), 23-33.

Williamson, B. (2019, April 30). Learning from surveillance capitalism. Code Acts in Education. [https://codeactsineducation.wordpress.com/2019/04/30/learning-from-surveillance-capitalism/](https://codeactsineducation.wordpress.com/2019/04/30/learning-from-surveillance-capitalism/)

Caffentzis, G., & Federici, S. (2014). Commons against and beyond capitalism. Community Development Journal, 49(suppl_1), i92-i105.

Hartmann, T. (2016). Parasocial interaction, parasocial relationships, and well-being. In L. Reinecke & M. B. Oliver (Eds.), The Routledge handbook of media use and well-being: International perspectives on theory and research on positive media effects (pp. 131-144). Routledge.

Bobinski, G. S. (2021). Carnegie libraries: The future made bright. National Park Service. [https://www.nps.gov/articles/carnegie-libraries-the-future-made-bright-teaching-with-historic-places.htm](https://www.nps.gov/articles/carnegie-libraries-the-future-made-bright-teaching-with-historic-places.htm)

Hertel, G., Niedner, S., & Herrmann, S. (2003). Motivation of software developers in Open Source projects: an Internet-based survey of contributors to the Linux kernel. Research Policy, 32(7), 1159-1177.

Mazzucato, M. (2019). GPS, the internet, airbags: The government is often behind innovation. TED Blog. [https://blog.ted.com/qa-mariana-mazzucato-governments-often-fuel-innovation/](https://blog.ted.com/qa-mariana-mazzucato-governments-often-fuel-innovation/)

NASA Spinoff. (2019). NASA brings accuracy to world's global positioning systems. [https://spinoff.nasa.gov/Spinoff2019/ps_1.html](https://spinoff.nasa.gov/Spinoff2019/ps_1.html)

Barabas, C., Virza, M., Dinakar, K., Ito, J., & Zittrain, J. (2018). Interventions over predictions: Reframing the ethical debate for actuarial risk assessment. Proceedings of the 1st Conference on Fairness, Accountability and Transparency, 81, 62-76.

Downes, S. (2019). Ethical issues in learning analytics – Ethics, analytics and the duty of care. [https://pressbooks.pub/downes/chapter/chapter-three/](https://pressbooks.pub/downes/chapter/chapter-three/)

Wong, B. T. M., & Li, K. C. (2020). Learning analytics in higher education: An analysis of case studies. Asian Association of Open Universities Journal, 15(1), 13-25.

Mühlhoff, R. (2021). Predictive privacy: Towards an applied ethics of data analytics. Ethics and Information Technology, 23(4), 675-690.